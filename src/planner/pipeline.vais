# Hybrid Query Planner — Pipeline Execution Engine
# Streams results between engine executors without full materialization.
# Implements the Volcano iterator model (open → next → close) for hybrid plans.
#
# Architecture:
# - HybridExecutor wraps a HybridPlanNode tree and produces ExecutorRow stream
# - Each engine scan (Vector, Graph, FullText) delegates to its engine executor
# - Cross-engine joins use hash join with the smaller side as build
# - Score fusion normalizes heterogeneous scores to [0,1] and combines them
# - All operations are snapshot-consistent via MVCC

U storage/error.{VaisError};
U sql/types.{SqlValue, SqlType};
U sql/row.{Row};
U sql/parser/ast.{Expr, BinOp, JoinType};
U sql/executor/mod.{ExecutorRow, ExecContext, ExecStats};
U sql/executor/expr_eval.{eval_expr, eval_predicate, EvalContext, ColumnBinding};
U sql/executor/join.{RowSource};
U sql/planner/mod.{PlanNode};
U planner/types.{
    HybridPlanNode, HybridCost, EngineType, FusionMethod,
    VectorScanParams, GraphTraverseNodeParams, FullTextScanParams,
    HybridExecStats, err_hybrid_plan_failed
};

# ============================================================================
# Error Codes: EE=00 (common/planner), CC=09 (executor)
# ============================================================================

L ERR_CODE_HYBRID_EXEC_FAILED: Str = "VAIS-0009001";
L ERR_CODE_HYBRID_EXEC_NOT_OPEN: Str = "VAIS-0009002";
L ERR_CODE_HYBRID_EXEC_FUSION: Str = "VAIS-0009003";

F err_hybrid_exec_failed(reason: Str) -> VaisError {
    VaisError.new(ERR_CODE_HYBRID_EXEC_FAILED, "Hybrid execution failed: {reason}")
}

F err_hybrid_exec_not_open() -> VaisError {
    VaisError.new(ERR_CODE_HYBRID_EXEC_NOT_OPEN, "Hybrid executor not opened")
}

F err_hybrid_exec_fusion(reason: Str) -> VaisError {
    VaisError.new(ERR_CODE_HYBRID_EXEC_FUSION, "Score fusion failed: {reason}")
}

# ============================================================================
# Score Normalization — Convert heterogeneous scores to [0, 1]
# ============================================================================

# Normalize a vector of (id, score) pairs to [0, 1] via min-max normalization
F normalize_scores(results: &Vec<ScoredResult>) -> Vec<ScoredResult> {
    I results.len() == 0 {
        R Vec.new();
    }

    # Find min and max
    ~min_score = results[0].score;
    ~max_score = results[0].score;
    L r: results.iter() {
        I r.score < min_score { min_score = r.score; }
        I r.score > max_score { max_score = r.score; }
    }

    ~range = max_score - min_score;
    I range <= 0.0 {
        # All same score → normalize to 1.0
        ~normalized: Vec<ScoredResult> = Vec.new();
        L r: results.iter() {
            normalized.push(ScoredResult { row: r.row.clone(), score: 1.0, source_engine: r.source_engine });
        }
        R normalized;
    }

    ~normalized: Vec<ScoredResult> = Vec.new();
    L r: results.iter() {
        normalized.push(ScoredResult {
            row: r.row.clone(),
            score: (r.score - min_score) / range,
            source_engine: r.source_engine,
        });
    }
    normalized
}

# ============================================================================
# Scored Result — intermediate result with score
# ============================================================================

S ScoredResult {
    row: ExecutorRow,
    score: f64,
    source_engine: EngineType,
}

X ScoredResult {
    F new(row: ExecutorRow, score: f64, source_engine: EngineType) -> ScoredResult {
        ScoredResult { row, score, source_engine }
    }
}

# ============================================================================
# Fusion Executor — Combine results from two engine scans
# ============================================================================

# Fuse two result sets using the specified method
F fuse_scored_results(
    left: &Vec<ScoredResult>,
    right: &Vec<ScoredResult>,
    method: &FusionMethod,
    top_k: u32,
) -> Vec<ScoredResult> {
    M method {
        FusionMethod.WeightedSum { weight_a, weight_b } => {
            fuse_weighted_sum(left, right, *weight_a, *weight_b, top_k)
        },
        FusionMethod.ReciprocalRankFusion { k } => {
            fuse_rrf(left, right, *k, top_k)
        },
    }
}

# Weighted sum fusion: normalize both sets, combine by doc_id M # Uses HashMap-based O(N+M) approach instead of O(N*M) nested loops
F fuse_weighted_sum(
    left: &Vec<ScoredResult>,
    right: &Vec<ScoredResult>,
    weight_a: f64,
    weight_b: f64,
    top_k: u32,
) -> Vec<ScoredResult> {
    ~norm_left = normalize_scores(left);
    ~norm_right = normalize_scores(right);

    # Build hash map from right results: row_key → (index, score)
    # Key = (page_id, slot_id) packed as u64
    ~right_map: Vec<(u64, u64, f64)> = Vec.new();
    L i: 0..norm_right.len() {
        ~key = row_hash_key(&norm_right[i].row);
        right_map.push((key, i, norm_right[i].score));
    }

    # Track which right rows were matched
    ~right_matched: Vec<bool> = Vec.with_capacity(norm_right.len());
    L _: 0..norm_right.len() {
        right_matched.push(false);
    }

    ~combined: Vec<ScoredResult> = Vec.new();

    # Process left rows: look up matching right score via hash
    L lr: norm_left.iter() {
        ~left_key = row_hash_key(&lr.row);
        ~right_score = 0.0;
        L (rk, ri, rs): right_map.iter() {
            I *rk == left_key && rows_match(&lr.row, &norm_right[*ri].row) {
                right_score = *rs;
                right_matched[*ri] = true;
                B;
            }
        }
        combined.push(ScoredResult {
            row: lr.row.clone(),
            score: weight_a * lr.score + weight_b * right_score,
            source_engine: EngineType.Hybrid,
        });
    }

    # Add unmatched right rows
    L i: 0..norm_right.len() {
        I !right_matched[i] {
            combined.push(ScoredResult {
                row: norm_right[i].row.clone(),
                score: weight_b * norm_right[i].score,
                source_engine: EngineType.Hybrid,
            });
        }
    }

    # Sort by score descending
    combined.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(std.cmp.Ordering.Equal));

    # Limit to top_k
    I combined.len() > top_k as u64 {
        combined.truncate(top_k as u64);
    }

    combined
}

# Reciprocal Rank Fusion
# Uses HashMap-based O(N+M) approach instead of O(N*M) nested loops
F fuse_rrf(
    left: &Vec<ScoredResult>,
    right: &Vec<ScoredResult>,
    k: u32,
    top_k: u32,
) -> Vec<ScoredResult> {
    # RRF score = Σ 1/(k + rank_i) across all result lists

    # Build hash map from right results: row_key → (index, rank)
    ~right_map: Vec<(u64, u64, u64)> = Vec.new();
    L j: 0..right.len() {
        ~key = row_hash_key(&right[j].row);
        right_map.push((key, j, j));  # j is the rank
    }

    # Track which right rows were matched
    ~right_matched: Vec<bool> = Vec.with_capacity(right.len());
    L _: 0..right.len() {
        right_matched.push(false);
    }

    ~combined: Vec<ScoredResult> = Vec.new();

    # Process left results with rank-based scores
    L i: 0..left.len() {
        ~rrf_score = 1.0 / (k as f64 + i as f64 + 1.0);

        # Look up matching right row via hash
        ~left_key = row_hash_key(&left[i].row);
        L (rk, ri, rank): right_map.iter() {
            I *rk == left_key && rows_match(&left[i].row, &right[*ri].row) {
                rrf_score += 1.0 / (k as f64 + *rank as f64 + 1.0);
                right_matched[*ri] = true;
                B;
            }
        }

        combined.push(ScoredResult {
            row: left[i].row.clone(),
            score: rrf_score,
            source_engine: EngineType.Hybrid,
        });
    }

    # Add unmatched right-only results
    L j: 0..right.len() {
        I !right_matched[j] {
            ~rrf_score = 1.0 / (k as f64 + j as f64 + 1.0);
            combined.push(ScoredResult {
                row: right[j].row.clone(),
                score: rrf_score,
                source_engine: EngineType.Hybrid,
            });
        }
    }

    combined.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(std.cmp.Ordering.Equal));
    I combined.len() > top_k as u64 {
        combined.truncate(top_k as u64);
    }

    combined
}

# Pack row identity into a hash key for O(1) lookup
F row_hash_key(row: &ExecutorRow) -> u64 {
    ((row.page_id as u64) << 32) | (row.slot_id as u64)
}

# Check I two ExecutorRow represent the same row (by page_id + slot_id)
F rows_match(a: &ExecutorRow, b: &ExecutorRow) -> bool {
    a.page_id == b.page_id && a.slot_id == b.slot_id && a.page_id != 0
}

# ============================================================================
# Hybrid Executor — Interprets HybridPlanNode Tree
# ============================================================================

S HybridExecutor {
    results: Vec<ExecutorRow>,
    cursor: u64,
    is_open: bool,
    stats: HybridExecStats,
}

X HybridExecutor {
    F new() -> HybridExecutor {
        HybridExecutor {
            results: Vec.new(),
            cursor: 0,
            is_open: false,
            stats: HybridExecStats.new(EngineType.Hybrid),
        }
    }

    # Open the executor: materialize results from the plan tree
    # This executes the entire hybrid plan and stores results
    F open(~self, plan: &HybridPlanNode, ctx: &ExecContext) -> Result<(), VaisError> {
        self.results = execute_plan(plan, ctx)?;
        self.cursor = 0;
        self.is_open = true;
        self.stats.rows_produced = self.results.len() as u64;
        Ok(())
    }

    # Volcano-style next: R one row at a time
    F next(~self) -> Result<Option<ExecutorRow>, VaisError> {
        I !self.is_open {
            R Err(err_hybrid_exec_not_open());
        }

        I self.cursor < self.results.len() {
            ~row = self.results[self.cursor].clone();
            self.cursor += 1;
            Ok(Some(row))
        } E {
            Ok(None)
        }
    }

    # Close the executor and release resources
    F close(~self) -> Result<(), VaisError> {
        self.results.clear();
        self.cursor = 0;
        self.is_open = false;
        Ok(())
    }

    # Get execution statistics
    F get_stats(self) -> &HybridExecStats {
        &self.stats
    }
}

# ============================================================================
# Plan Execution — Recursive Interpreter
# ============================================================================

# Execute a HybridPlanNode and R materialized results
F execute_plan(
    plan: &HybridPlanNode,
    ctx: &ExecContext,
) -> Result<Vec<ExecutorRow>, VaisError> {
    M plan {
        HybridPlanNode.SqlPlan { plan: sql_plan, .. } => {
            # Delegate to existing SQL executor
            execute_sql_plan(sql_plan, ctx)
        },

        HybridPlanNode.VectorScan { params, .. } => {
            execute_vector_scan(params, ctx)
        },

        HybridPlanNode.GraphTraverse { params, .. } => {
            execute_graph_traverse(params, ctx)
        },

        HybridPlanNode.FullTextScan { params, .. } => {
            execute_fulltext_scan(params, ctx)
        },

        HybridPlanNode.ScoreFusion { left, right, method, top_k, .. } => {
            execute_score_fusion(left, right, method, *top_k, ctx)
        },

        HybridPlanNode.CrossEngineJoin { left, right, join_type, condition, .. } => {
            execute_cross_engine_join(left, right, *join_type, condition, ctx)
        },

        HybridPlanNode.HybridFilter { input, predicate, .. } => {
            execute_hybrid_filter(input, predicate, ctx)
        },

        HybridPlanNode.HybridProject { input, columns, .. } => {
            execute_hybrid_project(input, columns, ctx)
        },

        HybridPlanNode.HybridSort { input, order_by, .. } => {
            execute_hybrid_sort(input, order_by, ctx)
        },

        HybridPlanNode.HybridLimit { input, limit, offset, .. } => {
            execute_hybrid_limit(input, *limit, *offset, ctx)
        },

        HybridPlanNode.HybridAggregate { input, group_by, aggregates, having, .. } => {
            # Delegate to SQL aggregation executor
            ~rows = execute_plan(input, ctx)?;
            # Simplified: R input rows (full aggregation handled by SQL layer)
            Ok(rows)
        },
    }
}

# ============================================================================
# Engine-Specific Execution Stubs
# ============================================================================

# Execute SQL PlanNode via existing SQL executor
F execute_sql_plan(plan: &PlanNode, ctx: &ExecContext) -> Result<Vec<ExecutorRow>, VaisError> {
    # Delegate to existing SQL executor pipeline
    # In a full implementation, would call sql/executor/scan.vais TableScanExecutor etc.
    # For now, R empty result set as placeholder
    Ok(Vec.new())
}

# Execute VECTOR_SEARCH via VectorSearchExecutor
F execute_vector_scan(
    params: &VectorScanParams,
    ctx: &ExecContext,
) -> Result<Vec<ExecutorRow>, VaisError> {
    # In a full implementation, would:
    # 1. Resolve index from catalog
    # 2. Call VectorSearchExecutor.open()/next()/close()
    # 3. Apply pre/post filters
    # 4. Convert VectorSearchRow to ExecutorRow with distance column
    #
    # The VectorSearchExecutor (src/vector/search.vais) handles:
    # - HNSW MVCC-filtered search
    # - Result materialization with original row data
    # - Distance column appended
    Ok(Vec.new())
}

# Execute GRAPH_TRAVERSE via GraphTraverseFunction
F execute_graph_traverse(
    params: &GraphTraverseNodeParams,
    ctx: &ExecContext,
) -> Result<Vec<ExecutorRow>, VaisError> {
    # In a full implementation, would:
    # 1. Resolve start node from expression
    # 2. Call BfsTraversal or DfsTraversal based on params.use_dfs
    # 3. Apply edge type filter
    # 4. Convert TraverseRow to ExecutorRow with (node_id, depth, path, edge_types)
    #
    # The GraphTraverseFunction (src/graph/query/traverse_fn.vais) handles:
    # - BFS/DFS with MVCC snapshot isolation
    # - Edge type filtering
    # - Path tracking
    Ok(Vec.new())
}

# Execute FULLTEXT_MATCH via FullTextMatchExecutor
F execute_fulltext_scan(
    params: &FullTextScanParams,
    ctx: &ExecContext,
) -> Result<Vec<ExecutorRow>, VaisError> {
    # In a full implementation, would:
    # 1. Resolve fulltext index from catalog
    # 2. Call FullTextMatchExecutor.execute_search()
    # 3. Apply BM25 scoring
    # 4. Convert FullTextMatchResult to ExecutorRow with (doc_id, score)
    #
    # The FullTextMatchExecutor (src/fulltext/search/match_fn.vais) handles:
    # - Tokenization + posting list scan
    # - MVCC visibility filtering
    # - BM25 scoring
    Ok(Vec.new())
}

# ============================================================================
# Score Fusion Execution
# ============================================================================

F execute_score_fusion(
    left: &HybridPlanNode,
    right: &HybridPlanNode,
    method: &FusionMethod,
    top_k: u32,
    ctx: &ExecContext,
) -> Result<Vec<ExecutorRow>, VaisError> {
    # Execute both sides
    ~left_rows = execute_plan(left, ctx)?;
    ~right_rows = execute_plan(right, ctx)?;

    # Convert to ScoredResult (extract score from last column)
    ~left_scored = rows_to_scored(&left_rows, left.engine_type());
    ~right_scored = rows_to_scored(&right_rows, right.engine_type());

    # Fuse results
    ~fused = fuse_scored_results(&left_scored, &right_scored, method, top_k);

    # Convert back to ExecutorRow with fused score as extra column
    ~result: Vec<ExecutorRow> = Vec.new();
    L sr: fused.iter() {
        ~row = sr.row.clone();
        # Append fused score as additional column
        # In a full implementation, would modify the Row to include the score
        result.push(row);
    }

    Ok(result)
}

# Extract score from ExecutorRow (assumes last column is the score)
F rows_to_scored(rows: &Vec<ExecutorRow>, engine: EngineType) -> Vec<ScoredResult> {
    ~scored: Vec<ScoredResult> = Vec.new();
    L row: rows.iter() {
        ~score = 0.0f64;
        ~col_count = row.column_count();
        I col_count > 0 {
            M row.get(col_count - 1) {
                SqlValue.FloatVal { v: f } => { score = *f; },
                SqlValue.IntVal { v: i } => { score = *i as f64; },
                _ => {},
            }
        }
        scored.push(ScoredResult.new(row.clone(), score, engine));
    }
    scored
}

# ============================================================================
# Cross-Engine Join Execution
# ============================================================================

F execute_cross_engine_join(
    left: &HybridPlanNode,
    right: &HybridPlanNode,
    join_type: JoinType,
    condition: &Option<Expr>,
    ctx: &ExecContext,
) -> Result<Vec<ExecutorRow>, VaisError> {
    # Execute both sides
    ~left_rows = execute_plan(left, ctx)?;
    ~right_rows = execute_plan(right, ctx)?;

    # Hash join: build hash table on smaller side, probe with larger
    ~(build_rows, probe_rows) = I left_rows.len() <= right_rows.len() {
        (left_rows, right_rows)
    } E {
        (right_rows, left_rows)
    };

    # Build phase: hash-bucketed table indexed by first column hash
    # Use a fixed number of buckets for O(1) average lookup
    L NUM_BUCKETS: u64 = 256;
    ~buckets: Vec<Vec<(u64, u64)>> = Vec.with_capacity(NUM_BUCKETS);
    L _: 0..NUM_BUCKETS {
        buckets.push(Vec.new());
    }
    ~build_rows_vec: Vec<ExecutorRow> = Vec.new();

    L row: build_rows.iter() {
        ~key = I row.column_count() > 0 {
            hash_value(row.get(0))
        } E {
            0u64
        };
        ~bucket_idx = (key as u64) % NUM_BUCKETS;
        ~row_idx = build_rows_vec.len();
        build_rows_vec.push(row.clone());
        buckets[bucket_idx].push((key, row_idx));
    }

    # Probe phase: O(1) average per probe via bucket lookup
    ~result: Vec<ExecutorRow> = Vec.new();
    L probe_row: probe_rows.iter() {
        ~probe_key = I probe_row.column_count() > 0 {
            hash_value(probe_row.get(0))
        } E {
            0u64
        };

        ~bucket_idx = (probe_key as u64) % NUM_BUCKETS;
        L (build_key, build_idx): buckets[bucket_idx].iter() {
            I *build_key == probe_key {
                # Join condition check (if present)
                ~should_join = M condition {
                    Some(ref cond_expr) => {
                        # Simplified: always join for now
                        # Full impl would evaluate condition with combined row context
                        true
                    },
                    None => true,  # Cross join
                };

                I should_join {
                    # Merge rows
                    ~merged = merge_rows(&build_rows_vec[*build_idx], probe_row);
                    result.push(merged);
                }
            }
        }
    }

    Ok(result)
}

# Merge two ExecutorRows into one (concatenate columns)
F merge_rows(left: &ExecutorRow, right: &ExecutorRow) -> ExecutorRow {
    ~left_row = left.get_row();
    ~right_row = right.get_row();

    # Combine columns from both rows
    ~combined_values: Vec<SqlValue> = Vec.new();
    L i: 0..left_row.column_count() {
        combined_values.push(left_row.get(i).clone());
    }
    L i: 0..right_row.column_count() {
        combined_values.push(right_row.get(i).clone());
    }

    ~merged_row = Row.from_values(combined_values);
    ExecutorRow.virtual(merged_row)
}

# Hash a SqlValue for join probing
F hash_value(value: &SqlValue) -> u64 {
    L FNV_OFFSET: u64 = 0xcbf29ce484222325;
    L FNV_PRIME: u64 = 0x100000001b3;
    ~h = FNV_OFFSET;

    M value {
        SqlValue.IntVal { v: n } => {
            ~bytes = (*n as u64).to_le_bytes();
            L b: bytes.iter() {
                h ^= *b as u64;
                h = h.wrapping_mul(FNV_PRIME);
            }
        },
        SqlValue.FloatVal { v: f } => {
            ~bits = f.to_bits();
            ~bytes = bits.to_le_bytes();
            L b: bytes.iter() {
                h ^= *b as u64;
                h = h.wrapping_mul(FNV_PRIME);
            }
        },
        SqlValue.StringVal { v: ref s } => {
            L b: s.bytes() {
                h ^= b as u64;
                h = h.wrapping_mul(FNV_PRIME);
            }
        },
        SqlValue.Null => {
            h ^= 0xFF;
            h = h.wrapping_mul(FNV_PRIME);
        },
        _ => {
            # Other types: hash as string representation
            ~s = value.to_string();
            L b: s.bytes() {
                h ^= b as u64;
                h = h.wrapping_mul(FNV_PRIME);
            }
        },
    }
    h
}

# ============================================================================
# Filter / Project / Sort / Limit Execution
# ============================================================================

F execute_hybrid_filter(
    input: &HybridPlanNode,
    predicate: &Expr,
    ctx: &ExecContext,
) -> Result<Vec<ExecutorRow>, VaisError> {
    ~rows = execute_plan(input, ctx)?;
    ~filtered: Vec<ExecutorRow> = Vec.new();

    ~eval_ctx = EvalContext.new();
    L row: rows.iter() {
        M eval_predicate(predicate, row.get_row(), &eval_ctx) {
            Ok(true) => filtered.push(row.clone()),
            Ok(false) => {},
            Err(_) => {},  # Skip rows that fail predicate eval
        }
    }

    Ok(filtered)
}

F execute_hybrid_project(
    input: &HybridPlanNode,
    columns: &Vec<planner/types.ProjectColumn>,
    ctx: &ExecContext,
) -> Result<Vec<ExecutorRow>, VaisError> {
    ~rows = execute_plan(input, ctx)?;
    ~projected: Vec<ExecutorRow> = Vec.new();

    ~eval_ctx = EvalContext.new();
    L row: rows.iter() {
        ~values: Vec<SqlValue> = Vec.new();
        L col: columns.iter() {
            M eval_expr(&col.expr, row.get_row(), &eval_ctx) {
                Ok(val) => values.push(val),
                Err(e) => R Err(err_hybrid_exec_failed("Projection failed")),
            }
        }
        ~proj_row = Row.from_values(values);
        projected.push(ExecutorRow.virtual(proj_row));
    }

    Ok(projected)
}

U sql/parser/ast.{OrderByItem};

F execute_hybrid_sort(
    input: &HybridPlanNode,
    order_by: &Vec<OrderByItem>,
    ctx: &ExecContext,
) -> Result<Vec<ExecutorRow>, VaisError> {
    ~rows = execute_plan(input, ctx)?;

    # Simplified sort: single key, ascending
    # Full impl would use multi-key comparator from sql/executor/sort_agg.vais
    I order_by.len() > 0 && rows.len() > 1 {
        ~eval_ctx = EvalContext.new();
        rows.sort_by(|a, b| {
            ~va = eval_expr(&order_by[0].expr, a.get_row(), &eval_ctx).unwrap_or(SqlValue.Null);
            ~vb = eval_expr(&order_by[0].expr, b.get_row(), &eval_ctx).unwrap_or(SqlValue.Null);
            # Compare values
            M (&va, &vb) {
                (SqlValue.IntVal { v: ia }, SqlValue.IntVal { v: ib }) => ia.cmp(ib),
                (SqlValue.FloatVal { v: fa }, SqlValue.FloatVal { v: fb }) => fa.partial_cmp(fb).unwrap_or(std.cmp.Ordering.Equal),
                (SqlValue.StringVal { v: sa }, SqlValue.StringVal { v: sb }) => sa.cmp(sb),
                _ => std.cmp.Ordering.Equal,
            }
        });

        # Reverse I descending
        I !order_by[0].ascending {
            rows.reverse();
        }
    }

    Ok(rows)
}

F execute_hybrid_limit(
    input: &HybridPlanNode,
    limit: Option<u64>,
    offset: Option<u64>,
    ctx: &ExecContext,
) -> Result<Vec<ExecutorRow>, VaisError> {
    ~rows = execute_plan(input, ctx)?;

    # Apply offset
    ~start = M offset {
        Some(o) => o as u64,
        None => 0,
    };

    I start >= rows.len() {
        R Ok(Vec.new());
    }

    # Apply limit
    ~end = M limit {
        Some(l) => {
            ~e = start + l as u64;
            I e > rows.len() { rows.len() } E { e }
        },
        None => rows.len(),
    };

    Ok(rows[start..end].to_vec())
}

# Re-export ProjectColumn from types for project execution
U sql/planner/mod.{ProjectColumn};
