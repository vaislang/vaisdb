# Graph Engine Concurrency
# Node-level locking with RwLock-based lock manager
# Based on Stage 4 Memory Architecture - concurrency control for graph operations
# Error codes: EE=03 (graph), CC=08 (concurrency)

U std/sync.{RwLock};
U std/vec.Vec;
U std/option.{Option, Some, None};
U std/result.Result;
U storage/error.{VaisError, ErrorSeverity};

# ============================================================================
# Constants
# ============================================================================

# Default capacity for lock table (number of lock slots)
L DEFAULT_LOCK_CAPACITY: u32 = 1024;

# Lock timeout in milliseconds
L LOCK_TIMEOUT_MS: u64 = 5000;

# ============================================================================
# Error constructors (EE=03 graph, CC=08 concurrency)
# ============================================================================

F err_graph_lock_timeout(node_id: u64) -> VaisError {
    VaisError.new(
        "VAIS-0308001",
        "Lock timeout on graph node {node_id} after {LOCK_TIMEOUT_MS}ms"
    )
}

F err_graph_deadlock_prevention(node_id: u64) -> VaisError {
    VaisError.new(
        "VAIS-0308002",
        "Deadlock prevention: cannot acquire lock on node {node_id}"
    )
}

F err_graph_lock_poisoned(node_id: u64) -> VaisError {
    VaisError.new(
        "VAIS-0308003",
        "Lock poisoned for graph node {node_id}"
    ).with_severity(ErrorSeverity.Fatal)
}

# ============================================================================
# Lock modes
# ============================================================================

L LockMode = Shared | Exclusive;

# ============================================================================
# NodeLock — Per-node RwLock wrapper
# ============================================================================

S NodeLock {
    node_id: u64,
    lock: RwLock<()>,
    ref_count: u32,    # Number of active guards
}

X NodeLock {
    F new(node_id: u64) -> NodeLock {
        NodeLock {
            node_id,
            lock: RwLock.new(()),
            ref_count: 0,
        }
    }

    # Acquire shared (read) lock
    F read_lock(~self) {
        self.lock.read_lock();
        self.ref_count += 1;
    }

    # Acquire exclusive (write) lock
    F write_lock(~self) {
        self.lock.write_lock();
        self.ref_count += 1;
    }

    # Try to acquire shared lock without blocking
    F try_read_lock(~self) -> bool {
        I self.lock.try_read_lock() {
            self.ref_count += 1;
            true
        } E {
            false
        }
    }

    # Try to acquire exclusive lock without blocking
    F try_write_lock(~self) -> bool {
        I self.lock.try_write_lock() {
            self.ref_count += 1;
            true
        } E {
            false
        }
    }

    # Release shared lock
    F read_unlock(~self) {
        self.lock.read_unlock();
        I self.ref_count > 0 {
            self.ref_count -= 1;
        }
    }

    # Release exclusive lock
    F write_unlock(~self) {
        self.lock.write_unlock();
        I self.ref_count > 0 {
            self.ref_count -= 1;
        }
    }

    # Check if lock is currently held
    F is_held(self) -> bool {
        self.ref_count > 0
    }
}

# ============================================================================
# LockSlot — Hash table entry for lock striping
# ============================================================================

S LockSlot {
    slot_lock: RwLock<()>,      # Protects the locks vector
    locks: Vec<NodeLock>,        # Locks in this slot
}

X LockSlot {
    F new() -> LockSlot {
        LockSlot {
            slot_lock: RwLock.new(()),
            locks: Vec.new(),
        }
    }

    # Find or create node lock in this slot
    F get_or_create_lock(~self, node_id: u64) -> &~NodeLock {
        # Acquire slot lock for modification
        self.slot_lock.write_lock();

        # Search for existing lock
        ~i: u64 = 0;
        W i < self.locks.len() {
            I self.locks[i].node_id == node_id {
                self.slot_lock.write_unlock();
                R &~self.locks[i];
            }
            i += 1;
        }

        # Create new lock
        self.locks.push(NodeLock.new(node_id));
        ~idx = self.locks.len() - 1;
        self.slot_lock.write_unlock();
        &~self.locks[idx]
    }

    # Find existing node lock in this slot (read-only)
    F find_lock(self, node_id: u64) -> Option<&NodeLock> {
        self.slot_lock.read_lock();
        ~i: u64 = 0;
        W i < self.locks.len() {
            I self.locks[i].node_id == node_id {
                ~result = Some(&self.locks[i]);
                self.slot_lock.read_unlock();
                R result;
            }
            i += 1;
        }
        self.slot_lock.read_unlock();
        None
    }
}

# ============================================================================
# GraphLockManager — Node-level lock manager
# ============================================================================

S GraphLockManager {
    slots: Vec<LockSlot>,
    capacity: u32,
}

X GraphLockManager {
    # Create new lock manager with specified capacity
    F new(capacity: u32) -> GraphLockManager {
        ~slots = Vec.with_capacity(capacity as u64);
        ~i: u32 = 0;
        W i < capacity {
            slots.push(LockSlot.new());
            i += 1;
        }
        GraphLockManager { slots, capacity }
    }

    # Create with default capacity
    F default() -> GraphLockManager {
        GraphLockManager.new(DEFAULT_LOCK_CAPACITY)
    }

    # Hash node_id to slot index
    F hash(self, node_id: u64) -> u64 {
        (node_id % (self.capacity as u64)) as u64
    }

    # Acquire shared (read) lock on node
    F lock_node_shared(~self, node_id: u64) -> Result<NodeLockGuard, VaisError> {
        ~slot_idx = self.hash(node_id);
        ~lock = self.slots[slot_idx].get_or_create_lock(node_id);
        lock.read_lock();
        Ok(NodeLockGuard.new(self, node_id, LockMode.Shared))
    }

    # Acquire exclusive (write) lock on node
    F lock_node_exclusive(~self, node_id: u64) -> Result<NodeLockGuard, VaisError> {
        ~slot_idx = self.hash(node_id);
        ~lock = self.slots[slot_idx].get_or_create_lock(node_id);
        lock.write_lock();
        Ok(NodeLockGuard.new(self, node_id, LockMode.Exclusive))
    }

    # Try to acquire shared lock without blocking
    F try_lock_node_shared(~self, node_id: u64) -> Result<Option<NodeLockGuard>, VaisError> {
        ~slot_idx = self.hash(node_id);
        ~lock = self.slots[slot_idx].get_or_create_lock(node_id);
        I lock.try_read_lock() {
            Ok(Some(NodeLockGuard.new(self, node_id, LockMode.Shared)))
        } E {
            Ok(None)
        }
    }

    # Try to acquire exclusive lock without blocking
    F try_lock_node_exclusive(~self, node_id: u64) -> Result<Option<NodeLockGuard>, VaisError> {
        ~slot_idx = self.hash(node_id);
        ~lock = self.slots[slot_idx].get_or_create_lock(node_id);
        I lock.try_write_lock() {
            Ok(Some(NodeLockGuard.new(self, node_id, LockMode.Exclusive)))
        } E {
            Ok(None)
        }
    }

    # Lock an edge (both src and dst nodes) in ID order to prevent deadlock
    F lock_edge(~self, src: u64, dst: u64) -> Result<EdgeLockGuard, VaisError> {
        # Always lock in ascending node ID order to prevent deadlock
        I src < dst {
            ~src_guard = self.lock_node_exclusive(src)?;
            ~dst_guard = self.lock_node_exclusive(dst)?;
            Ok(EdgeLockGuard.new(src, dst, src_guard, dst_guard))
        } E I src > dst {
            ~dst_guard = self.lock_node_exclusive(dst)?;
            ~src_guard = self.lock_node_exclusive(src)?;
            Ok(EdgeLockGuard.new(src, dst, src_guard, dst_guard))
        } E {
            # Self-loop: only lock once
            ~src_guard = self.lock_node_exclusive(src)?;
            Ok(EdgeLockGuard.new_self_loop(src, src_guard))
        }
    }

    # Release node lock explicitly (normally handled by guard drop)
    F release_node(~self, node_id: u64, mode: LockMode) {
        ~slot_idx = self.hash(node_id);
        I ~opt_lock = self.slots[slot_idx].find_lock(node_id) {
            I ~lock = opt_lock {
                M mode {
                    LockMode.Shared => lock.read_unlock(),
                    LockMode.Exclusive => lock.write_unlock(),
                }
            }
        }
    }
}

# ============================================================================
# NodeLockGuard — RAII-style guard for node locks
# ============================================================================

S NodeLockGuard {
    manager: &~GraphLockManager,
    node_id: u64,
    mode: LockMode,
    held: bool,
}

X NodeLockGuard {
    F new(manager: &~GraphLockManager, node_id: u64, mode: LockMode) -> NodeLockGuard {
        NodeLockGuard {
            manager,
            node_id,
            mode,
            held: true,
        }
    }

    # Release the lock early (before drop)
    F release(~self) {
        I self.held {
            self.manager.release_node(self.node_id, self.mode);
            self.held = false;
        }
    }

    # Get node ID
    F node_id(self) -> u64 {
        self.node_id
    }

    # Get lock mode
    F mode(self) -> LockMode {
        self.mode
    }

    # Check if lock is still held
    F is_held(self) -> bool {
        self.held
    }
}

# Drop implementation: release lock on scope exit
X Drop for NodeLockGuard {
    F drop(~self) {
        self.release();
    }
}

# ============================================================================
# EdgeLockGuard — Lock guard for edge operations
# ============================================================================

S EdgeLockGuard {
    src_id: u64,
    dst_id: u64,
    src_guard: NodeLockGuard,
    dst_guard: Option<NodeLockGuard>,   # None for self-loops
}

X EdgeLockGuard {
    # Create edge guard for two different nodes
    F new(
        src_id: u64,
        dst_id: u64,
        src_guard: NodeLockGuard,
        dst_guard: NodeLockGuard
    ) -> EdgeLockGuard {
        EdgeLockGuard {
            src_id,
            dst_id,
            src_guard,
            dst_guard: Some(dst_guard),
        }
    }

    # Create edge guard for self-loop (src == dst)
    F new_self_loop(node_id: u64, guard: NodeLockGuard) -> EdgeLockGuard {
        EdgeLockGuard {
            src_id: node_id,
            dst_id: node_id,
            src_guard: guard,
            dst_guard: None,
        }
    }

    # Get source node ID
    F src_id(self) -> u64 {
        self.src_id
    }

    # Get destination node ID
    F dst_id(self) -> u64 {
        self.dst_id
    }

    # Check if this is a self-loop
    F is_self_loop(self) -> bool {
        self.src_id == self.dst_id
    }

    # Release both locks early (before drop)
    F release(~self) {
        self.src_guard.release();
        I ~opt_dst = &~self.dst_guard {
            I ~dst = opt_dst {
                dst.release();
            }
        }
    }
}

# Drop implementation: release both locks on scope exit
X Drop for EdgeLockGuard {
    F drop(~self) {
        self.release();
    }
}

# ============================================================================
# GraphRwLatch — Engine-level read-write latch for metadata operations
# ============================================================================

S GraphRwLatch {
    latch: RwLock<()>,
}

X GraphRwLatch {
    # Create new graph-level latch
    F new() -> GraphRwLatch {
        GraphRwLatch {
            latch: RwLock.new(()),
        }
    }

    # Acquire read (shared) latch
    F read_lock(~self) -> GraphLatchGuard {
        self.latch.read_lock();
        GraphLatchGuard.new(&~self, LatchMode.Read)
    }

    # Acquire write (exclusive) latch
    F write_lock(~self) -> GraphLatchGuard {
        self.latch.write_lock();
        GraphLatchGuard.new(&~self, LatchMode.Write)
    }

    # Try to acquire read latch without blocking
    F try_read_lock(~self) -> Option<GraphLatchGuard> {
        I self.latch.try_read_lock() {
            Some(GraphLatchGuard.new(&~self, LatchMode.Read))
        } E {
            None
        }
    }

    # Try to acquire write latch without blocking
    F try_write_lock(~self) -> Option<GraphLatchGuard> {
        I self.latch.try_write_lock() {
            Some(GraphLatchGuard.new(&~self, LatchMode.Write))
        } E {
            None
        }
    }

    # Release read latch
    F read_unlock(~self) {
        self.latch.read_unlock();
    }

    # Release write latch
    F write_unlock(~self) {
        self.latch.write_unlock();
    }
}

# ============================================================================
# Latch mode for engine-level operations
# ============================================================================

L LatchMode = Read | Write;

# ============================================================================
# GraphLatchGuard — RAII guard for graph-level latch
# ============================================================================

S GraphLatchGuard {
    latch: &~GraphRwLatch,
    mode: LatchMode,
    held: bool,
}

X GraphLatchGuard {
    F new(latch: &~GraphRwLatch, mode: LatchMode) -> GraphLatchGuard {
        GraphLatchGuard {
            latch,
            mode,
            held: true,
        }
    }

    # Release latch early (before drop)
    F release(~self) {
        I self.held {
            M self.mode {
                LatchMode.Read => self.latch.read_unlock(),
                LatchMode.Write => self.latch.write_unlock(),
            }
            self.held = false;
        }
    }

    # Get latch mode
    F mode(self) -> LatchMode {
        self.mode
    }

    # Check if latch is still held
    F is_held(self) -> bool {
        self.held
    }
}

# Drop implementation: release latch on scope exit
X Drop for GraphLatchGuard {
    F drop(~self) {
        self.release();
    }
}

# ============================================================================
# Deadlock Prevention Utilities
# ============================================================================

# Sort two node IDs in ascending order for deadlock-free locking
F order_node_ids(a: u64, b: u64) -> (u64, u64) {
    I a < b {
        (a, b)
    } E {
        (b, a)
    }
}

# Lock multiple nodes in ID order to prevent deadlock
# Returns guards in the same order as input node_ids
F lock_nodes_ordered(
    manager: &~GraphLockManager,
    node_ids: &[u64],
    exclusive: bool
) -> Result<Vec<NodeLockGuard>, VaisError> {
    # Sort node IDs to ensure consistent lock ordering
    ~sorted_ids = Vec.with_capacity(node_ids.len());
    ~i: u64 = 0;
    W i < node_ids.len() {
        sorted_ids.push(node_ids[i]);
        i += 1;
    }

    # Simple bubble sort for small arrays
    ~j: u64 = 0;
    W j < sorted_ids.len() {
        ~k: u64 = 0;
        W k < sorted_ids.len() - 1 - j {
            I sorted_ids[k] > sorted_ids[k + 1] {
                ~temp = sorted_ids[k];
                sorted_ids[k] = sorted_ids[k + 1];
                sorted_ids[k + 1] = temp;
            }
            k += 1;
        }
        j += 1;
    }

    # Acquire locks in sorted order
    ~guards = Vec.with_capacity(sorted_ids.len());
    ~idx: u64 = 0;
    W idx < sorted_ids.len() {
        ~guard = I exclusive {
            manager.lock_node_exclusive(sorted_ids[idx])?
        } E {
            manager.lock_node_shared(sorted_ids[idx])?
        };
        guards.push(guard);
        idx += 1;
    }

    Ok(guards)
}
