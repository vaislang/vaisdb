# ARIES Recovery - UNDO Phase
# Based on Stage 2 Section 7 - Crash Recovery Phase 3
#
# For each transaction in ATT that was not committed:
#   - Walk prev_lsn chain backwards
#   - Apply compensating operations (undo)
#   - Write CLR (Compensation Log Record) to WAL
# After all uncommitted transactions are undone, recovery complete
#
# CLR ensures idempotent recovery:
# If crash during undo, CLR's undo_next_lsn skips already-undone records

U std/bytes.{ByteBuffer};
U std/hashmap.{HashMap};

U storage/error.{VaisError, err_internal};
U storage/constants.{WAL_RECORD_HEADER_SIZE};
U storage/wal/header.{WalRecordHeader, ENGINE_META};
U storage/wal/record_types.{
    CLR, TXN_ABORT, ClrPayload, TxnAbortPayload,
    TUPLE_INSERT, TUPLE_DELETE, TUPLE_UPDATE,
    BTREE_INSERT, BTREE_DELETE, BTREE_SPLIT, BTREE_MERGE,
    PAGE_ALLOC, PAGE_DEALLOC,
};
U storage/wal/record_rel.{
    TupleInsertPayload, TupleDeletePayload, TupleUpdatePayload,
    BtreeInsertPayload, BtreeDeletePayload, BtreeSplitPayload, BtreeMergePayload,
};
U storage/page/heap.{HeapPage};
U storage/page/tuple.{Tuple};
U storage/page/mvcc.{MvccTupleMeta};
U storage/page/header.{PageHeader};
U storage/wal/writer.{WalWriter};
U storage/wal/lsn.{NULL_LSN, lsn_compare};
U storage/buffer/pool.{BufferPool};
U storage/recovery/redo.{
    AnalysisResult, RecoveryTxnState, RecoveryTxnEntry,
    WalRecordIterator,
};

# Undo result
S UndoResult {
    txns_undone: u32,          # Number of transactions undone
    records_undone: u64,       # Total CLR records written
}

# Phase 3: UNDO
# Undo all uncommitted transactions by walking their prev_lsn chains backward
F recovery_undo(
    wal_dir: &str,
    analysis: &AnalysisResult,
    writer: &~WalWriter,
    pool: &~BufferPool,
) -> Result<UndoResult, VaisError> {
    ~txns_undone: u32 = 0;
    ~records_undone: u64 = 0;

    # Collect uncommitted (active) transactions sorted by last_lsn descending
    # Process in reverse LSN order to maximize efficiency
    ~active_txns: Vec<(u64, u64)> = Vec.new();  # (txn_id, last_lsn)

    L (txn_id, entry): &analysis.txn_table {
        M entry.state {
            RecoveryTxnState.Active => {
                active_txns.push((*txn_id, entry.last_lsn));
            },
            _ => {},
        }
    }

    I active_txns.is_empty() {
        R Ok(UndoResult { txns_undone: 0, records_undone: 0 });
    }

    # Sort by last_lsn descending (process most recent first)
    active_txns.sort_by(|a, b| lsn_compare(b.1, a.1));

    # Build a map of LSN -> WalRecordHeader+payload for quick lookup
    # In practice, we'd re-read from WAL segments, but we track undo progress per txn
    ~undo_progress: HashMap<u64, u64> = HashMap.new();  # txn_id -> next_lsn_to_undo

    L &(txn_id, last_lsn): &active_txns {
        # Check if there's a CLR-based undo_next_lsn
        ~start_lsn = M analysis.txn_table.get(&txn_id) {
            Some(entry) => {
                I entry.undo_next_lsn != NULL_LSN {
                    entry.undo_next_lsn
                } E {
                    entry.last_lsn
                }
            },
            None => last_lsn,
        };
        undo_progress.insert(txn_id, start_lsn);
    }

    # Process each active transaction
    L &(txn_id, _): &active_txns {
        M undo_progress.get(&txn_id) {
            Some(&current_lsn) => {
                ~count = undo_single_transaction(
                    wal_dir, txn_id, current_lsn, writer, pool
                )?;

                records_undone += count;
                txns_undone += 1;

                # Write TXN_ABORT for this transaction
                ~abort_payload = TxnAbortPayload {};
                ~buf = ByteBuffer.with_capacity(4);
                abort_payload.serialize(&buf);
                writer.write_record(txn_id, TXN_ABORT, ENGINE_META, buf.as_bytes())?;
            },
            None => {},
        }
    }

    # Sync WAL after all undo operations
    writer.sync()?;

    Ok(UndoResult { txns_undone, records_undone })
}

# Undo a single transaction by walking its prev_lsn chain backward
F undo_single_transaction(
    wal_dir: &str,
    txn_id: u64,
    start_lsn: u64,
    writer: &~WalWriter,
    pool: &~BufferPool,
) -> Result<u64, VaisError> {
    ~records_undone: u64 = 0;
    ~current_lsn = start_lsn;

    W current_lsn != NULL_LSN {
        # Read the WAL record at current_lsn
        ~(header, payload) = read_record_at_lsn(wal_dir, current_lsn)?;

        # Skip CLR records (follow their undo_next_lsn instead)
        I header.record_type == CLR {
            ~buf = ByteBuffer.wrap_readonly(&payload);
            ~clr = ClrPayload.deserialize(&buf)?;
            current_lsn = clr.undo_next_lsn;
        } E {
            # Determine the next LSN to undo (prev_lsn of current record)
            ~next_undo_lsn = header.prev_lsn;

            # Apply undo for this record and write CLR
            ~undone = undo_single_record(
                &header, &payload, txn_id, next_undo_lsn, writer, pool
            )?;

            I undone {
                records_undone += 1;
            }

            current_lsn = next_undo_lsn;
        }
    }

    Ok(records_undone)
}

# Read a WAL record at a specific LSN
F read_record_at_lsn(
    wal_dir: &str,
    lsn: u64,
) -> Result<(WalRecordHeader, Vec<u8>), VaisError> {
    ~iter = WalRecordIterator.from_lsn(wal_dir.to_string(), lsn)?;
    M iter.next()? {
        Some((header, payload)) => {
            I header.lsn == lsn {
                Ok((header, payload))
            } E {
                Err(err_internal("WAL record at expected LSN not found"))
            }
        },
        None => Err(err_internal("WAL record not found at LSN")),
    }
}

# Undo a single WAL record and write a CLR
# Returns true if undo was performed, false if record type doesn't need undo
F undo_single_record(
    header: &WalRecordHeader,
    payload: &[u8],
    txn_id: u64,
    undo_next_lsn: u64,
    writer: &~WalWriter,
    pool: &~BufferPool,
) -> Result<bool, VaisError> {
    # Extract page info from the record for CLR
    ~(file_id, page_id) = extract_page_info(payload);

    M header.record_type {
        TUPLE_INSERT => {
            # Undo insert: mark tuple as deleted
            undo_tuple_insert(pool, payload)?;
        },
        TUPLE_DELETE => {
            # Undo delete: restore tuple
            undo_tuple_delete(pool, payload)?;
        },
        TUPLE_UPDATE => {
            # Undo update: restore old values
            undo_tuple_update(pool, payload)?;
        },
        BTREE_INSERT => {
            # Undo B+Tree insert: remove the key
            undo_btree_insert(pool, payload)?;
        },
        BTREE_DELETE => {
            # Undo B+Tree delete: re-insert the key
            undo_btree_delete(pool, payload)?;
        },
        BTREE_SPLIT => {
            # Undo split: merge pages back
            undo_btree_split(pool, payload)?;
        },
        BTREE_MERGE => {
            # Undo merge: re-split pages
            undo_btree_merge(pool, payload)?;
        },
        PAGE_ALLOC => {
            # Undo allocation: deallocate the page
            undo_page_alloc(pool, payload)?;
        },
        PAGE_DEALLOC => {
            # Undo deallocation: re-allocate the page
            undo_page_dealloc(pool, payload)?;
        },
        _ => {
            # Record type doesn't need undo (CLR, TXN_BEGIN, etc.)
            R Ok(false);
        },
    }

    # Write CLR (Compensation Log Record)
    ~clr_payload = ClrPayload {
        original_record_type: header.record_type,
        undo_next_lsn,
        file_id,
        page_id,
    };
    ~clr_buf = ByteBuffer.with_capacity(16);
    clr_payload.serialize(&clr_buf);
    writer.write_record(txn_id, CLR, ENGINE_META, clr_buf.as_bytes())?;

    Ok(true)
}

# Extract file_id and page_id from a WAL record payload
F extract_page_info(payload: &[u8]) -> (u8, u32) {
    I payload.len() >= 5 {
        ~buf = ByteBuffer.wrap_readonly(payload);
        M (buf.get_u8(), buf.get_u32_le()) {
            (Ok(fid), Ok(pid)) => (fid, pid),
            _ => (0, 0),
        }
    } E {
        (0, 0)
    }
}

# Undo handlers for each record type
# These apply the inverse operation to restore the page to its previous state

# Undo TUPLE_INSERT: mark the inserted tuple's slot as dead (tombstone)
# This effectively removes the tuple from the page so it won't be visible
F undo_tuple_insert(pool: &~BufferPool, payload: &[u8]) -> Result<(), VaisError> {
    ~buf = ByteBuffer.wrap_readonly(payload);
    ~p = TupleInsertPayload.deserialize(&buf)?;

    ~frame = pool.fetch_page(p.file_id, p.page_id)?;
    ~page_data = pool.get_page_mut(frame);
    ~heap = HeapPage.from_page_data(page_data, pool.page_size)?;

    # Mark the slot as dead to logically remove the inserted tuple
    heap.mark_slot_dead(p.slot)?;

    # Flush modified heap page back to frame
    ~flushed = heap.flush();
    L i: 0..flushed.len() {
        page_data[i] = flushed[i];
    }

    pool.unpin_page(frame, true);
    Ok(())
}

# Undo TUPLE_DELETE: restore the deleted tuple by clearing its txn_id_expire
# The old tuple data is stored in the WAL record for restoration
F undo_tuple_delete(pool: &~BufferPool, payload: &[u8]) -> Result<(), VaisError> {
    ~buf = ByteBuffer.wrap_readonly(payload);
    ~p = TupleDeletePayload.deserialize(&buf)?;

    ~frame = pool.fetch_page(p.file_id, p.page_id)?;
    ~page_data = pool.get_page_mut(frame);
    ~heap = HeapPage.from_page_data(page_data, pool.page_size)?;

    # Restore the tuple by clearing its expiration (set txn_id_expire = 0)
    # This makes the tuple visible again
    ~mvcc = MvccTupleMeta.new_insert(0, 0);  # Will be overwritten
    ~old_tuple = Tuple.read_from(&p.old_tuple, 0, p.old_tuple.len() as u64)?;
    # The old_tuple contains the original MVCC metadata with txn_id_expire = 0
    # We need to reset the MVCC expire fields on the page
    heap.update_mvcc(p.slot, &old_tuple.mvcc)?;

    ~flushed = heap.flush();
    L i: 0..flushed.len() {
        page_data[i] = flushed[i];
    }

    pool.unpin_page(frame, true);
    Ok(())
}

# Undo TUPLE_UPDATE: restore old tuple values
# The WAL record contains both old and new tuple data
F undo_tuple_update(pool: &~BufferPool, payload: &[u8]) -> Result<(), VaisError> {
    ~buf = ByteBuffer.wrap_readonly(payload);
    ~p = TupleUpdatePayload.deserialize(&buf)?;

    ~frame = pool.fetch_page(p.file_id, p.page_id)?;
    ~page_data = pool.get_page_mut(frame);
    ~heap = HeapPage.from_page_data(page_data, pool.page_size)?;

    # Step 1: Remove the new version that was inserted during the UPDATE
    # The update added a new tuple version; we need to find and remove it
    # For MVCC-style updates: the old tuple was marked expired, the new was inserted
    # Undo: un-expire old tuple, remove new tuple

    # Restore old tuple's MVCC metadata (clear expiration)
    ~old_tuple = Tuple.read_from(&p.old_tuple, 0, p.old_tuple.len() as u64)?;
    heap.update_mvcc(p.slot, &old_tuple.mvcc)?;

    # The new version tuple was inserted as the latest slot
    # Mark it as dead to remove it
    ~last_slot = heap.item_count() - 1;
    I last_slot > p.slot {
        heap.mark_slot_dead(last_slot)?;
    }

    ~flushed = heap.flush();
    L i: 0..flushed.len() {
        page_data[i] = flushed[i];
    }

    pool.unpin_page(frame, true);
    Ok(())
}

# Undo BTREE_INSERT: remove the key that was inserted
F undo_btree_insert(pool: &~BufferPool, payload: &[u8]) -> Result<(), VaisError> {
    ~buf = ByteBuffer.wrap_readonly(payload);
    ~p = BtreeInsertPayload.deserialize(&buf)?;

    ~frame = pool.fetch_page(p.file_id, p.page_id)?;
    ~page_data = pool.get_page_mut(frame);

    # Remove the key from the B+Tree leaf page
    btree_page_delete_key(page_data, &p.key)?;

    pool.unpin_page(frame, true);
    Ok(())
}

# Undo BTREE_DELETE: re-insert the deleted key
F undo_btree_delete(pool: &~BufferPool, payload: &[u8]) -> Result<(), VaisError> {
    ~buf = ByteBuffer.wrap_readonly(payload);
    ~p = BtreeDeletePayload.deserialize(&buf)?;

    ~frame = pool.fetch_page(p.file_id, p.page_id)?;
    ~page_data = pool.get_page_mut(frame);

    # Re-insert the key-value pair
    btree_page_insert_key(page_data, &p.key, &p.old_value)?;

    pool.unpin_page(frame, true);
    Ok(())
}

# Undo BTREE_SPLIT: merge the split pages back together
# This is the reverse of a split: move entries from new_page back to original
F undo_btree_split(pool: &~BufferPool, payload: &[u8]) -> Result<(), VaisError> {
    ~buf = ByteBuffer.wrap_readonly(payload);
    ~p = BtreeSplitPayload.deserialize(&buf)?;

    # Read entries from new page and merge back into original
    ~frame_old = pool.fetch_page(p.file_id, p.page_id)?;
    ~frame_new = pool.fetch_page(p.file_id, p.new_page_id)?;

    ~old_data = pool.get_page_mut(frame_old);
    ~new_data = pool.get_page(frame_new);

    # Merge new page entries back into old page
    btree_page_redo_merge(old_data, new_data)?;

    pool.unpin_page(frame_old, true);
    pool.unpin_page(frame_new, false);

    # Deallocate the new page that was created during split
    pool.deallocate_page(p.file_id, p.new_page_id)?;

    Ok(())
}

# Undo BTREE_MERGE: re-split the merged page
# This is the reverse of a merge: split entries back into two pages
F undo_btree_merge(pool: &~BufferPool, payload: &[u8]) -> Result<(), VaisError> {
    ~buf = ByteBuffer.wrap_readonly(payload);
    ~p = BtreeMergePayload.deserialize(&buf)?;

    # Re-allocate the sibling page and split entries back
    pool.allocate_specific_page(p.file_id, p.sibling_id)?;

    ~frame_main = pool.fetch_page(p.file_id, p.page_id)?;
    ~frame_sib = pool.fetch_page(p.file_id, p.sibling_id)?;

    ~main_data = pool.get_page_mut(frame_main);
    ~sib_data = pool.get_page_mut(frame_sib);

    # Split entries: move the second half back to sibling
    ~header = PageHeader.read_from_page(main_data)?;
    ~total_count = header.item_count;
    ~keep_count = total_count / 2;
    ~move_count = total_count - keep_count;

    # Copy key directory entries from main to sibling
    ~key_dir_start: u32 = 48;  # PAGE_HEADER_SIZE
    ~entry_size: u32 = 8;
    L i: 0..move_count {
        ~src_off = key_dir_start + ((keep_count + i) as u32) * entry_size;
        ~dst_off = key_dir_start + (i as u32) * entry_size;
        L j: 0..entry_size {
            sib_data[(dst_off + j) as u64] = main_data[(src_off + j) as u64];
        }
    }

    set_item_count(main_data, keep_count);
    set_item_count(sib_data, move_count);

    pool.unpin_page(frame_main, true);
    pool.unpin_page(frame_sib, true);

    Ok(())
}

# B+Tree page-level helpers for undo (shared with redo.vais)
# These are duplicated here to avoid circular imports; in a real compiler
# these would be in a shared utility module

# Insert a key-value pair into a B+Tree leaf page
F btree_page_insert_key(page_data: &~[u8], key: &[u8], value: &[u8]) -> Result<(), VaisError> {
    ~header = PageHeader.read_from_page(page_data)?;
    ~item_count = header.item_count;
    ~key_dir_start: u32 = 48;  # PAGE_HEADER_SIZE
    ~entry_size: u32 = 8;

    ~insert_pos = item_count as u32;
    L i: 0..item_count {
        ~entry_off = key_dir_start + (i as u32) * entry_size;
        ~existing_key_off = read_u32_le(page_data, entry_off);
        ~existing_key_len = read_u32_le(page_data, entry_off + 4);
        ~existing_key = &page_data[existing_key_off as u64..(existing_key_off + existing_key_len) as u64];
        I key < existing_key {
            insert_pos = i as u32;
        }
    }

    ~data_offset = header.free_space_offset as u32;
    ~total_kv_len = key.len() as u32 + value.len() as u32;
    L i: 0..key.len() {
        page_data[data_offset as u64 + i] = key[i];
    }
    L i: 0..value.len() {
        page_data[(data_offset + key.len() as u32) as u64 + i] = value[i];
    }

    ~i = item_count as u32;
    W i > insert_pos {
        ~src_off = key_dir_start + (i - 1) * entry_size;
        ~dst_off = key_dir_start + i * entry_size;
        L j: 0..entry_size {
            page_data[(dst_off + j) as u64] = page_data[(src_off + j) as u64];
        }
        i -= 1;
    }

    ~entry_off = key_dir_start + insert_pos * entry_size;
    write_u32_le(page_data, entry_off, data_offset);
    write_u32_le(page_data, entry_off + 4, total_kv_len);

    set_item_count(page_data, item_count + 1);
    set_free_space_offset(page_data, (data_offset + total_kv_len) as u16);
    Ok(())
}

# Delete a key from a B+Tree leaf page
F btree_page_delete_key(page_data: &~[u8], key: &[u8]) -> Result<(), VaisError> {
    ~header = PageHeader.read_from_page(page_data)?;
    ~item_count = header.item_count;
    ~key_dir_start: u32 = 48;
    ~entry_size: u32 = 8;

    ~found_pos: i32 = -1;
    L i: 0..item_count {
        ~entry_off = key_dir_start + (i as u32) * entry_size;
        ~existing_key_off = read_u32_le(page_data, entry_off);
        ~existing_key_len = read_u32_le(page_data, entry_off + 4);
        ~existing_key = &page_data[existing_key_off as u64..(existing_key_off + existing_key_len) as u64];
        I key == existing_key && found_pos == -1 {
            found_pos = i as i32;
        }
    }

    I found_pos >= 0 {
        ~pos = found_pos as u32;
        L i: pos..(item_count as u32 - 1) {
            ~src_off = key_dir_start + (i + 1) * entry_size;
            ~dst_off = key_dir_start + i * entry_size;
            L j: 0..entry_size {
                page_data[(dst_off + j) as u64] = page_data[(src_off + j) as u64];
            }
        }
        set_item_count(page_data, item_count - 1);
    }
    Ok(())
}

# Merge entries from sibling page into target page
F btree_page_redo_merge(
    target_page: &~[u8],
    sibling_page: &[u8],
) -> Result<(), VaisError> {
    ~target_header = PageHeader.read_from_page(target_page)?;
    ~sib_header = PageHeader.read_from_page(sibling_page)?;
    ~key_dir_start: u32 = 48;
    ~entry_size: u32 = 8;
    ~target_count = target_header.item_count;
    ~sib_count = sib_header.item_count;

    L i: 0..sib_count {
        ~src_off = key_dir_start + (i as u32) * entry_size;
        ~dst_off = key_dir_start + ((target_count + i) as u32) * entry_size;
        L j: 0..entry_size {
            target_page[(dst_off + j) as u64] = sibling_page[(src_off + j) as u64];
        }
    }
    set_item_count(target_page, target_count + sib_count);
    Ok(())
}

# Helper: read a u32 little-endian from page data
F read_u32_le(data: &[u8], offset: u32) -> u32 {
    ~b0 = data[offset as u64] as u32;
    ~b1 = data[(offset + 1) as u64] as u32;
    ~b2 = data[(offset + 2) as u64] as u32;
    ~b3 = data[(offset + 3) as u64] as u32;
    b0 | (b1 << 8) | (b2 << 16) | (b3 << 24)
}

# Helper: write a u32 little-endian to page data
F write_u32_le(data: &~[u8], offset: u32, value: u32) {
    data[offset as u64] = (value & 0xFF) as u8;
    data[(offset + 1) as u64] = ((value >> 8) & 0xFF) as u8;
    data[(offset + 2) as u64] = ((value >> 16) & 0xFF) as u8;
    data[(offset + 3) as u64] = ((value >> 24) & 0xFF) as u8;
}

# Helper: write a u16 little-endian to page data
F write_u16_le(data: &~[u8], offset: u32, value: u16) {
    data[offset as u64] = (value & 0xFF) as u8;
    data[(offset + 1) as u64] = ((value >> 8) & 0xFF) as u8;
}

# Page header field setters (write directly to page byte array)
# Layout: page_lsn(u64@0), txn_id(u64@8), page_id(u32@16), checksum(u32@20),
# prev_page(u32@24), next_page(u32@28), overflow_page(u32@32),
# free_space_offset(u16@36), item_count(u16@38)

F set_item_count(page_data: &~[u8], count: u16) {
    write_u16_le(page_data, 38, count);
}

F set_free_space_offset(page_data: &~[u8], offset: u16) {
    write_u16_le(page_data, 36, offset);
}

F undo_page_alloc(pool: &~BufferPool, payload: &[u8]) -> Result<(), VaisError> {
    ~buf = ByteBuffer.wrap_readonly(payload);
    ~file_id = buf.get_u8()?;
    ~page_id = buf.get_u32_le()?;
    pool.deallocate_page(file_id, page_id)?;
    Ok(())
}

F undo_page_dealloc(pool: &~BufferPool, payload: &[u8]) -> Result<(), VaisError> {
    ~buf = ByteBuffer.wrap_readonly(payload);
    ~file_id = buf.get_u8()?;
    ~page_id = buf.get_u32_le()?;
    pool.allocate_specific_page(file_id, page_id)?;
    Ok(())
}
