# B+Tree Node Page Format
# Internal nodes (BTREE_INTERNAL 0x11) and leaf nodes (BTREE_LEAF 0x12)
# Slotted structure with sorted key order
# Based on Stage 1 Section 9

U std/bytes.{ByteBuffer};
U storage/constants.{
    PAGE_HEADER_SIZE, BTREE_INTERNAL_ENTRY_SIZE, BTREE_LEAF_ENTRY_SIZE,
    BTREE_LEFTMOST_CHILD_SIZE, FLAG_IS_LEAF, FLAG_IS_ROOT,
    FLAG_IS_COMPRESSED, NULL_PAGE,
};
U storage/error.{VaisError, err_internal, err_page_full};
U storage/page/header.{PageHeader};
U storage/page/types.{PAGE_TYPE_BTREE_INTERNAL, PAGE_TYPE_BTREE_LEAF, ENGINE_TAG_SQL};
U storage/page/flags.{PageFlags};
U storage/btree/entry.{BTreeInternalEntry, BTreeLeafEntry, compare_keys, KeyCmp, key_less_than};
U storage/btree/prefix.{CompressedKey, compress_keys_with_restarts, decompress_keys};
U storage/checksum.{calculate_page_checksum};

# B+Tree Internal Node
S BTreeInternalNode {
    page_size: u32,
    header: PageHeader,
    leftmost_child: u32,                # Page ID of leftmost child
    entries: Vec<BTreeInternalEntry>,   # Sorted key directory entries
    key_data: Vec<u8>,                  # Key data area (variable length keys)
    data: Vec<u8>,                      # Full page buffer
}

X BTreeInternalNode {
    # Create a new empty internal node
    F new(page_id: u32, page_size: u32, is_root: bool) -> BTreeInternalNode {
        ~header = PageHeader.new(page_id, PAGE_TYPE_BTREE_INTERNAL, ENGINE_TAG_SQL);
        ~flags = PageFlags.new();
        if is_root { flags.set_root(true); }
        header.set_flags(flags);
        header.free_space_offset = (PAGE_HEADER_SIZE + BTREE_LEFTMOST_CHILD_SIZE) as u16;

        ~data = Vec.with_capacity(page_size as usize);
        data.resize(page_size as usize, 0u8);

        BTreeInternalNode {
            page_size,
            header,
            leftmost_child: NULL_PAGE,
            entries: Vec.new(),
            key_data: Vec.new(),
            data,
        }
    }

    # Load from page data
    # Handles both compressed and uncompressed pages transparently
    F from_page_data(page_data: &[u8], page_size: u32) -> Result<BTreeInternalNode, VaisError> {
        ~header = PageHeader.read_from_page(page_data)?;
        ~buf = ByteBuffer.wrap_readonly(&page_data[PAGE_HEADER_SIZE as usize..]);

        # Read leftmost_child (first 4 bytes of body)
        ~leftmost_child = buf.get_u32_le()?;

        # Read entry directory
        ~entries = Vec.with_capacity(header.item_count as usize);
        for _ in 0..header.item_count {
            entries.push(BTreeInternalEntry.deserialize(&buf)?);
        }

        ~is_compressed = header.get_flags().is_compressed();

        if !is_compressed || header.item_count == 0 {
            return Ok(BTreeInternalNode {
                page_size,
                header,
                leftmost_child,
                entries,
                key_data: Vec.new(),
                data: page_data.to_vec(),
            });
        }

        # Compressed page: decompress keys and rebuild data buffer
        ~compressed_keys = Vec.with_capacity(entries.len());
        for entry in &entries {
            ~ck_buf = ByteBuffer.wrap_readonly(&page_data[entry.key_off as usize..]);
            ~ck = CompressedKey.deserialize(&ck_buf)?;
            compressed_keys.push(ck);
        }

        ~full_keys = decompress_keys(&compressed_keys);

        # Rebuild page data with decompressed keys
        ~new_data = Vec.with_capacity(page_size as usize);
        new_data.resize(page_size as usize, 0u8);

        # Copy header + leftmost_child + entry directory region
        ~dir_end = (PAGE_HEADER_SIZE + BTREE_LEFTMOST_CHILD_SIZE
            + (entries.len() as u32) * BTREE_INTERNAL_ENTRY_SIZE) as usize;
        for i in 0..dir_end {
            new_data[i] = page_data[i];
        }

        # Write decompressed keys from bottom of page
        ~write_off = page_size;
        for i in 0..entries.len() {
            ~key = &full_keys[i];
            write_off -= key.len() as u32;
            for j in 0..key.len() {
                new_data[write_off as usize + j] = key[j];
            }
            entries[i].key_off = write_off as u16;
            entries[i].key_len = key.len() as u16;
        }

        # Clear compressed flag in-memory
        ~flags = header.get_flags();
        flags.set_compressed(false);
        header.set_flags(flags);

        Ok(BTreeInternalNode {
            page_size,
            header,
            leftmost_child,
            entries,
            key_data: Vec.new(),
            data: new_data,
        })
    }

    # Get key data for an entry from the page buffer
    F get_key(self, entry_index: usize) -> &[u8] {
        ~entry = &self.entries[entry_index];
        &self.data[entry.key_off as usize..(entry.key_off + entry.key_len) as usize]
    }

    # Binary search for child page containing the given key
    # Returns the child page_id to descend to
    F find_child(self, key: &[u8]) -> u32 {
        if self.entries.is_empty() {
            return self.leftmost_child;
        }

        # Binary search for smallest key_i where key < key_i
        ~lo: usize = 0;
        ~hi: usize = self.entries.len();

        while lo < hi {
            ~mid = lo + (hi - lo) / 2;
            ~mid_key = self.get_key(mid);
            M compare_keys(key, mid_key) {
                KeyCmp.Less => { hi = mid; },
                KeyCmp.Equal | KeyCmp.Greater => { lo = mid + 1; },
            }
        }

        # lo is now the index of first key > search_key
        if lo == 0 {
            self.leftmost_child
        } else {
            self.entries[lo - 1].child_page
        }
    }

    # Calculate available space for new entries
    F available_space(self) -> u32 {
        ~dir_end = PAGE_HEADER_SIZE + BTREE_LEFTMOST_CHILD_SIZE
            + (self.entries.len() as u32) * BTREE_INTERNAL_ENTRY_SIZE;
        ~key_data_start = self.find_key_data_start();
        if key_data_start <= dir_end { 0 }
        else { key_data_start - dir_end }
    }

    # Check if node can fit a new key
    F can_fit(self, key_len: u32) -> bool {
        self.available_space() >= BTREE_INTERNAL_ENTRY_SIZE + key_len
    }

    # Insert a separator key and child pointer
    F insert_entry(~self, key: &[u8], child_page: u32) -> Result<(), VaisError> {
        ~needed = BTREE_INTERNAL_ENTRY_SIZE + key.len() as u32;
        if !self.can_fit(key.len() as u32) {
            return Err(err_page_full(self.header.page_id, needed));
        }

        # Find insertion position (maintain sorted order)
        ~pos: usize = 0;
        for i in 0..self.entries.len() {
            if key_less_than(key, self.get_key(i)) {
                break;
            }
            pos = i + 1;
        }

        # Write key data at the end of page (growing from bottom)
        ~key_offset = self.find_key_data_start() - key.len() as u32;
        for i in 0..key.len() {
            self.data[key_offset as usize + i] = key[i];
        }

        ~entry = BTreeInternalEntry.new(key_offset as u16, key.len() as u16, child_page);
        self.entries.insert(pos, entry);
        self.header.item_count = self.entries.len() as u16;

        Ok(())
    }

    # Get entry count
    F entry_count(self) -> u16 {
        self.entries.len() as u16
    }

    # Set leftmost child
    F set_leftmost_child(~self, page_id: u32) {
        self.leftmost_child = page_id;
    }

    # Serialize to page buffer with prefix compression
    F flush(~self) -> &[u8] {
        # Clear and rebuild page buffer
        self.data.fill(0u8);

        if self.entries.is_empty() {
            # No entries: just write header + leftmost_child
            ~buf = ByteBuffer.wrap(&self.data);
            self.header.serialize(&buf);
            buf.put_u32_le(self.leftmost_child);
            PageHeader.update_checksum(&self.data);
            return &self.data;
        }

        # Extract sorted keys for compression
        ~keys_owned = Vec.with_capacity(self.entries.len());
        for i in 0..self.entries.len() {
            keys_owned.push(self.get_key(i).to_vec());
        }

        ~key_refs: Vec<&[u8]> = Vec.with_capacity(keys_owned.len());
        for k in &keys_owned {
            key_refs.push(k.as_slice());
        }

        # Compress keys with restart points
        ~(compressed, restart_points) = compress_keys_with_restarts(&key_refs);

        ~compressed_data_size: u32 = 0;
        for ck in &compressed {
            compressed_data_size += ck.serialized_size();
        }
        ~restart_header_size: u32 = 2 + (restart_points.len() as u32) * 2;

        ~dir_end = PAGE_HEADER_SIZE + BTREE_LEFTMOST_CHILD_SIZE
            + (self.entries.len() as u32) * BTREE_INTERNAL_ENTRY_SIZE;
        ~bottom_region = restart_header_size + compressed_data_size;

        if dir_end + bottom_region > self.page_size {
            # Fall back to uncompressed
            ~buf = ByteBuffer.wrap(&self.data);
            ~flags = self.header.get_flags();
            flags.set_compressed(false);
            self.header.set_flags(flags);
            self.header.serialize(&buf);
            buf.put_u32_le(self.leftmost_child);
            ~write_off = self.page_size;
            for i in 0..self.entries.len() {
                ~key = &keys_owned[i];
                write_off -= key.len() as u32;
                for j in 0..key.len() {
                    self.data[write_off as usize + j] = key[j];
                }
                self.entries[i].key_off = write_off as u16;
                self.entries[i].key_len = key.len() as u16;
            }
            for entry in &self.entries {
                entry.serialize(&buf);
            }
            PageHeader.update_checksum(&self.data);
            return &self.data;
        }

        # Set compressed flag
        ~flags = self.header.get_flags();
        flags.set_compressed(true);
        self.header.set_flags(flags);

        # Write compressed key data from bottom
        ~write_off = self.page_size;
        ~compressed_offsets = Vec.with_capacity(compressed.len());
        for i in (0..compressed.len()).rev() {
            ~ck = &compressed[i];
            ~ck_size = ck.serialized_size();
            write_off -= ck_size;
            compressed_offsets.push(write_off);
            ~ck_buf = ByteBuffer.wrap(&self.data[write_off as usize..]);
            ck.serialize(&ck_buf);
        }
        compressed_offsets.reverse();

        # Write restart points
        write_off -= restart_header_size;
        ~rp_buf = ByteBuffer.wrap(&self.data[write_off as usize..]);
        rp_buf.put_u16_le(restart_points.len() as u16);
        for rp in &restart_points {
            rp_buf.put_u16_le(*rp);
        }

        # Update entry directory
        for i in 0..self.entries.len() {
            self.entries[i].key_off = compressed_offsets[i] as u16;
            self.entries[i].key_len = compressed[i].serialized_size() as u16;
        }

        # Write header, leftmost_child, and entry directory
        ~buf = ByteBuffer.wrap(&self.data);
        self.header.serialize(&buf);
        buf.put_u32_le(self.leftmost_child);
        for entry in &self.entries {
            entry.serialize(&buf);
        }

        PageHeader.update_checksum(&self.data);
        &self.data
    }

    # Helper: find start of key data area (lowest key offset)
    F find_key_data_start(self) -> u32 {
        ~min_off = self.page_size;
        for entry in &self.entries {
            if (entry.key_off as u32) < min_off {
                min_off = entry.key_off as u32;
            }
        }
        min_off
    }

    F is_root(self) -> bool {
        self.header.get_flags().is_root()
    }
}

# B+Tree Leaf Node
S BTreeLeafNode {
    page_size: u32,
    header: PageHeader,
    entries: Vec<BTreeLeafEntry>,       # Sorted entry directory
    data: Vec<u8>,                      # Full page buffer
}

X BTreeLeafNode {
    # Create a new empty leaf node
    F new(page_id: u32, page_size: u32, is_root: bool) -> BTreeLeafNode {
        ~header = PageHeader.new(page_id, PAGE_TYPE_BTREE_LEAF, ENGINE_TAG_SQL);
        ~flags = PageFlags.new();
        flags.set_leaf(true);
        if is_root { flags.set_root(true); }
        header.set_flags(flags);
        header.free_space_offset = PAGE_HEADER_SIZE as u16;

        ~data = Vec.with_capacity(page_size as usize);
        data.resize(page_size as usize, 0u8);

        BTreeLeafNode {
            page_size,
            header,
            entries: Vec.new(),
            data,
        }
    }

    # Load from page data
    # Handles both compressed and uncompressed pages transparently
    F from_page_data(page_data: &[u8], page_size: u32) -> Result<BTreeLeafNode, VaisError> {
        ~header = PageHeader.read_from_page(page_data)?;
        ~buf = ByteBuffer.wrap_readonly(&page_data[PAGE_HEADER_SIZE as usize..]);

        ~entries = Vec.with_capacity(header.item_count as usize);
        for _ in 0..header.item_count {
            entries.push(BTreeLeafEntry.deserialize(&buf)?);
        }

        ~is_compressed = header.get_flags().is_compressed();

        if !is_compressed || header.item_count == 0 {
            # Uncompressed: entries already have correct key_off/key_len
            return Ok(BTreeLeafNode {
                page_size,
                header,
                entries,
                data: page_data.to_vec(),
            });
        }

        # Compressed page: decompress keys and rebuild data buffer
        # Read compressed keys from the entry offsets
        ~compressed_keys = Vec.with_capacity(entries.len());
        for entry in &entries {
            ~ck_buf = ByteBuffer.wrap_readonly(&page_data[entry.key_off as usize..]);
            ~ck = CompressedKey.deserialize(&ck_buf)?;
            compressed_keys.push(ck);
        }

        # Decompress all keys
        ~full_keys = decompress_keys(&compressed_keys);

        # Rebuild page data with decompressed keys (bottom-up layout)
        ~new_data = Vec.with_capacity(page_size as usize);
        new_data.resize(page_size as usize, 0u8);

        # Copy original data up to entry directory end
        ~dir_end = (PAGE_HEADER_SIZE + (entries.len() as u32) * BTREE_LEAF_ENTRY_SIZE) as usize;
        for i in 0..dir_end {
            new_data[i] = page_data[i];
        }

        # Write decompressed keys from bottom of page, update entries
        ~write_off = page_size;
        for i in 0..entries.len() {
            ~key = &full_keys[i];
            write_off -= key.len() as u32;
            for j in 0..key.len() {
                new_data[write_off as usize + j] = key[j];
            }
            entries[i].key_off = write_off as u16;
            entries[i].key_len = key.len() as u16;
        }

        # Clear compressed flag in the in-memory header
        ~flags = header.get_flags();
        flags.set_compressed(false);
        header.set_flags(flags);

        Ok(BTreeLeafNode {
            page_size,
            header,
            entries,
            data: new_data,
        })
    }

    # Get key data for an entry
    F get_key(self, entry_index: usize) -> &[u8] {
        ~entry = &self.entries[entry_index];
        &self.data[entry.key_off as usize..(entry.key_off + entry.key_len) as usize]
    }

    # Search for key, returns entry index or None
    F search(self, key: &[u8]) -> Option<usize> {
        ~lo: usize = 0;
        ~hi: usize = self.entries.len();

        while lo < hi {
            ~mid = lo + (hi - lo) / 2;
            ~mid_key = self.get_key(mid);
            M compare_keys(key, mid_key) {
                KeyCmp.Less => { hi = mid; },
                KeyCmp.Equal => { return Some(mid); },
                KeyCmp.Greater => { lo = mid + 1; },
            }
        }
        None
    }

    # Find first entry >= key (for range scan start)
    F lower_bound(self, key: &[u8]) -> usize {
        ~lo: usize = 0;
        ~hi: usize = self.entries.len();

        while lo < hi {
            ~mid = lo + (hi - lo) / 2;
            ~mid_key = self.get_key(mid);
            M compare_keys(key, mid_key) {
                KeyCmp.Less | KeyCmp.Equal => { hi = mid; },
                KeyCmp.Greater => { lo = mid + 1; },
            }
        }
        lo  # First entry >= key
    }

    # Calculate available space
    F available_space(self) -> u32 {
        ~dir_end = PAGE_HEADER_SIZE + (self.entries.len() as u32) * BTREE_LEAF_ENTRY_SIZE;
        ~key_data_start = self.find_key_data_start();
        if key_data_start <= dir_end { 0 }
        else { key_data_start - dir_end }
    }

    # Check if leaf can fit a new key
    F can_fit(self, key_len: u32) -> bool {
        self.available_space() >= BTREE_LEAF_ENTRY_SIZE + key_len
    }

    # Insert a key-TID pair
    F insert(~self, key: &[u8], tid: u32) -> Result<(), VaisError> {
        if !self.can_fit(key.len() as u32) {
            return Err(err_page_full(self.header.page_id, BTREE_LEAF_ENTRY_SIZE + key.len() as u32));
        }

        # Find insertion position
        ~pos = self.lower_bound(key);

        # Write key data from bottom of page
        ~key_offset = self.find_key_data_start() - key.len() as u32;
        for i in 0..key.len() {
            self.data[key_offset as usize + i] = key[i];
        }

        ~entry = BTreeLeafEntry.new(key_offset as u16, key.len() as u16, tid);
        self.entries.insert(pos, entry);
        self.header.item_count = self.entries.len() as u16;

        Ok(())
    }

    # Delete a key, returns the removed TID if found
    F delete(~self, key: &[u8]) -> Option<u32> {
        M self.search(key) {
            Some(idx) => {
                ~entry = self.entries.remove(idx);
                self.header.item_count = self.entries.len() as u16;
                Some(entry.tid)
            },
            None => None,
        }
    }

    # Get TID for a key
    F get_tid(self, key: &[u8]) -> Option<u32> {
        M self.search(key) {
            Some(idx) => Some(self.entries[idx].tid),
            None => None,
        }
    }

    # Leaf chain navigation
    F next_leaf(self) -> u32 { self.header.next_page }
    F prev_leaf(self) -> u32 { self.header.prev_page }
    F set_next_leaf(~self, page_id: u32) { self.header.next_page = page_id; }
    F set_prev_leaf(~self, page_id: u32) { self.header.prev_page = page_id; }
    F has_next_leaf(self) -> bool { self.header.next_page != NULL_PAGE }
    F has_prev_leaf(self) -> bool { self.header.prev_page != NULL_PAGE }

    # Entry count
    F entry_count(self) -> u16 { self.entries.len() as u16 }

    # Utilization (0.0 - 1.0)
    F utilization(self) -> f64 {
        ~used = (self.entries.len() as u32) * BTREE_LEAF_ENTRY_SIZE;
        ~total = self.page_size - PAGE_HEADER_SIZE;
        (used as f64) / (total as f64)
    }

    # Serialize to page buffer with prefix compression
    F flush(~self) -> &[u8] {
        # Clear and rebuild page buffer
        self.data.fill(0u8);

        if self.entries.is_empty() {
            # No entries: just write header, no compression needed
            ~buf = ByteBuffer.wrap(&self.data);
            self.header.serialize(&buf);
            PageHeader.update_checksum(&self.data);
            return &self.data;
        }

        # Extract sorted keys for compression
        ~keys_owned = Vec.with_capacity(self.entries.len());
        for i in 0..self.entries.len() {
            keys_owned.push(self.get_key(i).to_vec());
        }

        # Build key reference slice
        ~key_refs: Vec<&[u8]> = Vec.with_capacity(keys_owned.len());
        for k in &keys_owned {
            key_refs.push(k.as_slice());
        }

        # Compress keys with restart points
        ~(compressed, restart_points) = compress_keys_with_restarts(&key_refs);

        # Calculate compressed key data size
        ~compressed_data_size: u32 = 0;
        for ck in &compressed {
            compressed_data_size += ck.serialized_size();
        }
        # Restart points header: 2B count + N × 2B
        ~restart_header_size: u32 = 2 + (restart_points.len() as u32) * 2;

        # Layout: [Header][Entry Dir][...free space...][Restart Header][Compressed Keys (bottom-up)]
        ~dir_end = PAGE_HEADER_SIZE + (self.entries.len() as u32) * BTREE_LEAF_ENTRY_SIZE;
        ~bottom_region = restart_header_size + compressed_data_size;

        # Check if compression fits
        if dir_end + bottom_region > self.page_size {
            # Compression doesn't save space — fall back to uncompressed
            ~buf = ByteBuffer.wrap(&self.data);
            ~flags = self.header.get_flags();
            flags.set_compressed(false);
            self.header.set_flags(flags);
            self.header.serialize(&buf);
            # Write key data from bottom of page, update entries
            ~write_off = self.page_size;
            for i in 0..self.entries.len() {
                ~key = &keys_owned[i];
                write_off -= key.len() as u32;
                for j in 0..key.len() {
                    self.data[write_off as usize + j] = key[j];
                }
                self.entries[i].key_off = write_off as u16;
                self.entries[i].key_len = key.len() as u16;
            }
            for entry in &self.entries {
                entry.serialize(&buf);
            }
            PageHeader.update_checksum(&self.data);
            return &self.data;
        }

        # Set compressed flag
        ~flags = self.header.get_flags();
        flags.set_compressed(true);
        self.header.set_flags(flags);

        # Write compressed key data from bottom of page
        ~write_off = self.page_size;

        # Write compressed keys bottom-up (last compressed key at highest offset)
        # We iterate forward but write from bottom
        ~compressed_offsets = Vec.with_capacity(compressed.len());
        for i in (0..compressed.len()).rev() {
            ~ck = &compressed[i];
            ~ck_size = ck.serialized_size();
            write_off -= ck_size;
            compressed_offsets.push(write_off);
            # Serialize CompressedKey directly into data buffer
            ~ck_buf = ByteBuffer.wrap(&self.data[write_off as usize..]);
            ck.serialize(&ck_buf);
        }
        compressed_offsets.reverse();

        # Write restart points above compressed keys
        write_off -= restart_header_size;
        ~rp_buf = ByteBuffer.wrap(&self.data[write_off as usize..]);
        rp_buf.put_u16_le(restart_points.len() as u16);
        for rp in &restart_points {
            rp_buf.put_u16_le(*rp);
        }

        # Update entry directory: key_off/key_len point into compressed region
        # For compressed pages, key_off points to the CompressedKey, key_len = serialized size
        for i in 0..self.entries.len() {
            self.entries[i].key_off = compressed_offsets[i] as u16;
            self.entries[i].key_len = compressed[i].serialized_size() as u16;
        }

        # Write header and entry directory
        ~buf = ByteBuffer.wrap(&self.data);
        self.header.serialize(&buf);
        for entry in &self.entries {
            entry.serialize(&buf);
        }

        PageHeader.update_checksum(&self.data);
        &self.data
    }

    # Get median entry index (for split point)
    F median_index(self) -> usize {
        self.entries.len() / 2
    }

    # Helper: find start of key data
    F find_key_data_start(self) -> u32 {
        ~min_off = self.page_size;
        for entry in &self.entries {
            if (entry.key_off as u32) < min_off {
                min_off = entry.key_off as u32;
            }
        }
        min_off
    }

    F is_root(self) -> bool {
        self.header.get_flags().is_root()
    }
}
