# Undo Log Manager
# Writes undo entries to undo.vdb pages, manages append-only log
# Based on Stage 1 Section 10 and Stage 3 Section 1

U std/sync.{Mutex};
U std/bytes.{ByteBuffer};
U storage/constants.{
    FILE_ID_UNDO, PAGE_HEADER_SIZE, NULL_UNDO_PTR, PAGE_BODY_SIZE_8K, PAGE_BODY_SIZE_16K
};
U storage/error.{VaisError, err_internal, err_page_full};
U storage/bytes.{encode_undo_ptr, decode_undo_ptr};
U storage/txn/undo_entry.{UndoEntry};
U storage/buffer/pool.{BufferPool};
U storage/page/header.{PageHeader};
U storage/page/types.{PAGE_TYPE_UNDO_LOG};

# Undo log: append-only log of previous tuple versions
S UndoLog {
    file_id: u8,              # UNDO_FILE_ID = 4
    page_size: u32,
    current_page_id: u32,     # Current page being written to
    current_offset: u32,      # Write offset within current page
    lock: Mutex,
}

X UndoLog {
    # Create a new undo log
    F new(page_size: u32) -> UndoLog {
        UndoLog {
            file_id: FILE_ID_UNDO,
            page_size,
            current_page_id: 1,  # Page 0 is the file header
            current_offset: PAGE_HEADER_SIZE,  # Start after page header
            lock: Mutex.new(),
        }
    }

    # Write an undo entry and return its undo_ptr
    # The undo_ptr encodes (file_id, page_id, offset) for direct access
    F write_entry(~self, entry: &UndoEntry, pool: &~BufferPool) -> Result<u64, VaisError> {
        ~guard = self.lock.lock();

        ~entry_size = entry.serialized_size();
        ~available_space = self.available_space_in_current_page();

        # Check if we need a new page
        I entry_size > available_space {
            self.allocate_new_page(pool)?;
        }

        # Serialize the entry to a buffer
        ~buf = ByteBuffer.with_capacity(entry_size as u64);
        entry.serialize(&buf);
        ~entry_data = buf.as_bytes();

        # Fetch the current undo page
        ~frame_id = pool.fetch_page(self.file_id, self.current_page_id)?;
        ~frame = pool.get_frame_mut(frame_id);

        # Write entry data at current offset
        ~page_data = frame.get_data_mut();
        ~write_offset = self.current_offset as u64;
        page_data[write_offset..(write_offset + entry_size as u64)]
            .copy_from_slice(entry_data);

        # Update page header's free_space_offset and item_count
        ~header = PageHeader.read_from_page(page_data)?;
        header.free_space_offset = (self.current_offset + entry_size) as u16;
        header.item_count += 1;
        header.write_to_page(page_data);

        # Mark the page as dirty
        frame.mark_dirty();

        # Encode undo_ptr for this entry
        ~undo_ptr = encode_undo_ptr(
            self.file_id,
            self.current_page_id,
            self.current_offset as u16
        );

        # Advance current offset
        self.current_offset += entry_size;

        # Unpin the page
        pool.unpin_page(frame_id)?;

        drop(guard);

        Ok(undo_ptr)
    }

    # Read an undo entry by its undo_ptr
    F read_entry(self, undo_ptr: u64, pool: &~BufferPool) -> Result<UndoEntry, VaisError> {
        I undo_ptr == NULL_UNDO_PTR {
            R Err(err_internal("Cannot read null undo pointer"));
        }

        # Decode undo_ptr
        ~(file_id, page_id, offset) = decode_undo_ptr(undo_ptr);

        I file_id != self.file_id {
            R Err(err_internal("Invalid undo file_id: {file_id}, expected {self.file_id}"));
        }

        # Fetch the undo page
        ~frame_id = pool.fetch_page(file_id, page_id)?;
        ~frame = pool.get_frame(frame_id);
        ~page_data = frame.get_data();

        # Create a ByteBuffer starting at the offset
        ~buf = ByteBuffer.from_slice(&page_data[offset as u64..]);

        # Deserialize the entry
        ~entry = UndoEntry.deserialize(&buf)?;

        # Unpin the page
        pool.unpin_page(frame_id)?;

        Ok(entry)
    }

    # Read a chain of undo entries by following prev_undo_ptr links
    # Returns entries in reverse chronological order (newest first)
    F read_chain(self, undo_ptr: u64, pool: &~BufferPool) -> Result<Vec<UndoEntry>, VaisError> {
        ~entries = Vec.new();
        ~current_ptr = undo_ptr;

        W current_ptr != NULL_UNDO_PTR {
            ~entry = self.read_entry(current_ptr, pool)?;
            current_ptr = entry.prev_undo_ptr;
            entries.push(entry);
        }

        Ok(entries)
    }

    # Calculate available space in current page
    F available_space_in_current_page(self) -> u32 {
        ~page_body_size = self.page_size - PAGE_HEADER_SIZE;
        page_body_size - (self.current_offset - PAGE_HEADER_SIZE)
    }

    # Allocate a new undo page when current page is full
    F allocate_new_page(~self, pool: &~BufferPool) -> Result<(), VaisError> {
        # Get the next page ID
        # In a full implementation, this would allocate from the freelist
        # For now, we simply increment the page ID
        ~new_page_id = self.current_page_id + 1;

        # Initialize the new undo page with a header
        ~frame_id = pool.fetch_page(self.file_id, new_page_id)?;
        ~frame = pool.get_frame_mut(frame_id);
        ~page_data = frame.get_data_mut();

        # Initialize page header
        ~header = PageHeader.new(
            new_page_id,
            PAGE_TYPE_UNDO_LOG,
            0  # engine_tag: common
        );
        header.prev_page = self.current_page_id;  # Chain to previous page
        header.free_space_offset = PAGE_HEADER_SIZE as u16;
        header.item_count = 0;

        header.write_to_page(page_data);

        # Mark dirty
        frame.mark_dirty();

        # Unpin
        pool.unpin_page(frame_id)?;

        # Update current page tracking
        self.current_page_id = new_page_id;
        self.current_offset = PAGE_HEADER_SIZE;

        Ok(())
    }

    # Get current page ID (for debugging/testing)
    F get_current_page_id(self) -> u32 {
        self.current_page_id
    }

    # Get current write offset (for debugging/testing)
    F get_current_offset(self) -> u32 {
        self.current_offset
    }
}
