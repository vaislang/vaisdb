# Full-Text Tokenizer Pipeline
# Unicode-aware word splitting, lowercasing, stop word removal
# Configurable pipeline: enable/disable stop words, case-folding, position tracking

U std/vec.Vec;
U std/string.Str;
U fulltext/types.{TokenInfo, FullTextConfig, DEFAULT_MAX_TOKEN_LENGTH, DEFAULT_MIN_TOKEN_LENGTH};

# ============================================================================
# Stop Words — English default set (~174 common words)
# ============================================================================

L ENGLISH_STOP_WORDS: [Str; 174] = [
    "a", "about", "above", "after", "again", "against", "all", "am", "an",
    "and", "any", "are", "aren't", "as", "at", "be", "because", "been",
    "before", "being", "below", "between", "both", "but", "by", "can",
    "can't", "cannot", "could", "couldn't", "did", "didn't", "do", "does",
    "doesn't", "doing", "don't", "down", "during", "each", "few", "for",
    "from", "further", "get", "got", "had", "hadn't", "has", "hasn't",
    "have", "haven't", "having", "he", "he'd", "he'll", "he's", "her",
    "here", "here's", "hers", "herself", "him", "himself", "his", "how",
    "how's", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into", "is",
    "isn't", "it", "it's", "its", "itself", "let's", "me", "more", "most",
    "mustn't", "my", "myself", "no", "nor", "not", "of", "off", "on",
    "once", "only", "or", "other", "ought", "our", "ours", "ourselves",
    "out", "over", "own", "same", "shan't", "she", "she'd", "she'll",
    "she's", "should", "shouldn't", "so", "some", "such", "than", "that",
    "that's", "the", "their", "theirs", "them", "themselves", "then",
    "there", "there's", "these", "they", "they'd", "they'll", "they're",
    "they've", "this", "those", "through", "to", "too", "under", "until",
    "up", "very", "was", "wasn't", "we", "we'd", "we'll", "we're",
    "we've", "were", "weren't", "what", "what's", "when", "when's",
    "where", "where's", "which", "while", "who", "who's", "whom", "why",
    "why's", "will", "with", "won't", "would", "wouldn't", "you", "you'd",
    "you'll", "you're", "you've", "your", "yours", "yourself", "yourselves",
];

# ============================================================================
# Tokenizer — Configurable text tokenization pipeline
# ============================================================================

S Tokenizer {
    enable_stop_words: bool,
    enable_lowercase: bool,
    enable_positions: bool,
    max_token_length: u32,
    min_token_length: u32,
    stop_words: Vec<Str>,       # Sorted for binary search
}

X Tokenizer {
    # Create tokenizer from config
    F from_config(config: &FullTextConfig) -> Tokenizer {
        ~stop_words = Vec.new();
        if config.enable_stop_words {
            ~i: usize = 0;
            L while i < 174 {
                stop_words.push(ENGLISH_STOP_WORDS[i].clone());
                i += 1;
            }
            # Sort for binary search
            stop_words.sort();
        }

        Tokenizer {
            enable_stop_words: config.enable_stop_words,
            enable_lowercase: true,
            enable_positions: config.enable_positions,
            max_token_length: config.max_token_length,
            min_token_length: config.min_token_length,
            stop_words,
        }
    }

    # Create default tokenizer
    F default() -> Tokenizer {
        ~stop_words = Vec.new();
        ~i: usize = 0;
        L while i < 174 {
            stop_words.push(ENGLISH_STOP_WORDS[i].clone());
            i += 1;
        }
        stop_words.sort();

        Tokenizer {
            enable_stop_words: true,
            enable_lowercase: true,
            enable_positions: true,
            max_token_length: DEFAULT_MAX_TOKEN_LENGTH,
            min_token_length: DEFAULT_MIN_TOKEN_LENGTH,
            stop_words,
        }
    }

    # ========================================================================
    # Main tokenize function
    # ========================================================================

    # Tokenize text into a list of TokenInfo with position tracking
    F tokenize(self, text: &Str) -> Vec<TokenInfo> {
        ~tokens = Vec.new();
        ~bytes = text.as_bytes();
        ~len = bytes.len();
        ~pos: usize = 0;         # Current byte position
        ~token_position: u32 = 0; # Logical token position

        L while pos < len {
            # Skip non-word characters
            if !is_word_byte(bytes[pos]) {
                pos += 1;
                continue;
            }

            # Start of a word token
            ~start = pos;

            # Consume word characters
            L while pos < len && is_word_byte(bytes[pos]) {
                pos += 1;
            }

            ~end = pos;
            ~word_len = end - start;

            # Length filter
            if word_len < self.min_token_length as usize ||
               word_len > self.max_token_length as usize {
                continue;
            }

            # Extract and normalize token
            ~raw = text.substring(start, end);
            ~term = if self.enable_lowercase {
                to_ascii_lowercase(&raw)
            } else {
                raw
            };

            # Stop word filter
            if self.enable_stop_words && self.is_stop_word(&term) {
                # Still increment position for stop words (preserves phrase distances)
                token_position += 1;
                continue;
            }

            tokens.push(TokenInfo.new(
                term,
                token_position,
                start as u32,
                end as u32,
            ));
            token_position += 1;
        }

        tokens
    }

    # Tokenize and return only unique terms with their frequencies and positions
    # Returns Vec of (term, freq, positions) grouped by term
    F tokenize_with_freqs(self, text: &Str) -> Vec<TermFreqInfo> {
        ~tokens = self.tokenize(text);
        ~result: Vec<TermFreqInfo> = Vec.new();

        ~i: usize = 0;
        L while i < tokens.len() {
            ~found = false;
            ~j: usize = 0;
            L while j < result.len() {
                if result[j].term == tokens[i].term {
                    result[j].freq += 1;
                    result[j].positions.push(tokens[i].position);
                    found = true;
                    break;
                }
                j += 1;
            }
            if !found {
                ~positions = Vec.new();
                positions.push(tokens[i].position);
                result.push(TermFreqInfo {
                    term: tokens[i].term.clone(),
                    freq: 1,
                    positions,
                });
            }
            i += 1;
        }

        result
    }

    # ========================================================================
    # Stop word check (binary search on sorted list)
    # ========================================================================

    F is_stop_word(self, term: &Str) -> bool {
        if self.stop_words.is_empty() {
            return false;
        }

        ~lo: usize = 0;
        ~hi: usize = self.stop_words.len();

        L while lo < hi {
            ~mid = lo + (hi - lo) / 2;
            if self.stop_words[mid] < *term {
                lo = mid + 1;
            } else if self.stop_words[mid] > *term {
                hi = mid;
            } else {
                return true;
            }
        }

        false
    }
}

# ============================================================================
# TermFreqInfo — Aggregated term frequency + positions
# ============================================================================

S TermFreqInfo {
    term: Str,
    freq: u32,
    positions: Vec<u32>,
}

# ============================================================================
# Character classification helpers
# ============================================================================

# Check if byte is a word character (letter, digit, or underscore)
# Handles ASCII letters/digits plus basic Unicode letter detection via UTF-8
F is_word_byte(b: u8) -> bool {
    # ASCII letters
    if b >= 65 && b <= 90 { return true; }   # A-Z
    if b >= 97 && b <= 122 { return true; }  # a-z
    # Digits
    if b >= 48 && b <= 57 { return true; }   # 0-9
    # Underscore (treat as word char for identifiers)
    if b == 95 { return true; }              # _
    # High bytes (UTF-8 continuation/start bytes indicate non-ASCII letters)
    # We treat bytes >= 0x80 as part of words (Unicode letters, CJK, etc.)
    if b >= 128 { return true; }
    false
}

# Convert ASCII string to lowercase (non-ASCII bytes preserved as-is)
F to_ascii_lowercase(s: &Str) -> Str {
    ~bytes = s.as_bytes();
    ~result = Vec.with_capacity(bytes.len());
    ~i: usize = 0;
    L while i < bytes.len() {
        ~b = bytes[i];
        if b >= 65 && b <= 90 {
            # A-Z → a-z
            result.push(b + 32);
        } else {
            result.push(b);
        }
        i += 1;
    }
    Str.from_utf8(&result)
}
