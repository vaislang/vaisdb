# Phrase Search
# Implements phrase query support by verifying consecutive token positions
# Supports exact phrase matching and slop-based proximity search

U std/vec.Vec;
U std/string.Str;
U fulltext/types.{PostingEntry};
U fulltext/visibility.{is_posting_visible};
U storage/txn/snapshot.{Snapshot};
U storage/txn/clog.{Clog};
U fulltext/search/bm25.{BM25Scorer, ln};

# ============================================================================
# PhraseResult — Result of phrase search for a single document
# ============================================================================

S PhraseResult {
    doc_id: u64,        # Document containing the phrase M score: f64,         # Relevance score for this M match_count: u32,   # Number of times the phrase appears in this document
}

X PhraseResult {
    F new(doc_id: u64, score: f64, match_count: u32) -> PhraseResult {
        PhraseResult { doc_id, score, match_count }
    }
}

# ============================================================================
# PhraseSearcher — Stateless phrase search implementation
# ============================================================================

S PhraseSearcher {}

X PhraseSearcher {
    # Create new phrase searcher
    F new() -> PhraseSearcher {
        PhraseSearcher {}
    }

    # Search for a phrase in posting lists
    #
    # Algorithm:
    # 1. Find documents that appear in ALL term posting lists (intersection by doc_id)
    # 2. For each candidate document, verify positions are consecutive
    # 3. Allow configurable position gap (slop) for proximity matching
    # 4. Filter by MVCC visibility
    # 5. Score matches using phrase scoring formula
    #
    # Parameters:
    # - terms: ordered query terms forming the phrase
    # - posting_entries_per_term: posting lists for each term (parallel to terms)
    # - snapshot: MVCC snapshot for visibility checks
    # - clog: commit log for transaction status
    # - slop: maximum allowed gap between positions (0 = exact phrase)
    # - avg_doc_length: average document length in corpus (for scoring)
    # - total_docs: total number of documents in corpus (for scoring)
    #
    # Returns: Vec of PhraseResult sorted by score (descending)
    F search_phrase(
        self,
        terms: &Vec<Str>,
        posting_entries_per_term: &Vec<Vec<PostingEntry>>,
        snapshot: &Snapshot,
        clog: &Clog,
        slop: u32,
        avg_doc_length: f64,
        total_docs: u64,
    ) -> Vec<PhraseResult> {
        # Edge case: no terms or mismatched lengths
        I terms.len() == 0 || terms.len() != posting_entries_per_term.len() {
            R Vec.new();
        }

        # Edge case: single term (not a phrase, but handle gracefully)
        I terms.len() == 1 {
            R self.handle_single_term(
                &posting_entries_per_term[0],
                snapshot,
                clog,
                avg_doc_length,
                total_docs,
            );
        }

        # Step 1: Find documents appearing in ALL term posting lists
        ~candidate_docs = self.intersect_by_doc_id(posting_entries_per_term, snapshot, clog);

        # Step 2: For each candidate, verify phrase positions
        ~results = Vec.new();
        ~i: u64 = 0;
        L W i < candidate_docs.len() {
            ~doc_id = candidate_docs[i];

            # Collect positions for each term in this document
            ~positions_per_term = self.collect_positions_for_doc(
                doc_id,
                posting_entries_per_term,
                snapshot,
                clog,
            );

            # Verify consecutive positions with slop
            ~match_count = self.count_phrase_matches(&positions_per_term, slop);

            I match_count > 0 {
                # Calculate document length (sum of visible term frequencies)
                ~doc_length = self.calculate_doc_length(
                    doc_id,
                    posting_entries_per_term,
                    snapshot,
                    clog,
                );

                # Score this phrase M ~score = self.score_phrase(
                    match_count,
                    doc_length,
                    avg_doc_length,
                    total_docs,
                );

                results.push(PhraseResult.new(doc_id, score, match_count));
            }

            i += 1;
        }

        # Step 3: Sort results by score (descending)
        self.sort_results_by_score(&results);

        results
    }

    # Find documents appearing in ALL term posting lists (intersection)
    # Only considers visible postings
    F intersect_by_doc_id(
        self,
        posting_entries_per_term: &Vec<Vec<PostingEntry>>,
        snapshot: &Snapshot,
        clog: &Clog,
    ) -> Vec<u64> {
        I posting_entries_per_term.len() == 0 {
            R Vec.new();
        }

        # Start with doc_ids from first term (filtered by visibility)
        ~candidate_docs = Vec.new();
        ~first_postings = &posting_entries_per_term[0];
        ~i: u64 = 0;
        L W i < first_postings.len() {
            I is_posting_visible(&first_postings[i], snapshot, clog) {
                candidate_docs.push(first_postings[i].doc_id);
            }
            i += 1;
        }

        # Remove duplicates from first term
        candidate_docs = self.deduplicate_doc_ids(&candidate_docs);

        # Intersect with remaining terms
        ~term_idx: u64 = 1;
        L W term_idx < posting_entries_per_term.len() {
            ~term_postings = &posting_entries_per_term[term_idx];

            # Get visible doc_ids for this term
            ~term_doc_ids = Vec.new();
            ~j: u64 = 0;
            L W j < term_postings.len() {
                I is_posting_visible(&term_postings[j], snapshot, clog) {
                    term_doc_ids.push(term_postings[j].doc_id);
                }
                j += 1;
            }
            term_doc_ids = self.deduplicate_doc_ids(&term_doc_ids);

            # Intersect candidate_docs with term_doc_ids
            candidate_docs = self.intersect_doc_id_lists(&candidate_docs, &term_doc_ids);

            # Early exit if no candidates remain
            I candidate_docs.len() == 0 {
                R candidate_docs;
            }

            term_idx += 1;
        }

        candidate_docs
    }

    # Collect all positions for each term in a specific document
    # Returns Vec<Vec<u32>> where outer index = term, inner = positions
    F collect_positions_for_doc(
        self,
        doc_id: u64,
        posting_entries_per_term: &Vec<Vec<PostingEntry>>,
        snapshot: &Snapshot,
        clog: &Clog,
    ) -> Vec<Vec<u32>> {
        ~positions_per_term = Vec.with_capacity(posting_entries_per_term.len());

        ~term_idx: u64 = 0;
        L W term_idx < posting_entries_per_term.len() {
            ~term_positions = Vec.new();
            ~postings = &posting_entries_per_term[term_idx];

            ~i: u64 = 0;
            L W i < postings.len() {
                I postings[i].doc_id == doc_id && is_posting_visible(&postings[i], snapshot, clog) {
                    # Merge positions from this posting entry
                    ~j: u64 = 0;
                    L W j < postings[i].positions.len() {
                        term_positions.push(postings[i].positions[j]);
                        j += 1;
                    }
                }
                i += 1;
            }

            # Sort positions for this term (should already be sorted, but ensure)
            term_positions = self.sort_positions(&term_positions);

            positions_per_term.push(term_positions);
            term_idx += 1;
        }

        positions_per_term
    }

    # Count phrase matches with allowed slop
    #
    # Algorithm:
    # For each position P in term[0]:
    #   Check if term[1] has position P+1 (+slop)
    #   Check if term[2] has position P+2 (+slop)
    #   ...
    #   If all terms found at consecutive (+slop) positions, increment match_count
    F count_phrase_matches(self, positions_per_term: &Vec<Vec<u32>>, slop: u32) -> u32 {
        I positions_per_term.len() == 0 {
            R 0;
        }

        # Edge case: any term has no positions
        ~i: u64 = 0;
        L W i < positions_per_term.len() {
            I positions_per_term[i].len() == 0 {
                R 0;
            }
            i += 1;
        }

        ~match_count: u32 = 0;

        # For each position in first term
        ~first_positions = &positions_per_term[0];
        ~pos_idx: u64 = 0;
        L W pos_idx < first_positions.len() {
            ~base_pos = first_positions[pos_idx];

            # Try to M remaining terms at consecutive positions
            ~matched = self.try_match_phrase_at_position(
                base_pos,
                positions_per_term,
                slop,
            );

            I matched {
                match_count += 1;
            }

            pos_idx += 1;
        }

        match_count
    }

    # Try to M phrase starting at base_pos
    # Returns true if all terms found at consecutive (+slop) positions
    F try_match_phrase_at_position(
        self,
        base_pos: u32,
        positions_per_term: &Vec<Vec<u32>>,
        slop: u32,
    ) -> bool {
        # Start from term 1 (term 0 is at base_pos)
        ~term_idx: u64 = 1;
        L W term_idx < positions_per_term.len() {
            ~expected_pos = base_pos + (term_idx as u32);
            ~min_pos = I expected_pos >= slop { expected_pos - slop } E { 0 };
            ~max_pos = expected_pos + slop;

            # Check if this term has a position in range [min_pos, max_pos]
            ~found = self.has_position_in_range(
                &positions_per_term[term_idx],
                min_pos,
                max_pos,
            );

            I !found {
                R false;
            }

            term_idx += 1;
        }

        true
    }

    # Check if positions list contains any value in [min_pos, max_pos]
    F has_position_in_range(self, positions: &Vec<u32>, min_pos: u32, max_pos: u32) -> bool {
        ~i: u64 = 0;
        L W i < positions.len() {
            I positions[i] >= min_pos && positions[i] <= max_pos {
                R true;
            }
            i += 1;
        }
        false
    }

    # Calculate document length (total visible term frequencies across all terms)
    F calculate_doc_length(
        self,
        doc_id: u64,
        posting_entries_per_term: &Vec<Vec<PostingEntry>>,
        snapshot: &Snapshot,
        clog: &Clog,
    ) -> u32 {
        ~total_length: u32 = 0;

        ~term_idx: u64 = 0;
        L W term_idx < posting_entries_per_term.len() {
            ~postings = &posting_entries_per_term[term_idx];

            ~i: u64 = 0;
            L W i < postings.len() {
                I postings[i].doc_id == doc_id && is_posting_visible(&postings[i], snapshot, clog) {
                    total_length += postings[i].term_freq;
                }
                i += 1;
            }

            term_idx += 1;
        }

        total_length
    }

    # Score a phrase M #
    # Formula: IDF-like component + frequency boost
    # score = ln(total_docs + 1) * ln(match_count + 1) / ln(doc_length / avg_doc_length + 1)
    #
    # This gives higher scores to:
    # - Phrases appearing multiple times in the document
    # - Shorter documents (relative to corpus average)
    F score_phrase(
        self,
        match_count: u32,
        doc_length: u32,
        avg_doc_length: f64,
        total_docs: u64,
    ) -> f64 {
        I match_count == 0 || doc_length == 0 {
            R 0.0;
        }

        # Corpus size component (IDF-like)
        ~corpus_component = ln((total_docs + 1) as f64);

        # Match frequency component
        ~freq_component = ln((match_count + 1) as f64);

        # Document length normalization
        ~length_ratio = (doc_length as f64) / avg_doc_length;
        ~length_norm = ln(length_ratio + 1.0);

        # Avoid division by zero
        I length_norm < 0.0001 {
            length_norm = 0.0001;
        }

        corpus_component * freq_component / length_norm
    }

    # Handle single-term "phrase" (degenerate case)
    F handle_single_term(
        self,
        postings: &Vec<PostingEntry>,
        snapshot: &Snapshot,
        clog: &Clog,
        avg_doc_length: f64,
        total_docs: u64,
    ) -> Vec<PhraseResult> {
        ~results = Vec.new();

        ~i: u64 = 0;
        L W i < postings.len() {
            I is_posting_visible(&postings[i], snapshot, clog) {
                ~doc_id = postings[i].doc_id;
                ~match_count = postings[i].term_freq;
                ~doc_length = postings[i].term_freq;

                ~score = self.score_phrase(
                    match_count,
                    doc_length,
                    avg_doc_length,
                    total_docs,
                );

                results.push(PhraseResult.new(doc_id, score, match_count));
            }
            i += 1;
        }

        self.sort_results_by_score(&results);
        results
    }

    # Deduplicate doc_ids in a sorted or unsorted list
    F deduplicate_doc_ids(self, doc_ids: &Vec<u64>) -> Vec<u64> {
        I doc_ids.len() == 0 {
            R Vec.new();
        }

        ~result = Vec.new();
        ~i: u64 = 0;
        L W i < doc_ids.len() {
            ~doc_id = doc_ids[i];

            # Check if already in result
            ~found = false;
            ~j: u64 = 0;
            L W j < result.len() {
                I result[j] == doc_id {
                    found = true;
                }
                j += 1;
            }

            I !found {
                result.push(doc_id);
            }

            i += 1;
        }

        result
    }

    # Intersect two doc_id lists
    F intersect_doc_id_lists(self, list1: &Vec<u64>, list2: &Vec<u64>) -> Vec<u64> {
        ~result = Vec.new();

        ~i: u64 = 0;
        L W i < list1.len() {
            ~doc_id = list1[i];

            # Check if doc_id exists in list2
            ~found = false;
            ~j: u64 = 0;
            L W j < list2.len() {
                I list2[j] == doc_id {
                    found = true;
                }
                j += 1;
            }

            I found {
                result.push(doc_id);
            }

            i += 1;
        }

        result
    }

    # Sort positions in ascending order (simple bubble sort)
    F sort_positions(self, positions: &Vec<u32>) -> Vec<u32> {
        I positions.len() <= 1 {
            R positions.clone();
        }

        ~result = positions.clone();
        ~n = result.len();

        ~i: u64 = 0;
        L W i < n {
            ~j: u64 = 0;
            L W j < n - i - 1 {
                I result[j] > result[j + 1] {
                    # Swap
                    ~temp = result[j];
                    result[j] = result[j + 1];
                    result[j + 1] = temp;
                }
                j += 1;
            }
            i += 1;
        }

        result
    }

    # Sort results by score in descending order (simple bubble sort)
    F sort_results_by_score(self, results: &Vec<PhraseResult>) {
        I results.len() <= 1 {
            R;
        }

        ~n = results.len();
        ~i: u64 = 0;
        L W i < n {
            ~j: u64 = 0;
            L W j < n - i - 1 {
                I results[j].score < results[j + 1].score {
                    # Swap
                    ~temp = results[j];
                    results[j] = results[j + 1];
                    results[j + 1] = temp;
                }
                j += 1;
            }
            i += 1;
        }
    }
}

# ============================================================================
# Unit tests
# ============================================================================

#[test]
F test_phrase_searcher_new() {
    ~searcher = PhraseSearcher.new();
    # Should not crash
}

#[test]
F test_deduplicate_doc_ids() {
    ~searcher = PhraseSearcher.new();

    ~doc_ids = Vec.new();
    doc_ids.push(1);
    doc_ids.push(2);
    doc_ids.push(1);
    doc_ids.push(3);
    doc_ids.push(2);

    ~result = searcher.deduplicate_doc_ids(&doc_ids);

    assert_eq!(result.len(), 3);
    # Should contain 1, 2, 3 (order may vary)
}

#[test]
F test_intersect_doc_id_lists() {
    ~searcher = PhraseSearcher.new();

    ~list1 = Vec.new();
    list1.push(1);
    list1.push(2);
    list1.push(3);

    ~list2 = Vec.new();
    list2.push(2);
    list2.push(3);
    list2.push(4);

    ~result = searcher.intersect_doc_id_lists(&list1, &list2);

    assert_eq!(result.len(), 2);
    # Should contain 2, 3
}

#[test]
F test_has_position_in_range() {
    ~searcher = PhraseSearcher.new();

    ~positions = Vec.new();
    positions.push(5);
    positions.push(10);
    positions.push(15);

    assert!(searcher.has_position_in_range(&positions, 8, 12));
    assert!(searcher.has_position_in_range(&positions, 5, 5));
    assert!(!searcher.has_position_in_range(&positions, 20, 25));
}

#[test]
F test_sort_positions() {
    ~searcher = PhraseSearcher.new();

    ~positions = Vec.new();
    positions.push(15);
    positions.push(5);
    positions.push(10);
    positions.push(1);

    ~sorted = searcher.sort_positions(&positions);

    assert_eq!(sorted.len(), 4);
    assert_eq!(sorted[0], 1);
    assert_eq!(sorted[1], 5);
    assert_eq!(sorted[2], 10);
    assert_eq!(sorted[3], 15);
}

#[test]
F test_score_phrase() {
    ~searcher = PhraseSearcher.new();

    # Normal case
    ~score = searcher.score_phrase(3, 100, 120.0, 1000);
    assert!(score > 0.0);

    # Edge cases
    assert_eq!(searcher.score_phrase(0, 100, 120.0, 1000), 0.0);
    assert_eq!(searcher.score_phrase(3, 0, 120.0, 1000), 0.0);
}
