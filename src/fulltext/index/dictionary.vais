# Full-Text Dictionary Index
# B+Tree-based index mapping terms to posting list head pages
# Key format: encode_u64_be(term_hash) → posting_head_page
# Maintains DictEntry metadata for each term

U std/vec.Vec;
U std/string.Str;
U std/bytes.{ByteBuffer};
U std/option.{Option, Some, None};
U storage/constants.{FILE_ID_FULLTEXT, NULL_PAGE};
U storage/error.{VaisError};
U storage/buffer/pool.{BufferPool};
U storage/page/allocator.{PageAllocator};
U storage/btree/tree.{BTree};
U storage/btree/insert.{btree_insert};
U storage/btree/delete.{btree_delete};
U storage/btree/search.{search_lower_bound};
U storage/btree/key.{encode_u64_key};
U storage/btree/cursor.{BTreeCursor, ScanDirection};
U storage/wal/group_commit.{GroupCommitManager};
U fulltext/types.{DictEntry, err_fulltext_term_not_found, err_fulltext_dict_corrupt};
U storage/hash.{fnv1a_hash};

# ============================================================================
# Error codes: EE=04 (fulltext), CC=06 (dictionary index)
# ============================================================================

F err_dict_index_corrupt(term_hash: u64) -> VaisError {
    VaisError.new(
        "VAIS-0406001",
        "Dictionary index corrupted for term_hash {term_hash}"
    )
}

# ============================================================================
# DictionaryIndex — B+Tree wrapper for term-to-posting-list mapping
# ============================================================================

S DictionaryIndex {
    tree: BTree,
    entries: Vec<DictEntry>,   # In-memory dictionary cache
}

X DictionaryIndex {
    # Create new dictionary index with given root page
    F new(root_page_id: u32, page_size: u32) -> DictionaryIndex {
        DictionaryIndex {
            tree: BTree.new(root_page_id, FILE_ID_FULLTEXT, page_size),
            entries: Vec.new(),
        }
    }

    # Insert a new term into the dictionary
    # WAL logging is done by the caller (FullTextWalManager)
    F insert_term(
        ~self,
        term: &Str,
        posting_head_page: u32,
        txn_id: u64,
        gcm: &~GroupCommitManager,
        pool: &~BufferPool,
    ) -> Result<DictEntry, VaisError> {
        ~term_hash = fnv1a_hash(term);
        ~key = DictionaryIndex.encode_term_key(term_hash);

        # Insert into B+Tree (TID encodes posting_head_page)
        btree_insert(&self.tree, &key, posting_head_page, txn_id, gcm, pool)?;

        # Create and cache DictEntry
        ~entry = DictEntry.new(term.clone(), term_hash, posting_head_page);
        self.entries.push(entry.clone());

        Ok(entry)
    }

    # Delete a term from the dictionary
    # WAL logging is done by the caller (FullTextWalManager)
    F delete_term(
        ~self,
        term: &Str,
        txn_id: u64,
        gcm: &~GroupCommitManager,
        pool: &~BufferPool,
    ) -> Result<(), VaisError> {
        ~term_hash = fnv1a_hash(term);
        ~key = DictionaryIndex.encode_term_key(term_hash);

        # Delete from B+Tree
        btree_delete(&self.tree, &key, txn_id, gcm, pool)?;

        # Remove from cache
        ~i: u64 = 0;
        ~removed = false;
        W i < self.entries.len() && !removed {
            I self.entries[i].term_hash == term_hash {
                self.entries.remove(i);
                removed = true;
            } E {
                i += 1;
            }
        }

        Ok(())
    }

    # Look up a term by hash, returning its DictEntry
    F lookup_term(
        self,
        term: &Str,
        pool: &~BufferPool,
    ) -> Result<Option<DictEntry>, VaisError> {
        ~term_hash = fnv1a_hash(term);

        # Check in-memory cache first
        ~i: u64 = 0;
        W i < self.entries.len() {
            I self.entries[i].term_hash == term_hash && self.entries[i].term == *term {
                R Ok(Some(self.entries[i].clone()));
            }
            i += 1;
        }

        # Fall back to B+Tree lookup
        ~key = DictionaryIndex.encode_term_key(term_hash);
        M self.tree.search(&key, pool)? {
            Some(~posting_head_page) => {
                ~entry = DictEntry.new(term.clone(), term_hash, posting_head_page);
                Ok(Some(entry))
            },
            None => Ok(None),
        }
    }

    # Look up a term by hash directly
    F lookup_by_hash(
        self,
        term_hash: u64,
        pool: &~BufferPool,
    ) -> Result<Option<u32>, VaisError> {
        # Check cache
        ~i: u64 = 0;
        W i < self.entries.len() {
            I self.entries[i].term_hash == term_hash {
                R Ok(Some(self.entries[i].posting_head_page));
            }
            i += 1;
        }

        # B+Tree lookup
        ~key = DictionaryIndex.encode_term_key(term_hash);
        self.tree.search(&key, pool)
    }

    # Update the posting head page for a term
    F update_posting_head(
        ~self,
        term_hash: u64,
        new_posting_head: u32,
        txn_id: u64,
        gcm: &~GroupCommitManager,
        pool: &~BufferPool,
    ) -> Result<(), VaisError> {
        ~key = DictionaryIndex.encode_term_key(term_hash);

        # Delete old entry and insert with new value
        btree_delete(&self.tree, &key, txn_id, gcm, pool)?;
        btree_insert(&self.tree, &key, new_posting_head, txn_id, gcm, pool)?;

        # Update cache
        ~i: u64 = 0;
        ~updated = false;
        W i < self.entries.len() && !updated {
            I self.entries[i].term_hash == term_hash {
                self.entries[i].posting_head_page = new_posting_head;
                updated = true;
            } E {
                i += 1;
            }
        }

        Ok(())
    }

    # Update DictEntry statistics (doc_freq, total_term_freq)
    F update_entry_stats(
        ~self,
        term_hash: u64,
        delta_doc_freq: i32,
        delta_term_freq: i64,
    ) {
        ~i: u64 = 0;
        ~found = false;
        W i < self.entries.len() && !found {
            I self.entries[i].term_hash == term_hash {
                I delta_doc_freq > 0 {
                    self.entries[i].doc_freq += delta_doc_freq as u32;
                } E I delta_doc_freq < 0 {
                    ~abs = (-delta_doc_freq) as u32;
                    I self.entries[i].doc_freq >= abs {
                        self.entries[i].doc_freq -= abs;
                    } E {
                        self.entries[i].doc_freq = 0;
                    }
                }
                I delta_term_freq > 0 {
                    self.entries[i].total_term_freq += delta_term_freq as u64;
                } E I delta_term_freq < 0 {
                    ~abs = (-delta_term_freq) as u64;
                    I self.entries[i].total_term_freq >= abs {
                        self.entries[i].total_term_freq -= abs;
                    } E {
                        self.entries[i].total_term_freq = 0;
                    }
                }
                found = true;
            } E {
                i += 1;
            }
        }
    }

    # Scan all terms in dictionary (full scan of B+Tree)
    F scan_all_terms(
        self,
        pool: &~BufferPool,
    ) -> Result<Vec<DictEntry>, VaisError> {
        I !self.entries.is_empty() {
            R Ok(self.entries.clone());
        }

        # Fall back to B+Tree full scan
        ~results = Vec.new();
        ~entries = self.tree.full_scan(pool)?;
        ~i: u64 = 0;
        W i < entries.len() {
            ~term_hash = DictionaryIndex.decode_term_key(&entries[i].0);
            ~posting_head = entries[i].1;
            results.push(DictEntry {
                term: "",  # Term text not stored in B+Tree, only hash
                term_hash,
                posting_head_page: posting_head,
                doc_freq: 0,
                total_term_freq: 0,
            });
            i += 1;
        }
        Ok(results)
    }

    # Get total number of terms in dictionary
    F term_count(self) -> u64 {
        self.entries.len()
    }

    # Clear in-memory cache
    F clear_cache(~self) {
        self.entries.clear();
    }

    # Load all entries from B+Tree into cache
    F load_cache(
        ~self,
        pool: &~BufferPool,
    ) -> Result<(), VaisError> {
        self.entries = self.scan_all_terms(pool)?;
        Ok(())
    }

    # ========================================================================
    # Key encoding/decoding
    # ========================================================================

    # Encode term hash as big-endian u64 key for B+Tree ordering
    F encode_term_key(term_hash: u64) -> Vec<u8> {
        encode_u64_key(term_hash)
    }

    # Decode term hash from big-endian u64 key
    F decode_term_key(key: &[u8]) -> u64 {
        I key.len() < 8 {
            R 0;
        }
        ~hash: u64 = 0;
        hash |= (key[0] as u64) << 56;
        hash |= (key[1] as u64) << 48;
        hash |= (key[2] as u64) << 40;
        hash |= (key[3] as u64) << 32;
        hash |= (key[4] as u64) << 24;
        hash |= (key[5] as u64) << 16;
        hash |= (key[6] as u64) << 8;
        hash |= key[7] as u64;
        hash
    }

    # Get underlying B+Tree
    F get_tree(self) -> &BTree {
        &self.tree
    }

    F get_tree_mut(~self) -> &~BTree {
        &self.tree
    }
}
