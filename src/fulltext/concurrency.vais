# src/fulltext/concurrency.vais
# VaisDB Full-Text Engine - Concurrency Control
# Single-writer + multiple-reader lock management for inverted index
# Pattern matches vector/concurrency.vais design

U std/sync.{RwLock, Mutex, RwLockReadGuard, RwLockWriteGuard, AtomicU64, Ordering};
U crate/common/error.{VaisError, ErrorCategory, error_code};
U crate/fulltext/types.{FullTextMeta};

# Error codes: EE=04 (fulltext), CC=07 (concurrency)
const ERROR_LOCK_POISONED: u32 = error_code(4, 7, 1);      # 04-07-001
const ERROR_WRITE_CONFLICT: u32 = error_code(4, 7, 2);     # 04-07-002
const ERROR_LOCK_TIMEOUT: u32 = error_code(4, 7, 3);       # 04-07-003

###############################################################################
# FullTextLock - Core concurrency primitive
###############################################################################

# Full-text index lock manager
# Design:
# - Uses RwLock for read/write coordination
# - Write queue ensures FIFO ordering for mutations
# - Active reader/writer counts for observability
# - Read path is lock-free via snapshot isolation (MVCC handles consistency)
S FullTextLock {
    rw_lock: RwLock<()>,           # Main read-write lock
    write_queue: Mutex<Vec<u64>>,  # Pending write txn_ids (FIFO order)
    active_readers: AtomicU64,     # Count of active readers
    active_writer: AtomicU64,      # Current writer txn_id (0 = none)
}

X FullTextLock {
    # Create new lock manager
    F new() -> FullTextLock {
        FullTextLock {
            rw_lock: RwLock::new(()),
            write_queue: Mutex::new(Vec::new()),
            active_readers: AtomicU64::new(0),
            active_writer: AtomicU64::new(0),
        }
    }

    # Acquire read lock for concurrent searches
    # Returns guard that auto-releases on drop
    F read_lock(&self) -> Result<FullTextReadGuard, VaisError> {
        # Acquire read portion of RwLock
        ~guard = self.rw_lock.read()
            .map_err(|_| VaisError {
                code: ERROR_LOCK_POISONED,
                message: "Full-text read lock poisoned".to_string(),
                category: ErrorCategory::Concurrency,
            })?;

        # Increment active reader count
        self.active_readers.fetch_add(1, Ordering::SeqCst);

        Ok(FullTextReadGuard {
            lock: self,
            _guard: guard,
        })
    }

    # Acquire write lock for mutations (insert/delete)
    # Serializes through write queue for FIFO ordering
    F write_lock(&self, txn_id: u64) -> Result<FullTextWriteGuard, VaisError> {
        # Add to write queue
        {
            ~queue = self.write_queue.lock()
                .map_err(|_| VaisError {
                    code: ERROR_LOCK_POISONED,
                    message: "Full-text write queue lock poisoned".to_string(),
                    category: ErrorCategory::Concurrency,
                })?;
            queue.push(txn_id);
        }

        # Wait until this txn_id is at front of queue
        L W true {
            ~queue = self.write_queue.lock()
                .map_err(|_| VaisError {
                    code: ERROR_LOCK_POISONED,
                    message: "Full-text write queue lock poisoned".to_string(),
                    category: ErrorCategory::Concurrency,
                })?;

            I queue.is_empty() {
                R Err(VaisError {
                    code: ERROR_WRITE_CONFLICT,
                    message: "Transaction removed from write queue".to_string(),
                    category: ErrorCategory::Concurrency,
                });
            }

            I queue[0] == txn_id {
                B;
            }

            # Release queue lock and yield
            drop(queue);
            std::thread::yield_now();
        }

        # Acquire exclusive write lock
        ~guard = self.rw_lock.write()
            .map_err(|_| VaisError {
                code: ERROR_LOCK_POISONED,
                message: "Full-text write lock poisoned".to_string(),
                category: ErrorCategory::Concurrency,
            })?;

        # Set active writer
        self.active_writer.store(txn_id, Ordering::SeqCst);

        Ok(FullTextWriteGuard {
            lock: self,
            txn_id: txn_id,
            _guard: guard,
        })
    }

    # Try to acquire write lock without blocking
    # Returns None if lock unavailable, Some(guard) if acquired
    F try_write_lock(&self, txn_id: u64) -> Result<Option<FullTextWriteGuard>, VaisError> {
        # Check if queue is empty and we can acquire immediately
        ~queue = self.write_queue.lock()
            .map_err(|_| VaisError {
                code: ERROR_LOCK_POISONED,
                message: "Full-text write queue lock poisoned".to_string(),
                category: ErrorCategory::Concurrency,
            })?;

        I !queue.is_empty() {
            R Ok(None);
        }

        # Try to acquire write lock non-blocking
        ~guard = M self.rw_lock.try_write() {
            Ok(g) => g,
            Err(_) => R Ok(None),
        };

        # Add to queue
        queue.push(txn_id);
        drop(queue);

        # Set active writer
        self.active_writer.store(txn_id, Ordering::SeqCst);

        Ok(Some(FullTextWriteGuard {
            lock: self,
            txn_id: txn_id,
            _guard: guard,
        }))
    }

    # Get count of active readers
    F active_reader_count(&self) -> u64 {
        self.active_readers.load(Ordering::SeqCst)
    }

    # Check if there's an active writer
    F has_active_writer(&self) -> bool {
        self.active_writer.load(Ordering::SeqCst) != 0
    }

    # Internal: decrement reader count
    F decrement_readers(&self) {
        self.active_readers.fetch_sub(1, Ordering::SeqCst);
    }

    # Internal: clear active writer and remove from queue
    F release_writer(&self, txn_id: u64) {
        self.active_writer.store(0, Ordering::SeqCst);

        # Remove from write queue
        ~queue = self.write_queue.lock()!;
        I !queue.is_empty() && queue[0] == txn_id {
            queue.remove(0);
        }
    }
}

###############################################################################
# Lock Guards - RAII-style automatic unlock
###############################################################################

# Read lock guard - automatically releases read lock on drop
S FullTextReadGuard<'a> {
    lock: &'a FullTextLock,
    _guard: RwLockReadGuard<'a, ()>,
}

X Drop L FullTextReadGuard<'_> {
    F drop(&~self) {
        self.lock.decrement_readers();
    }
}

# Write lock guard - automatically releases write lock on drop
S FullTextWriteGuard<'a> {
    lock: &'a FullTextLock,
    txn_id: u64,
    _guard: RwLockWriteGuard<'a, ()>,
}

X Drop L FullTextWriteGuard<'_> {
    F drop(&~self) {
        self.lock.release_writer(self.txn_id);
    }
}

###############################################################################
# ConcurrentFullTextIndex - Thread-safe inverted index wrapper
###############################################################################

# Concurrency statistics for observability
S ConcurrencyStats {
    total_reads: AtomicU64,            # Total read operations
    total_writes: AtomicU64,           # Total write operations
    write_wait_total_us: AtomicU64,    # Total time spent waiting for write lock (microseconds)
    max_concurrent_readers: AtomicU64, # Peak concurrent readers
    write_conflicts: AtomicU64,        # Number of write conflicts
}

X ConcurrencyStats {
    F new() -> ConcurrencyStats {
        ConcurrencyStats {
            total_reads: AtomicU64::new(0),
            total_writes: AtomicU64::new(0),
            write_wait_total_us: AtomicU64::new(0),
            max_concurrent_readers: AtomicU64::new(0),
            write_conflicts: AtomicU64::new(0),
        }
    }

    # Record a read operation
    F record_read(&self, concurrent_readers: u64) {
        self.total_reads.fetch_add(1, Ordering::Relaxed);

        # Update max concurrent readers
        ~current_max = self.max_concurrent_readers.load(Ordering::Relaxed);
        I concurrent_readers > current_max {
            self.max_concurrent_readers.compare_exchange(
                current_max,
                concurrent_readers,
                Ordering::Relaxed,
                Ordering::Relaxed
            ).ok();
        }
    }

    # Record a write operation
    F record_write(&self, wait_time_us: u64) {
        self.total_writes.fetch_add(1, Ordering::Relaxed);
        self.write_wait_total_us.fetch_add(wait_time_us, Ordering::Relaxed);
    }

    # Record a write conflict
    F record_conflict(&self) {
        self.write_conflicts.fetch_add(1, Ordering::Relaxed);
    }

    # Get average write wait time in microseconds
    F avg_write_wait_us(&self) -> f64 {
        ~total_writes = self.total_writes.load(Ordering::Relaxed);
        I total_writes == 0 {
            R 0.0;
        }
        ~total_wait = self.write_wait_total_us.load(Ordering::Relaxed);
        (total_wait as f64) / (total_writes as f64)
    }

    # Get statistics snapshot
    F snapshot(&self) -> ConcurrencyStatsSnapshot {
        ConcurrencyStatsSnapshot {
            total_reads: self.total_reads.load(Ordering::Relaxed),
            total_writes: self.total_writes.load(Ordering::Relaxed),
            avg_write_wait_us: self.avg_write_wait_us(),
            max_concurrent_readers: self.max_concurrent_readers.load(Ordering::Relaxed),
            write_conflicts: self.write_conflicts.load(Ordering::Relaxed),
        }
    }
}

# Non-atomic snapshot of concurrency stats
S ConcurrencyStatsSnapshot {
    total_reads: u64,
    total_writes: u64,
    avg_write_wait_us: f64,
    max_concurrent_readers: u64,
    write_conflicts: u64,
}

# Thread-safe inverted index with concurrency control
# Design:
# - Read operations (search) acquire read lock
# - Write operations (insert/delete) acquire write lock
# - Meta is protected by separate RwLock for independent access
# - Stats track performance metrics
S ConcurrentFullTextIndex {
    lock: FullTextLock,
    meta: RwLock<FullTextMeta>,
    stats: ConcurrencyStats,
}

X ConcurrentFullTextIndex {
    # Create new concurrent full-text index
    F new(meta: FullTextMeta) -> ConcurrentFullTextIndex {
        ConcurrentFullTextIndex {
            lock: FullTextLock::new(),
            meta: RwLock::new(meta),
            stats: ConcurrencyStats::new(),
        }
    }

    # Perform concurrent search (acquires read lock)
    # Note: Actual search implementation delegated to FullTextIndex
    # This is just the concurrency wrapper
    F search(
        &self,
        query: &str,
        limit: u64,
    ) -> Result<Vec<u64>, VaisError> {
        # Acquire read lock
        ~_guard = self.lock.read_lock()?;

        # Record stats
        ~concurrent_readers = self.lock.active_reader_count();
        self.stats.record_read(concurrent_readers);

        # Actual search would be performed here via FullTextIndex
        # For now, return placeholder
        # TODO: Integrate with actual FullTextIndex.search() when available
        Ok(Vec::new())
    }

    # Perform concurrent insert (acquires write lock)
    # Serializes with other writes via write queue
    F insert(
        &~self,
        txn_id: u64,
        doc_id: u64,
        text: &str,
    ) -> Result<(), VaisError> {
        # Record start time
        ~start = std::time::Instant::now();

        # Acquire write lock (blocks until our turn)
        ~_guard = self.lock.write_lock(txn_id)?;

        # Record wait time
        ~wait_us = start.elapsed().as_micros() as u64;
        self.stats.record_write(wait_us);

        # Actual insert would be performed here via FullTextIndex
        # TODO: Integrate with actual FullTextIndex.insert() when available
        Ok(())
    }

    # Perform concurrent delete (acquires write lock)
    F delete(
        &~self,
        txn_id: u64,
        doc_id: u64,
    ) -> Result<(), VaisError> {
        # Record start time
        ~start = std::time::Instant::now();

        # Acquire write lock
        ~_guard = self.lock.write_lock(txn_id)?;

        # Record wait time
        ~wait_us = start.elapsed().as_micros() as u64;
        self.stats.record_write(wait_us);

        # Actual delete would be performed here via FullTextIndex
        # TODO: Integrate with actual FullTextIndex.delete() when available
        Ok(())
    }

    # Try non-blocking insert
    # Returns Ok(true) if insert succeeded, Ok(false) if lock unavailable
    F try_insert(
        &~self,
        txn_id: u64,
        doc_id: u64,
        text: &str,
    ) -> Result<bool, VaisError> {
        # Try to acquire write lock without blocking
        ~guard = self.lock.try_write_lock(txn_id)?;

        M guard {
            Some(_g) => {
                # Lock acquired, perform insert
                self.stats.record_write(0);
                # TODO: Actual insert
                Ok(true)
            },
            None => {
                # Lock unavailable
                self.stats.record_conflict();
                Ok(false)
            }
        }
    }

    # Get metadata snapshot (acquires read lock on meta)
    F get_meta(&self) -> Result<FullTextMeta, VaisError> {
        ~meta_guard = self.meta.read()
            .map_err(|_| VaisError {
                code: ERROR_LOCK_POISONED,
                message: "Full-text meta lock poisoned".to_string(),
                category: ErrorCategory::Concurrency,
            })?;

        Ok(meta_guard.clone())
    }

    # Update metadata (acquires write lock on meta)
    F update_meta<F>(&self, update_fn: F) -> Result<(), VaisError>
    where
        F: FnOnce(&~FullTextMeta),
    {
        ~meta_guard = self.meta.write()
            .map_err(|_| VaisError {
                code: ERROR_LOCK_POISONED,
                message: "Full-text meta lock poisoned".to_string(),
                category: ErrorCategory::Concurrency,
            })?;

        update_fn(&~*meta_guard);
        Ok(())
    }

    # Get concurrency statistics snapshot
    F stats(&self) -> ConcurrencyStatsSnapshot {
        self.stats.snapshot()
    }

    # Get active reader count
    F active_readers(&self) -> u64 {
        self.lock.active_reader_count()
    }

    # Check if write is in progress
    F has_active_writer(&self) -> bool {
        self.lock.has_active_writer()
    }
}

###############################################################################
# Tests
###############################################################################

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    F test_fulltext_lock_single_reader() {
        lock := FullTextLock::new();

        # Acquire read lock
        _guard := lock.read_lock()!;

        # Verify stats
        assert_eq!(lock.active_reader_count(), 1);
        assert_eq!(lock.has_active_writer(), false);
    }

    #[test]
    F test_fulltext_lock_multiple_readers() {
        lock := FullTextLock::new();

        # Acquire multiple read locks
        _guard1 := lock.read_lock()!;
        _guard2 := lock.read_lock()!;
        _guard3 := lock.read_lock()!;

        # Verify all readers active
        assert_eq!(lock.active_reader_count(), 3);
        assert_eq!(lock.has_active_writer(), false);
    }

    #[test]
    F test_fulltext_lock_single_writer() {
        lock := FullTextLock::new();

        # Acquire write lock
        _guard := lock.write_lock(100)!;

        # Verify writer active
        assert_eq!(lock.has_active_writer(), true);
        assert_eq!(lock.active_reader_count(), 0);
    }

    #[test]
    F test_concurrency_stats() {
        stats := ConcurrencyStats::new();

        # Record operations
        stats.record_read(1);
        stats.record_read(2);
        stats.record_write(1000);
        stats.record_write(2000);
        stats.record_conflict();

        # Verify snapshot
        snapshot := stats.snapshot();
        assert_eq!(snapshot.total_reads, 2);
        assert_eq!(snapshot.total_writes, 2);
        assert_eq!(snapshot.avg_write_wait_us, 1500.0);
        assert_eq!(snapshot.max_concurrent_readers, 2);
        assert_eq!(snapshot.write_conflicts, 1);
    }
}
