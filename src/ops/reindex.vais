# Production Operations — REINDEX & Database Compaction
# Phase 9 Task 10: Rebuild indexes, defragment database files
# Provides: ReindexExecutor, CompactionExecutor

U std/vec.Vec;
U std/string.Str;
U std/option.{Option, Some, None};
U std/sync.{Mutex};
U storage/error.{VaisError};
U storage/constants.{
    FILE_ID_DATA, FILE_ID_VECTORS, FILE_ID_GRAPH, FILE_ID_FULLTEXT,
};
U ops/types.{
    ReindexTarget, ReindexResult, CompactionResult,
    REINDEX_TARGET_TABLE, REINDEX_TARGET_INDEX, REINDEX_TARGET_DATABASE,
    err_reindex_failed, err_reindex_invalid_target, err_compaction_failed,
};
U ops/config.{ReindexConfig, CompactionConfig};

# ============================================================================
# ReindexExecutor — Rebuilds database indexes
# ============================================================================

## REINDEX rebuilds one or more indexes from scratch:
## - B+Tree indexes: scan heap, build new B+Tree, swap with old
## - HNSW vector indexes: re-insert all vectors into a new graph
## - Fulltext indexes: re-tokenize and rebuild posting lists
##
## REINDEX CONCURRENTLY builds the new index without blocking reads
## (writes are briefly blocked during the swap).
S ReindexExecutor {
    config: ReindexConfig,
    is_running: bool,
    lock: Mutex,
}

X ReindexExecutor {
    ## Create a new reindex executor
    F new(config: ReindexConfig) -> ReindexExecutor {
        ReindexExecutor {
            config,
            is_running: false,
            lock: Mutex.new(),
        }
    }

    ## Execute REINDEX on specified target
    F execute(
        ~self,
        target: &ReindexTarget,
        current_time_ms: u64,
    ) -> Result<ReindexResult, VaisError> {
        ~guard = self.lock.lock();

        I self.is_running {
            R Err(err_reindex_failed("REINDEX already in progress"));
        }
        self.is_running = true;

        ~start_ms = current_time_ms;
        ~result = ReindexResult.new();

        M target.target_type {
            0 => {
                # REINDEX_TARGET_TABLE: rebuild all indexes on a table
                result = self.reindex_table(&target.name)?;
            },
            1 => {
                # REINDEX_TARGET_INDEX: rebuild a specific index
                result = self.reindex_single_index(&target.name)?;
            },
            2 => {
                # REINDEX_TARGET_DATABASE: rebuild all indexes
                result = self.reindex_database()?;
            },
            _ => {
                self.is_running = false;
                R Err(err_reindex_failed("Unknown reindex target type"));
            },
        }

        result.duration_ms = current_time_ms - start_ms;
        self.is_running = false;
        Ok(result)
    }

    ## Check if reindex is running
    F is_running(self) -> bool {
        self.is_running
    }

    # ========================================================================
    # Reindex Implementations
    # ========================================================================

    ## Rebuild all indexes on a specific table
    F reindex_table(self, table_name: &Str) -> Result<ReindexResult, VaisError> {
        ~result = ReindexResult.new();

        # In production implementation:
        #
        # 1. Look up table in catalog to get all index definitions
        # 2. For each index on this table:
        #    a. Determine index type (B+Tree, HNSW, Fulltext)
        #    b. If CONCURRENTLY:
        #       - Build new index in background while reads continue on old
        #       - Replay any changes that occurred during build
        #       - Atomically swap old index -> new index
        #       - Drop old index pages
        #    c. If not CONCURRENTLY:
        #       - Acquire exclusive lock on table
        #       - Build new index from heap scan
        #       - Replace old index in catalog
        #       - Release lock
        # 3. Return aggregate statistics

        Ok(result)
    }

    ## Rebuild a single specific index
    F reindex_single_index(self, index_name: &Str) -> Result<ReindexResult, VaisError> {
        ~result = ReindexResult.new();

        # In production implementation:
        #
        # 1. Look up index in catalog
        # 2. Determine index type from catalog entry
        # 3. Based on index type:
        #    B+Tree:
        #      - Scan all leaf entries in the old index
        #      - Build new B+Tree using bulk loading
        #      - Swap root pages
        #    HNSW:
        #      - Read all vectors from vector storage
        #      - Build new HNSW graph (using bulk loader if available)
        #      - Swap meta page
        #    Fulltext:
        #      - Re-read all documents from heap
        #      - Re-tokenize and build new posting lists
        #      - Swap dictionary root

        result.indexes_rebuilt = 1;
        Ok(result)
    }

    ## Rebuild all indexes in the database
    F reindex_database(self) -> Result<ReindexResult, VaisError> {
        ~result = ReindexResult.new();

        # In production implementation:
        #
        # 1. List all tables in catalog
        # 2. For each table, call reindex_table
        # 3. Merge results

        Ok(result)
    }
}

# ============================================================================
# CompactionExecutor — Defragments database files
# ============================================================================

## Database file compaction reclaims fragmented free space:
## - Moves pages to fill gaps left by freed pages
## - Truncates file to release disk space
## - Updates all page references (catalog, indexes, WAL)
##
## More aggressive than VACUUM: actually moves data pages around.
## Requires exclusive access to each file being compacted.
S CompactionExecutor {
    config: CompactionConfig,
    is_running: bool,
    lock: Mutex,
}

X CompactionExecutor {
    ## Create a new compaction executor
    F new(config: CompactionConfig) -> CompactionExecutor {
        CompactionExecutor {
            config,
            is_running: false,
            lock: Mutex.new(),
        }
    }

    ## Execute database compaction
    F execute(
        ~self,
        current_time_ms: u64,
    ) -> Result<CompactionResult, VaisError> {
        ~guard = self.lock.lock();

        I self.is_running {
            R Err(err_compaction_failed("Compaction already in progress"));
        }
        self.is_running = true;

        # Validate config
        self.config.validate()?;

        ~start_ms = current_time_ms;
        ~result = CompactionResult.new();

        # Compact each file type if enabled
        I self.config.compact_data {
            ~data_result = self.compact_file(FILE_ID_DATA)?;
            result.pages_moved = result.pages_moved + data_result.pages_moved;
            result.pages_freed = result.pages_freed + data_result.pages_freed;
            result.files_compacted = result.files_compacted + 1;
        }

        I self.config.compact_vectors {
            ~vec_result = self.compact_file(FILE_ID_VECTORS)?;
            result.pages_moved = result.pages_moved + vec_result.pages_moved;
            result.pages_freed = result.pages_freed + vec_result.pages_freed;
            result.files_compacted = result.files_compacted + 1;
        }

        I self.config.compact_graph {
            ~graph_result = self.compact_file(FILE_ID_GRAPH)?;
            result.pages_moved = result.pages_moved + graph_result.pages_moved;
            result.pages_freed = result.pages_freed + graph_result.pages_freed;
            result.files_compacted = result.files_compacted + 1;
        }

        I self.config.compact_fulltext {
            ~ft_result = self.compact_file(FILE_ID_FULLTEXT)?;
            result.pages_moved = result.pages_moved + ft_result.pages_moved;
            result.pages_freed = result.pages_freed + ft_result.pages_freed;
            result.files_compacted = result.files_compacted + 1;
        }

        result.duration_ms = current_time_ms - start_ms;
        self.is_running = false;
        Ok(result)
    }

    ## Check if compaction is running
    F is_running(self) -> bool {
        self.is_running
    }

    # ========================================================================
    # File Compaction
    # ========================================================================

    ## Compact a single database file
    ## Moves used pages to fill gaps, truncates file end
    F compact_file(self, file_id: u8) -> Result<CompactionResult, VaisError> {
        ~result = CompactionResult.new();

        # In production implementation:
        #
        # 1. Read freelist bitmap for this file
        # 2. Calculate fragmentation ratio
        # 3. If fragmentation < min_fragmentation_percent, skip
        # 4. Build a relocation plan:
        #    a. Find free pages near the beginning of the file
        #    b. Find used pages near the end of the file
        #    c. Plan moves: end-page -> free-slot-near-beginning
        # 5. For each planned move:
        #    a. Read source page
        #    b. Write to destination page
        #    c. Update all references to this page:
        #       - B+Tree parent pointers
        #       - Page chain next/prev pointers
        #       - Catalog root page references
        #       - Index meta page references
        #    d. WAL-log the move
        #    e. Mark source page as free in freelist
        #    f. Mark destination page as used in freelist
        # 6. Truncate file: remove trailing free pages
        # 7. Update file size in metadata
        #
        # I/O throttling: if config.io_limit_mbps > 0, pace the moves

        Ok(result)
    }

    ## Estimate fragmentation percentage for a file
    ## Returns (total_pages, free_pages, fragmentation_percent)
    F estimate_fragmentation(
        self,
        total_pages: u64,
        free_pages: u64,
        free_at_end: u64,
    ) -> f64 {
        # Fragmentation = free pages NOT at the end of file
        # (free pages at the end can be reclaimed by simple truncation)
        I total_pages == 0 {
            R 0.0;
        }
        ~internal_free = I free_pages > free_at_end {
            free_pages - free_at_end
        } E {
            0
        };
        (internal_free as f64) / (total_pages as f64) * 100.0
    }

    ## Check if a file should be compacted based on fragmentation
    F should_compact(self, total_pages: u64, free_pages: u64, free_at_end: u64) -> bool {
        ~frag = self.estimate_fragmentation(total_pages, free_pages, free_at_end);
        frag >= self.config.min_fragmentation_percent
    }
}
