# Production Operations — System Metrics & Health Endpoint
# Phase 9 Task 5: System-wide metrics collection and health/readiness probes
# Provides: SystemMetricsCollector, HealthChecker, health/ready endpoint handlers

U std/vec.Vec;
U std/string.Str;
U std/option.{Option, Some, None};
U std/sync.{RwLock};
U storage/error.{VaisError};
U ops/types.{
    SystemMetrics, HealthStatus,
    HEALTH_HEALTHY, HEALTH_DEGRADED, HEALTH_UNHEALTHY,
    err_health_check_failed,
};
U ops/config.{MetricsConfig};

# ============================================================================
# SystemMetricsCollector — Collects and stores system-wide metrics
# ============================================================================

## Periodically samples system metrics (uptime, connections, memory, disk)
## Thread-safe via RwLock for concurrent read access from /metrics endpoint
S SystemMetricsCollector {
    config: MetricsConfig,
    current: RwLock<SystemMetrics>,
    server_start_time: i64,          # Epoch seconds when server started
    # Counters (monotonically increasing, never reset)
    total_connections: u64,
    total_queries: u64,
    total_transactions: u64,
    total_errors: u64,
}

X SystemMetricsCollector {
    ## Create new metrics collector
    F new(config: MetricsConfig, start_time: i64) -> SystemMetricsCollector {
        SystemMetricsCollector {
            config,
            current: RwLock.new(SystemMetrics.new(start_time)),
            server_start_time: start_time,
            total_connections: 0,
            total_queries: 0,
            total_transactions: 0,
            total_errors: 0,
        }
    }

    ## Record a new connection (increment counter)
    F record_connection(~self) {
        self.total_connections = self.total_connections + 1;
    }

    ## Record a query execution (increment counter)
    F record_query(~self) {
        self.total_queries = self.total_queries + 1;
    }

    ## Record a transaction completion (increment counter)
    F record_transaction(~self) {
        self.total_transactions = self.total_transactions + 1;
    }

    ## Record an error (increment counter)
    F record_error(~self) {
        self.total_errors = self.total_errors + 1;
    }

    ## Collect a fresh metrics snapshot
    ## Called periodically (every config.collection_interval_sec)
    F collect(
        ~self,
        current_time: i64,
        active_connections: u32,
        memory_used: u64,
        memory_budget: u64,
        disk_data_bytes: u64,
        disk_wal_bytes: u64,
    ) {
        ~metrics = SystemMetrics.new(current_time);
        metrics.uptime_secs = (current_time - self.server_start_time) as u64;
        metrics.active_connections = active_connections;
        metrics.total_connections = self.total_connections;
        metrics.memory_used_bytes = memory_used;
        metrics.memory_budget_bytes = memory_budget;
        metrics.disk_data_bytes = disk_data_bytes;
        metrics.disk_wal_bytes = disk_wal_bytes;
        metrics.disk_used_bytes = disk_data_bytes + disk_wal_bytes;
        metrics.total_queries = self.total_queries;
        metrics.total_transactions = self.total_transactions;
        metrics.total_errors = self.total_errors;

        # Update current snapshot under write lock
        ~guard = self.current.write();
        *guard = metrics;
    }

    ## Get current metrics snapshot (read-only)
    F get_current(self) -> SystemMetrics {
        ~guard = self.current.read();
        guard.clone()
    }

    ## Format metrics as JSON string for /metrics endpoint
    F to_json(self) -> Str {
        ~m = self.get_current();
        "{{\"uptime_secs\":{m.uptime_secs},\"active_connections\":{m.active_connections},\"total_connections\":{m.total_connections},\"memory_used_bytes\":{m.memory_used_bytes},\"memory_budget_bytes\":{m.memory_budget_bytes},\"disk_used_bytes\":{m.disk_used_bytes},\"disk_data_bytes\":{m.disk_data_bytes},\"disk_wal_bytes\":{m.disk_wal_bytes},\"total_queries\":{m.total_queries},\"total_transactions\":{m.total_transactions},\"total_errors\":{m.total_errors}}}"
    }
}

# ============================================================================
# HealthChecker — Performs health and readiness checks
# ============================================================================

## Checks all engine subsystems and produces a HealthStatus
## Used by /health (liveness) and /ready (readiness) endpoints
S HealthChecker {
    disk_space_threshold_bytes: u64,  # Warn if free disk < this (default 1GB)
    wal_age_threshold_sec: u64,       # Warn if checkpoint age > this (default 1800)
    buffer_pool_dirty_threshold: f64, # Warn if dirty ratio > this (default 0.9)
}

X HealthChecker {
    F new() -> HealthChecker {
        HealthChecker {
            disk_space_threshold_bytes: 1073741824,  # 1GB
            wal_age_threshold_sec: 1800,             # 30 minutes
            buffer_pool_dirty_threshold: 0.9,        # 90%
        }
    }

    ## Builder: set disk space threshold
    F with_disk_threshold(~self, bytes: u64) -> HealthChecker {
        self.disk_space_threshold_bytes = bytes;
        self
    }

    ## Builder: set WAL age threshold
    F with_wal_age_threshold(~self, secs: u64) -> HealthChecker {
        self.wal_age_threshold_sec = secs;
        self
    }

    ## Perform full health check
    ## Parameters represent current state from each subsystem
    F check(
        self,
        current_time: i64,
        sql_engine_open: bool,
        vector_engine_open: bool,
        graph_engine_open: bool,
        fulltext_engine_open: bool,
        rag_engine_open: bool,
        buffer_pool_dirty_ratio: f64,
        wal_checkpoint_age_sec: u64,
        disk_free_bytes: u64,
    ) -> HealthStatus {
        ~status = HealthStatus.healthy(current_time);

        # Check individual engines
        status.sql_engine_ok = sql_engine_open;
        status.vector_engine_ok = vector_engine_open;
        status.graph_engine_ok = graph_engine_open;
        status.fulltext_engine_ok = fulltext_engine_open;
        status.rag_engine_ok = rag_engine_open;

        # Check buffer pool
        status.buffer_pool_ok = buffer_pool_dirty_ratio < self.buffer_pool_dirty_threshold;

        # Check WAL (checkpoint not too old)
        status.wal_ok = wal_checkpoint_age_sec < self.wal_age_threshold_sec;

        # Check disk space
        status.disk_space_ok = disk_free_bytes >= self.disk_space_threshold_bytes;

        # Compute overall status
        status.compute_status();

        status
    }

    ## Quick liveness check (returns true if core systems are functional)
    F is_alive(
        self,
        sql_engine_open: bool,
        buffer_pool_ok: bool,
    ) -> bool {
        sql_engine_open && buffer_pool_ok
    }

    ## Format health status as JSON for /health endpoint
    F health_to_json(self, status: &HealthStatus) -> Str {
        ~s = status.status_string();
        "{{\"status\":\"{s}\",\"engines\":{{\"sql\":{status.sql_engine_ok},\"vector\":{status.vector_engine_ok},\"graph\":{status.graph_engine_ok},\"fulltext\":{status.fulltext_engine_ok},\"rag\":{status.rag_engine_ok}}},\"buffer_pool_ok\":{status.buffer_pool_ok},\"wal_ok\":{status.wal_ok},\"disk_space_ok\":{status.disk_space_ok},\"message\":\"{status.message}\"}}"
    }

    ## Format readiness as JSON for /ready endpoint
    F ready_to_json(self, status: &HealthStatus) -> Str {
        ~ready = status.is_ready();
        "{{\"ready\":{ready},\"status\":\"{status.status_string()}\"}}"
    }
}
