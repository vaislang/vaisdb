# Chunking Strategies — Document splitting strategies
# Provides fixed-size, sentence-boundary, and paragraph-boundary chunking
# Each strategy splits text into byte ranges that respect logical boundaries

U std/vec.Vec;
U std/string.Str;

# ============================================================================
# ChunkBoundary — Represents a chunk's byte range in the original text
# ============================================================================

S ChunkBoundary {
    start_byte: u32,            # Start byte offset in original text
    end_byte: u32,              # End byte offset (exclusive)
}

X ChunkBoundary {
    F new(start_byte: u32, end_byte: u32) -> ChunkBoundary {
        ChunkBoundary { start_byte, end_byte }
    }
}

# ============================================================================
# FixedSizeStrategy — Split by token count with word boundary respect
# ============================================================================

## Split text into fixed-size chunks by token count
## Respects word boundaries and targets the specified token count per chunk
## Falls back to mid-word split if a single word exceeds max_size
F split_fixed_size(
    content: &Str,
    target_size: u32,
    min_size: u32,
    max_size: u32,
) -> Vec<ChunkBoundary> {
    ~boundaries = Vec.new();
    ~bytes = content.as_bytes();
    ~len = bytes.len();

    I len == 0 {
        R boundaries;
    }

    ~chunk_start: u64 = 0;
    ~token_count: u32 = 0;
    ~pos: u64 = 0;
    ~last_word_end: u64 = 0;  # Last valid word boundary

    L while pos < len {
        # Skip whitespace
        I is_whitespace(bytes[pos]) {
            I pos > chunk_start && !is_whitespace(bytes[pos - 1]) {
                last_word_end = pos;
            }
            pos += 1;
            C;
        }

        # Start of a word
        ~word_start = pos;
        L while pos < len && !is_whitespace(bytes[pos]) {
            pos += 1;
        }

        # Completed a word
        token_count += 1;
        last_word_end = pos;

        # Check if we should create a chunk
        I token_count >= target_size {
            # Use word boundary if available and meets min_size
            ~chunk_end = last_word_end;
            ~chunk_tokens = count_tokens_in_range(content, chunk_start, chunk_end);

            # If chunk is too small, try to include more words
            I chunk_tokens < min_size && pos < len {
                # Continue to next word
                C;
            }

            # If chunk exceeds max_size, we may need to split mid-word
            I chunk_tokens > max_size {
                # Try to find a smaller boundary
                ~fallback_end = find_max_boundary(content, chunk_start, last_word_end, max_size);
                I fallback_end > chunk_start {
                    chunk_end = fallback_end;
                }
            }

            boundaries.push(ChunkBoundary.new(chunk_start as u32, chunk_end as u32));
            chunk_start = chunk_end;

            # Skip any whitespace at start of next chunk
            L while chunk_start < len && is_whitespace(bytes[chunk_start]) {
                chunk_start += 1;
            }

            pos = chunk_start;
            token_count = 0;
            last_word_end = chunk_start;
        }
    }

    # Add final chunk if any content remains
    I chunk_start < len {
        ~final_tokens = count_tokens_in_range(content, chunk_start, len);
        # Only add if it meets minimum size or it's the only chunk
        I final_tokens >= min_size || boundaries.is_empty() {
            boundaries.push(ChunkBoundary.new(chunk_start as u32, len as u32));
        } E I !boundaries.is_empty() {
            # Merge with previous chunk if too small
            ~last_idx = boundaries.len() - 1;
            boundaries[last_idx].end_byte = len as u32;
        }
    }

    boundaries
}

# ============================================================================
# SentenceBoundaryStrategy — Split at sentence boundaries
# ============================================================================

## Split text at sentence boundaries (. ! ?)
## Accumulates sentences until target_size is reached
## Falls back to fixed-size if a single sentence exceeds max_size
F split_sentence_boundary(
    content: &Str,
    target_size: u32,
    min_size: u32,
    max_size: u32,
) -> Vec<ChunkBoundary> {
    ~boundaries = Vec.new();
    ~sentence_ends = find_sentence_boundaries(content);

    I sentence_ends.is_empty() {
        # No sentence boundaries found, fall back to fixed-size
        R split_fixed_size(content, target_size, min_size, max_size);
    }

    ~chunk_start: u64 = 0;
    ~i: u64 = 0;

    L while i < sentence_ends.len() {
        ~sentence_end = sentence_ends[i];
        ~chunk_tokens = count_tokens_in_range(content, chunk_start, sentence_end);

        # Check if adding this sentence would exceed target
        I chunk_tokens >= target_size {
            # Check if this is a single giant sentence
            I chunk_start == sentence_ends[I i > 0 { i - 1 } E { 0 }] {
                # Single sentence is too large, fall back to fixed-size for this chunk
                ~fixed_chunks = split_fixed_size(
                    &content.substring(chunk_start, sentence_end),
                    target_size,
                    min_size,
                    max_size,
                );

                ~j: u64 = 0;
                L while j < fixed_chunks.len() {
                    boundaries.push(ChunkBoundary.new(
                        (chunk_start + fixed_chunks[j].start_byte as u64) as u32,
                        (chunk_start + fixed_chunks[j].end_byte as u64) as u32,
                    ));
                    j += 1;
                }
                chunk_start = sentence_end;
            } E I chunk_tokens <= max_size {
                # Good chunk size, create boundary
                boundaries.push(ChunkBoundary.new(chunk_start as u32, sentence_end as u32));
                chunk_start = sentence_end;
            } E {
                # Exceeded max_size, create chunk at previous sentence
                ~prev_end = I i > 0 { sentence_ends[i - 1] } E { sentence_end };
                I prev_end > chunk_start {
                    boundaries.push(ChunkBoundary.new(chunk_start as u32, prev_end as u32));
                    chunk_start = prev_end;
                    # Don't increment i, process this sentence in next iteration
                    C;
                } E {
                    # Edge case: first sentence, just split it
                    boundaries.push(ChunkBoundary.new(chunk_start as u32, sentence_end as u32));
                    chunk_start = sentence_end;
                }
            }
        }

        i += 1;
    }

    # Add final chunk
    ~bytes = content.as_bytes();
    ~content_len = bytes.len();
    I chunk_start < content_len {
        ~final_tokens = count_tokens_in_range(content, chunk_start, content_len);
        I final_tokens >= min_size || boundaries.is_empty() {
            boundaries.push(ChunkBoundary.new(chunk_start as u32, content_len as u32));
        } E I !boundaries.is_empty() {
            # Merge with previous chunk
            ~last_idx = boundaries.len() - 1;
            boundaries[last_idx].end_byte = content_len as u32;
        }
    }

    boundaries
}

# ============================================================================
# ParagraphBoundaryStrategy — Split at paragraph boundaries
# ============================================================================

## Split text at paragraph boundaries (double newline)
## Accumulates paragraphs until target_size is reached
## Falls back to sentence split if a single paragraph exceeds max_size
F split_paragraph_boundary(
    content: &Str,
    target_size: u32,
    min_size: u32,
    max_size: u32,
) -> Vec<ChunkBoundary> {
    ~boundaries = Vec.new();
    ~paragraph_ends = find_paragraph_boundaries(content);

    I paragraph_ends.is_empty() {
        # No paragraph boundaries found, fall back to sentence-based
        R split_sentence_boundary(content, target_size, min_size, max_size);
    }

    ~chunk_start: u64 = 0;
    ~i: u64 = 0;

    L while i < paragraph_ends.len() {
        ~para_end = paragraph_ends[i];
        ~chunk_tokens = count_tokens_in_range(content, chunk_start, para_end);

        I chunk_tokens >= target_size {
            I chunk_tokens <= max_size {
                # Good chunk size
                boundaries.push(ChunkBoundary.new(chunk_start as u32, para_end as u32));
                chunk_start = para_end;
            } E {
                # Paragraph too large, fall back to sentence splitting
                ~para_start = I i > 0 { paragraph_ends[i - 1] } E { 0 };
                I para_start < para_end {
                    ~sentence_chunks = split_sentence_boundary(
                        &content.substring(para_start, para_end),
                        target_size,
                        min_size,
                        max_size,
                    );

                    ~j: u64 = 0;
                    L while j < sentence_chunks.len() {
                        boundaries.push(ChunkBoundary.new(
                            (para_start + sentence_chunks[j].start_byte as u64) as u32,
                            (para_start + sentence_chunks[j].end_byte as u64) as u32,
                        ));
                        j += 1;
                    }
                }
                chunk_start = para_end;
            }
        }

        i += 1;
    }

    # Add final chunk
    ~bytes = content.as_bytes();
    ~content_len = bytes.len();
    I chunk_start < content_len {
        ~final_tokens = count_tokens_in_range(content, chunk_start, content_len);
        I final_tokens >= min_size || boundaries.is_empty() {
            boundaries.push(ChunkBoundary.new(chunk_start as u32, content_len as u32));
        } E I !boundaries.is_empty() {
            # Merge with previous chunk
            ~last_idx = boundaries.len() - 1;
            boundaries[last_idx].end_byte = content_len as u32;
        }
    }

    boundaries
}

# ============================================================================
# Helper Functions — Boundary detection and token counting
# ============================================================================

## Find all sentence boundaries in text (. ! ? followed by whitespace or end)
F find_sentence_boundaries(content: &Str) -> Vec<u64> {
    ~boundaries = Vec.new();
    ~bytes = content.as_bytes();
    ~len = bytes.len();
    ~i: u64 = 0;

    L while i < len {
        ~b = bytes[i];

        # Check for sentence terminators
        I b == 46 || b == 33 || b == 63 {  # . ! ?
            # Look ahead to confirm this is a sentence boundary
            ~j = i + 1;

            # Skip additional punctuation (e.g., "..." or "?!")
            L while j < len && (bytes[j] == 46 || bytes[j] == 33 || bytes[j] == 63) {
                j += 1;
            }

            # Check for whitespace or end of text
            I j >= len || is_whitespace(bytes[j]) {
                # Skip trailing whitespace to find actual content end
                L while j < len && is_whitespace(bytes[j]) {
                    j += 1;
                }
                boundaries.push(j);
                i = j;
                C;
            }
        }

        i += 1;
    }

    boundaries
}

## Find all paragraph boundaries in text (double newline)
F find_paragraph_boundaries(content: &Str) -> Vec<u64> {
    ~boundaries = Vec.new();
    ~bytes = content.as_bytes();
    ~len = bytes.len();
    ~i: u64 = 0;

    L while i < len {
        # Look for \n\n (with possible \r)
        I bytes[i] == 10 {  # \n
            ~j = i + 1;

            # Skip whitespace including additional newlines
            ~newline_count = 1;
            L while j < len && (bytes[j] == 10 || bytes[j] == 13 || bytes[j] == 32 || bytes[j] == 9) {
                I bytes[j] == 10 {
                    newline_count += 1;
                }
                j += 1;
            }

            # At least 2 newlines indicates paragraph boundary
            I newline_count >= 2 {
                boundaries.push(j);
                i = j;
                C;
            }
        }

        i += 1;
    }

    boundaries
}

## Count tokens (whitespace-delimited words) in a byte range
F count_tokens_in_range(content: &Str, start: u64, end: u64) -> u32 {
    I start >= end {
        R 0;
    }

    ~bytes = content.as_bytes();
    ~count: u32 = 0;
    ~pos = start;
    ~in_word = false;

    L while pos < end && pos < bytes.len() {
        I is_whitespace(bytes[pos]) {
            in_word = false;
        } E {
            I !in_word {
                count += 1;
                in_word = true;
            }
        }
        pos += 1;
    }

    count
}

## Find the maximum valid word boundary before max_size tokens
F find_max_boundary(content: &Str, start: u64, end: u64, max_size: u32) -> u64 {
    ~bytes = content.as_bytes();
    ~pos = start;
    ~token_count: u32 = 0;
    ~last_boundary = start;

    L while pos < end && pos < bytes.len() {
        # Skip whitespace
        I is_whitespace(bytes[pos]) {
            last_boundary = pos;
            pos += 1;
            C;
        }

        # Start of word
        L while pos < end && pos < bytes.len() && !is_whitespace(bytes[pos]) {
            pos += 1;
        }

        token_count += 1;

        I token_count >= max_size {
            # Use last boundary before exceeding max
            I last_boundary > start {
                R last_boundary;
            }
            R pos;  # No good boundary, use current position
        }

        last_boundary = pos;
    }

    end
}

## Check if byte is whitespace (space, tab, newline, carriage return)
F is_whitespace(b: u8) -> bool {
    b == 32 || b == 9 || b == 10 || b == 13
}
