# Phase 11: FullText Engine Unit Tests
# Tests: Tokenizer, BM25Scorer, VByte encoding, delta encoding,
#         PostingEntry, DictEntry, FullTextConfig, FullTextMeta,
#         PostingPageHeader, compression round-trips, stop words

U std/test.{assert_eq, assert_ne, assert_true, assert_false, describe, it};
U std/bytes.{ByteBuffer};

U fulltext/types.{
    FullTextConfig, FullTextMeta, PostingEntry, PostingPageHeader,
    DictEntry, TokenInfo, DocumentStats,
    FULLTEXT_FORMAT_VERSION, DEFAULT_BM25_K1, DEFAULT_BM25_B,
    DEFAULT_MAX_TOKEN_LENGTH, DEFAULT_MIN_TOKEN_LENGTH,
    POSTING_ENTRY_FIXED_SIZE, POSTING_PAGE_HEADER_SIZE,
    POSTING_FLAG_HAS_DELETION_BITMAP, POSTING_FLAG_COMPRESSED,
    MAX_POSITIONS_PER_ENTRY,
};
U fulltext/tokenizer.{Tokenizer, TermFreqInfo, is_word_byte, to_ascii_lowercase};
U fulltext/search/bm25.{BM25Scorer, ln};
U fulltext/index/compression.{
    vbyte_encode, vbyte_decode, delta_encode_doc_ids, delta_decode_doc_ids,
    PostingEntryCompact, encode_posting_list, decode_posting_list,
};
U storage/constants.{INVALID_TXN_ID, NULL_PAGE, DEFAULT_PAGE_SIZE};
U storage/hash.{fnv1a_hash};

# ============================================================================
# Constants
# ============================================================================

describe("FullText Constants") {
    it("should have correct format version") {
        assert_eq(FULLTEXT_FORMAT_VERSION, 1);
    }

    it("should have correct BM25 defaults") {
        assert_true(DEFAULT_BM25_K1 > 1.19 && DEFAULT_BM25_K1 < 1.21);
        assert_true(DEFAULT_BM25_B > 0.74 && DEFAULT_BM25_B < 0.76);
    }

    it("should have correct size constants") {
        assert_eq(POSTING_ENTRY_FIXED_SIZE, 40);
        assert_eq(POSTING_PAGE_HEADER_SIZE, 16);
    }
}

# ============================================================================
# Tokenizer
# ============================================================================

describe("Tokenizer") {
    it("should tokenize simple text") {
        ~tokenizer = Tokenizer.default();
        ~tokens = tokenizer.tokenize(&"hello world");
        assert_eq(tokens.len(), 2);
        assert_eq(tokens[0].term, "hello");
        assert_eq(tokens[1].term, "world");
    }

    it("should lowercase tokens") {
        ~tokenizer = Tokenizer.default();
        ~tokens = tokenizer.tokenize(&"Hello WORLD");
        assert_eq(tokens[0].term, "hello");
        assert_eq(tokens[1].term, "world");
    }

    it("should remove stop words") {
        ~tokenizer = Tokenizer.default();
        ~tokens = tokenizer.tokenize(&"the quick brown fox");
        # "the" is a stop word and should be removed
        ~has_the = false;
        L tok: &tokens {
            I tok.term == "the" { has_the = true; }
        }
        assert_false(has_the);
    }

    it("should track token positions") {
        ~tokenizer = Tokenizer.default();
        ~tokens = tokenizer.tokenize(&"database vector search");
        assert_eq(tokens[0].position, 0);
        assert_eq(tokens[1].position, 1);
        assert_eq(tokens[2].position, 2);
    }

    it("should skip position for stop words (preserving phrase distances)") {
        ~tokenizer = Tokenizer.default();
        # "the" is a stop word at position 0; "cat" gets position 1
        ~tokens = tokenizer.tokenize(&"the cat");
        # "the" is removed but "cat" has position 1 (not 0)
        assert_eq(tokens.len(), 1);
        assert_eq(tokens[0].term, "cat");
        assert_eq(tokens[0].position, 1);
    }

    it("should handle punctuation as word boundary") {
        ~tokenizer = Tokenizer.default();
        ~tokens = tokenizer.tokenize(&"hello, world! how?");
        # Expect: hello, world (how is a stop word)
        ~found_hello = false;
        ~found_world = false;
        L tok: &tokens {
            I tok.term == "hello" { found_hello = true; }
            I tok.term == "world" { found_world = true; }
        }
        assert_true(found_hello);
        assert_true(found_world);
    }

    it("should handle empty text") {
        ~tokenizer = Tokenizer.default();
        ~tokens = tokenizer.tokenize(&"");
        assert_eq(tokens.len(), 0);
    }

    it("should handle text with only stop words") {
        ~tokenizer = Tokenizer.default();
        ~tokens = tokenizer.tokenize(&"the is a an");
        assert_eq(tokens.len(), 0);
    }

    it("should tokenize_with_freqs correctly") {
        ~tokenizer = Tokenizer.default();
        ~freqs = tokenizer.tokenize_with_freqs(&"hello world hello");
        # "hello" appears 2 times, "world" appears 1 time
        ~hello_freq: u32 = 0;
        ~world_freq: u32 = 0;
        L tf: &freqs {
            I tf.term == "hello" { hello_freq = tf.freq; }
            I tf.term == "world" { world_freq = tf.freq; }
        }
        assert_eq(hello_freq, 2);
        assert_eq(world_freq, 1);
    }

    it("should record multiple positions for repeated terms") {
        ~tokenizer = Tokenizer.default();
        ~freqs = tokenizer.tokenize_with_freqs(&"cat dog cat");
        L tf: &freqs {
            I tf.term == "cat" {
                assert_eq(tf.positions.len(), 2);
                assert_eq(tf.positions[0], 0);
                assert_eq(tf.positions[1], 2);
            }
        }
    }
}

describe("Tokenizer Helpers") {
    it("should classify word bytes correctly") {
        # Letters
        assert_true(is_word_byte(65));  # A
        assert_true(is_word_byte(122)); # z
        # Digits
        assert_true(is_word_byte(48));  # 0
        assert_true(is_word_byte(57));  # 9
        # Underscore
        assert_true(is_word_byte(95));  # _
        # Non-word
        assert_false(is_word_byte(32)); # space
        assert_false(is_word_byte(44)); # comma
        assert_false(is_word_byte(46)); # period
    }

    it("should lowercase ASCII correctly") {
        assert_eq(to_ascii_lowercase(&"Hello WORLD"), "hello world");
        assert_eq(to_ascii_lowercase(&"already lowercase"), "already lowercase");
        assert_eq(to_ascii_lowercase(&"ABC 123"), "abc 123");
    }
}

# ============================================================================
# BM25 Scorer
# ============================================================================

describe("BM25Scorer") {
    it("should create with default params") {
        ~scorer = BM25Scorer.new();
        assert_true(scorer.k1 > 1.19 && scorer.k1 < 1.21);
        assert_true(scorer.b > 0.74 && scorer.b < 0.76);
    }

    it("should create with custom params") {
        ~scorer = BM25Scorer.with_params(1.5, 0.8);
        assert_true(scorer.k1 > 1.49 && scorer.k1 < 1.51);
        assert_true(scorer.b > 0.79 && scorer.b < 0.81);
    }

    it("should calculate IDF correctly") {
        # IDF for term in 10 docs out of 100 total
        ~idf = BM25Scorer.calculate_idf(10, 100);
        assert_true(idf > 2.2 && idf < 2.3);
    }

    it("should return 0 IDF for edge cases") {
        # df=0 → IDF=0
        assert_true(BM25Scorer.calculate_idf(0, 100) < 0.001);
        # total_docs=0 → IDF=0
        assert_true(BM25Scorer.calculate_idf(10, 0) < 0.001);
        # df > total_docs → IDF=0
        assert_true(BM25Scorer.calculate_idf(150, 100) < 0.001);
    }

    it("should score a single term positively") {
        ~scorer = BM25Scorer.new();
        ~score = scorer.score(3, 50, 100, 120.0, 1000);
        assert_true(score > 0.0);
    }

    it("should return 0 for tf=0") {
        ~scorer = BM25Scorer.new();
        assert_true(scorer.score(0, 50, 100, 120.0, 1000) < 0.001);
    }

    it("should return 0 for total_docs=0") {
        ~scorer = BM25Scorer.new();
        assert_true(scorer.score(3, 50, 100, 120.0, 0) < 0.001);
    }

    it("should return higher score for higher TF") {
        ~scorer = BM25Scorer.new();
        ~score_low = scorer.score(1, 50, 100, 120.0, 1000);
        ~score_high = scorer.score(5, 50, 100, 120.0, 1000);
        assert_true(score_high > score_low);
    }

    it("should return higher score for rarer terms (lower df)") {
        ~scorer = BM25Scorer.new();
        ~score_common = scorer.score(3, 500, 100, 120.0, 1000);
        ~score_rare = scorer.score(3, 5, 100, 120.0, 1000);
        assert_true(score_rare > score_common);
    }

    it("should penalize long documents (higher doc_length)") {
        ~scorer = BM25Scorer.new();
        ~score_short = scorer.score(3, 50, 50, 120.0, 1000);
        ~score_long = scorer.score(3, 50, 500, 120.0, 1000);
        assert_true(score_short > score_long);
    }

    it("should batch_score by summing") {
        ~scorer = BM25Scorer.new();
        ~scores = vec![1.5, 2.3, 0.8];
        ~total = scorer.batch_score(&scores);
        assert_true(total > 4.59 && total < 4.61);
    }

    it("should score multi-term queries") {
        ~scorer = BM25Scorer.new();
        ~term_freqs: Vec<(u32, u32)> = vec![(3, 50), (2, 30)];
        ~total = scorer.score_multi_term(&term_freqs, 100, 120.0, 1000);
        assert_true(total > 0.0);
    }
}

describe("Natural Log Approximation") {
    it("should compute ln(1) = 0") {
        assert_true(ln(1.0) < 0.0001 && ln(1.0) > -0.0001);
    }

    it("should compute ln(e) ≈ 1") {
        ~e: f64 = 2.718281828459045;
        ~result = ln(e);
        assert_true(result > 0.99 && result < 1.01);
    }

    it("should compute ln(10) ≈ 2.3026") {
        ~result = ln(10.0);
        assert_true(result > 2.30 && result < 2.31);
    }

    it("should compute ln(0.5) ≈ -0.6931") {
        ~result = ln(0.5);
        assert_true(result < -0.69 && result > -0.70);
    }

    it("should return 0 for x <= 0") {
        assert_true(ln(0.0) < 0.001);
        assert_true(ln(-1.0) < 0.001);
    }
}

# ============================================================================
# VByte Encoding
# ============================================================================

describe("VByte Encoding") {
    it("should encode 0 as single byte") {
        ~encoded = vbyte_encode(0);
        assert_eq(encoded.len(), 1);
        assert_eq(encoded[0], 0);
    }

    it("should encode small values in 1 byte") {
        ~encoded = vbyte_encode(127);
        assert_eq(encoded.len(), 1);
        assert_eq(encoded[0], 127);
    }

    it("should encode 128 in 2 bytes") {
        ~encoded = vbyte_encode(128);
        assert_eq(encoded.len(), 2);
    }

    it("should round-trip small values") {
        ~values: [u64] = [0, 1, 42, 127, 128, 255, 1000, 65535];
        L val: &values {
            ~encoded = vbyte_encode(*val);
            ~(decoded, consumed) = vbyte_decode(&encoded, 0)!;
            assert_eq(decoded, *val);
            assert_eq(consumed, encoded.len());
        }
    }

    it("should round-trip large values") {
        ~values: [u64] = [100000, 1000000, 4294967295, u64.MAX];
        L val: &values {
            ~encoded = vbyte_encode(*val);
            ~(decoded, _) = vbyte_decode(&encoded, 0)!;
            assert_eq(decoded, *val);
        }
    }
}

# ============================================================================
# Delta Encoding
# ============================================================================

describe("Delta Encoding") {
    it("should round-trip sorted doc_ids") {
        ~doc_ids = vec![10u64, 15, 23, 30, 100];
        ~encoded = delta_encode_doc_ids(&doc_ids);
        ~decoded = delta_decode_doc_ids(&encoded)!;

        assert_eq(decoded.len(), 5);
        assert_eq(decoded[0], 10);
        assert_eq(decoded[1], 15);
        assert_eq(decoded[2], 23);
        assert_eq(decoded[3], 30);
        assert_eq(decoded[4], 100);
    }

    it("should handle single element") {
        ~doc_ids = vec![42u64];
        ~encoded = delta_encode_doc_ids(&doc_ids);
        ~decoded = delta_decode_doc_ids(&encoded)!;
        assert_eq(decoded.len(), 1);
        assert_eq(decoded[0], 42);
    }

    it("should handle empty list") {
        ~doc_ids: Vec<u64> = Vec.new();
        ~encoded = delta_encode_doc_ids(&doc_ids);
        ~decoded = delta_decode_doc_ids(&encoded)!;
        assert_eq(decoded.len(), 0);
    }

    it("should save space with delta encoding") {
        # Large doc_ids with small gaps: deltas should be more compact
        ~doc_ids = vec![1000000u64, 1000001, 1000002, 1000003, 1000004];
        ~encoded = delta_encode_doc_ids(&doc_ids);

        # Raw encoding of 5 u64s = 40 bytes
        # Delta encoding should be much smaller since deltas are 1
        assert_true(encoded.len() < 40);
    }

    it("should handle consecutive doc_ids") {
        ~doc_ids = vec![1u64, 2, 3, 4, 5, 6, 7, 8, 9, 10];
        ~encoded = delta_encode_doc_ids(&doc_ids);
        ~decoded = delta_decode_doc_ids(&encoded)!;

        assert_eq(decoded.len(), 10);
        L i: 0..10 {
            assert_eq(decoded[i as u64], (i + 1) as u64);
        }
    }
}

# ============================================================================
# PostingEntry
# ============================================================================

describe("PostingEntry") {
    it("should create new active entry") {
        ~positions = vec![0u32, 5, 10];
        ~entry = PostingEntry.new(42, 3, positions, 100, 0);

        assert_eq(entry.doc_id, 42);
        assert_eq(entry.term_freq, 3);
        assert_eq(entry.positions.len(), 3);
        assert_eq(entry.txn_id_create, 100);
        assert_eq(entry.txn_id_expire, INVALID_TXN_ID);
        assert_true(entry.is_active());
    }

    it("should mark as expired") {
        ~entry = PostingEntry.new(1, 1, Vec.new(), 10, 0);
        entry.set_expired(20, 1);
        assert_false(entry.is_active());
        assert_eq(entry.txn_id_expire, 20);
    }

    it("should clear expiration (rollback)") {
        ~entry = PostingEntry.new(1, 1, Vec.new(), 10, 0);
        entry.set_expired(20, 1);
        entry.clear_expired();
        assert_true(entry.is_active());
    }

    it("should compute serialized size correctly") {
        ~positions = vec![0u32, 5, 10];
        ~entry = PostingEntry.new(1, 3, positions, 10, 0);
        # POSTING_ENTRY_FIXED_SIZE(40) + positions(3 * 4) = 52
        assert_eq(entry.serialized_size(), 52);
    }

    it("should serialize/deserialize round-trip") {
        ~positions = vec![0u32, 5, 10, 15];
        ~entry = PostingEntry.new(42, 4, positions, 100, 5);

        ~buf = ByteBuffer.new();
        entry.serialize(&buf);

        ~read_buf = ByteBuffer.wrap_readonly(buf.as_bytes());
        ~restored = PostingEntry.deserialize(&read_buf)!;

        assert_eq(restored.doc_id, 42);
        assert_eq(restored.term_freq, 4);
        assert_eq(restored.positions.len(), 4);
        assert_eq(restored.positions[0], 0);
        assert_eq(restored.positions[3], 15);
        assert_eq(restored.txn_id_create, 100);
        assert_eq(restored.cmd_id, 5);
        assert_true(restored.is_active());
    }

    it("should round-trip via to_bytes/from_bytes") {
        ~entry = PostingEntry.new(99, 2, vec![1u32, 3], 50, 0);
        ~bytes = entry.to_bytes();
        ~restored = PostingEntry.from_bytes(&bytes)!;
        assert_eq(restored.doc_id, 99);
        assert_eq(restored.term_freq, 2);
    }
}

# ============================================================================
# PostingPageHeader
# ============================================================================

describe("PostingPageHeader") {
    it("should create with defaults") {
        ~hdr = PostingPageHeader.new(12345);
        assert_eq(hdr.term_hash, 12345);
        assert_eq(hdr.entry_count, 0);
        assert_eq(hdr.next_posting_page, NULL_PAGE);
        assert_eq(hdr.flags, 0);
    }

    it("should check flags") {
        ~hdr = PostingPageHeader.new(0);
        assert_false(hdr.has_deletion_bitmap());
        assert_false(hdr.is_compressed());

        hdr.flags = POSTING_FLAG_HAS_DELETION_BITMAP;
        assert_true(hdr.has_deletion_bitmap());
        assert_false(hdr.is_compressed());

        hdr.flags = POSTING_FLAG_COMPRESSED;
        assert_false(hdr.has_deletion_bitmap());
        assert_true(hdr.is_compressed());
    }

    it("should serialize/deserialize round-trip (16 bytes)") {
        ~hdr = PostingPageHeader.new(999);
        hdr.entry_count = 42;
        hdr.next_posting_page = 100;
        hdr.flags = POSTING_FLAG_COMPRESSED;

        ~buf = ByteBuffer.new();
        hdr.serialize(&buf);

        ~read_buf = ByteBuffer.wrap_readonly(buf.as_bytes());
        ~restored = PostingPageHeader.deserialize(&read_buf)!;

        assert_eq(restored.term_hash, 999);
        assert_eq(restored.entry_count, 42);
        assert_eq(restored.next_posting_page, 100);
        assert_true(restored.is_compressed());
    }
}

# ============================================================================
# DictEntry
# ============================================================================

describe("DictEntry") {
    it("should create with defaults") {
        ~entry = DictEntry.new("hello", fnv1a_hash(&"hello"), 10);
        assert_eq(entry.term, "hello");
        assert_eq(entry.posting_head_page, 10);
        assert_eq(entry.doc_freq, 0);
        assert_eq(entry.total_term_freq, 0);
    }

    it("should increment doc frequency") {
        ~entry = DictEntry.new("test", 0, 0);
        entry.inc_doc_freq(3);
        assert_eq(entry.doc_freq, 1);
        assert_eq(entry.total_term_freq, 3);

        entry.inc_doc_freq(2);
        assert_eq(entry.doc_freq, 2);
        assert_eq(entry.total_term_freq, 5);
    }

    it("should decrement doc frequency") {
        ~entry = DictEntry.new("test", 0, 0);
        entry.inc_doc_freq(3);
        entry.inc_doc_freq(2);
        # doc_freq=2, total_term_freq=5

        entry.dec_doc_freq(3);
        assert_eq(entry.doc_freq, 1);
        assert_eq(entry.total_term_freq, 2);
    }

    it("should not underflow on decrement") {
        ~entry = DictEntry.new("test", 0, 0);
        entry.dec_doc_freq(5);
        assert_eq(entry.doc_freq, 0);
        assert_eq(entry.total_term_freq, 0);
    }

    it("should serialize/deserialize round-trip") {
        ~hash = fnv1a_hash(&"database");
        ~entry = DictEntry.new("database", hash, 42);
        entry.doc_freq = 100;
        entry.total_term_freq = 500;

        ~buf = ByteBuffer.new();
        entry.serialize(&buf);

        ~read_buf = ByteBuffer.wrap_readonly(buf.as_bytes());
        ~restored = DictEntry.deserialize(&read_buf)!;

        assert_eq(restored.term, "database");
        assert_eq(restored.term_hash, hash);
        assert_eq(restored.posting_head_page, 42);
        assert_eq(restored.doc_freq, 100);
        assert_eq(restored.total_term_freq, 500);
    }
}

# ============================================================================
# FullTextConfig
# ============================================================================

describe("FullTextConfig") {
    it("should create with defaults") {
        ~config = FullTextConfig.default(DEFAULT_PAGE_SIZE);
        assert_true(config.bm25_k1 > 1.19 && config.bm25_k1 < 1.21);
        assert_true(config.bm25_b > 0.74 && config.bm25_b < 0.76);
        assert_eq(config.max_token_length, DEFAULT_MAX_TOKEN_LENGTH);
        assert_eq(config.min_token_length, DEFAULT_MIN_TOKEN_LENGTH);
        assert_true(config.enable_stop_words);
        assert_true(config.enable_positions);
    }

    it("should serialize/deserialize round-trip") {
        ~config = FullTextConfig.default(DEFAULT_PAGE_SIZE);
        ~buf = ByteBuffer.new();
        config.serialize(&buf);

        ~read_buf = ByteBuffer.wrap_readonly(buf.as_bytes());
        ~restored = FullTextConfig.deserialize(&read_buf, DEFAULT_PAGE_SIZE)!;

        assert_true(restored.bm25_k1 > 1.19 && restored.bm25_k1 < 1.21);
        assert_true(restored.enable_stop_words);
    }
}

# ============================================================================
# FullTextMeta
# ============================================================================

describe("FullTextMeta") {
    it("should initialize empty metadata") {
        ~config = FullTextConfig.default(DEFAULT_PAGE_SIZE);
        ~meta = FullTextMeta.new(config);

        assert_eq(meta.format_version, FULLTEXT_FORMAT_VERSION);
        assert_eq(meta.total_docs, 0);
        assert_eq(meta.total_terms, 0);
        assert_eq(meta.total_tokens, 0);
        assert_true(meta.avg_doc_length < 0.001);
        assert_eq(meta.next_doc_id, 1);
        assert_eq(meta.dict_root_page, NULL_PAGE);
    }

    it("should update avg_doc_length on insert") {
        ~config = FullTextConfig.default(DEFAULT_PAGE_SIZE);
        ~meta = FullTextMeta.new(config);

        # Insert doc with 100 tokens
        meta.update_avg_doc_length(100, true);
        assert_eq(meta.total_docs, 1);
        assert_true(meta.avg_doc_length > 99.9 && meta.avg_doc_length < 100.1);

        # Insert doc with 200 tokens
        meta.update_avg_doc_length(200, true);
        assert_eq(meta.total_docs, 2);
        # avg = (100 + 200) / 2 = 150
        assert_true(meta.avg_doc_length > 149.9 && meta.avg_doc_length < 150.1);
    }

    it("should update avg_doc_length on delete") {
        ~config = FullTextConfig.default(DEFAULT_PAGE_SIZE);
        ~meta = FullTextMeta.new(config);

        # Insert two docs
        meta.update_avg_doc_length(100, true);
        meta.update_avg_doc_length(200, true);
        # avg = 150, total_docs = 2

        # Delete doc with 100 tokens
        meta.update_avg_doc_length(100, false);
        assert_eq(meta.total_docs, 1);
        # avg = (300 - 100) / 1 = 200
        assert_true(meta.avg_doc_length > 199.9 && meta.avg_doc_length < 200.1);
    }

    it("should handle delete of last doc") {
        ~config = FullTextConfig.default(DEFAULT_PAGE_SIZE);
        ~meta = FullTextMeta.new(config);
        meta.update_avg_doc_length(100, true);
        meta.update_avg_doc_length(100, false);
        assert_eq(meta.total_docs, 0);
        assert_true(meta.avg_doc_length < 0.001);
    }

    it("should serialize/deserialize round-trip") {
        ~config = FullTextConfig.default(DEFAULT_PAGE_SIZE);
        ~meta = FullTextMeta.new(config);
        meta.total_docs = 1000;
        meta.total_terms = 5000;
        meta.total_tokens = 100000;
        meta.avg_doc_length = 100.0;
        meta.next_doc_id = 1001;
        meta.dict_root_page = 42;

        ~buf = ByteBuffer.new();
        meta.serialize(&buf);

        ~read_buf = ByteBuffer.wrap_readonly(buf.as_bytes());
        ~restored = FullTextMeta.deserialize(&read_buf, DEFAULT_PAGE_SIZE)!;

        assert_eq(restored.total_docs, 1000);
        assert_eq(restored.total_terms, 5000);
        assert_eq(restored.total_tokens, 100000);
        assert_true(restored.avg_doc_length > 99.9 && restored.avg_doc_length < 100.1);
        assert_eq(restored.next_doc_id, 1001);
        assert_eq(restored.dict_root_page, 42);
    }
}

# ============================================================================
# Compressed Posting List (PostingEntryCompact)
# ============================================================================

describe("Compressed Posting List") {
    it("should encode and decode a posting list") {
        ~entries = vec![
            PostingEntryCompact { doc_id: 10, term_freq: 3, positions: vec![0u32, 5, 10] },
            PostingEntryCompact { doc_id: 20, term_freq: 1, positions: vec![7u32] },
            PostingEntryCompact { doc_id: 35, term_freq: 2, positions: vec![2u32, 8] },
        ];

        ~encoded = encode_posting_list(&entries);
        ~decoded = decode_posting_list(&encoded)!;

        assert_eq(decoded.len(), 3);

        assert_eq(decoded[0].doc_id, 10);
        assert_eq(decoded[0].term_freq, 3);
        assert_eq(decoded[0].positions.len(), 3);
        assert_eq(decoded[0].positions[0], 0);
        assert_eq(decoded[0].positions[2], 10);

        assert_eq(decoded[1].doc_id, 20);
        assert_eq(decoded[1].term_freq, 1);
        assert_eq(decoded[1].positions[0], 7);

        assert_eq(decoded[2].doc_id, 35);
        assert_eq(decoded[2].term_freq, 2);
    }

    it("should handle empty posting list") {
        ~entries: Vec<PostingEntryCompact> = Vec.new();
        ~encoded = encode_posting_list(&entries);
        ~decoded = decode_posting_list(&encoded)!;
        assert_eq(decoded.len(), 0);
    }

    it("should handle single entry") {
        ~entries = vec![
            PostingEntryCompact { doc_id: 42, term_freq: 5, positions: vec![1u32, 3, 5, 7, 9] },
        ];
        ~encoded = encode_posting_list(&entries);
        ~decoded = decode_posting_list(&encoded)!;

        assert_eq(decoded.len(), 1);
        assert_eq(decoded[0].doc_id, 42);
        assert_eq(decoded[0].term_freq, 5);
        assert_eq(decoded[0].positions.len(), 5);
    }

    it("should handle entries with no positions") {
        ~entries = vec![
            PostingEntryCompact { doc_id: 1, term_freq: 2, positions: Vec.new() },
            PostingEntryCompact { doc_id: 5, term_freq: 1, positions: Vec.new() },
        ];
        ~encoded = encode_posting_list(&entries);
        ~decoded = decode_posting_list(&encoded)!;

        assert_eq(decoded.len(), 2);
        assert_eq(decoded[0].doc_id, 1);
        assert_eq(decoded[0].positions.len(), 0);
        assert_eq(decoded[1].doc_id, 5);
    }
}

# ============================================================================
# FNV-1a Hash
# ============================================================================

describe("FNV-1a Hash") {
    it("should produce non-zero hashes") {
        ~h1 = fnv1a_hash(&"hello");
        ~h2 = fnv1a_hash(&"world");
        assert_ne(h1, 0);
        assert_ne(h2, 0);
    }

    it("should produce different hashes for different strings") {
        ~h1 = fnv1a_hash(&"hello");
        ~h2 = fnv1a_hash(&"world");
        assert_ne(h1, h2);
    }

    it("should produce same hash for same string") {
        ~h1 = fnv1a_hash(&"test");
        ~h2 = fnv1a_hash(&"test");
        assert_eq(h1, h2);
    }
}
