# Storage & SQL Engine Benchmarks
# Benchmarks for B+Tree, buffer pool, WAL, page allocation, SQL DML, JOIN, aggregation, transactions
#
# ROADMAP targets:
# - TPC-B (transactions): Within 2x of SQLite
# - TPC-H (analytics, simplified): Functional correctness
# - Durability: Crash recovery 100% data integrity
# - Concurrency: 64 clients mixed workload

U benches/harness.{
    BenchmarkHarness, BenchmarkConfig, BenchmarkResult, Timer,
    generate_random_key, generate_random_text,
};
U std/vec.Vec;
U std/string.Str;

U storage/constants.{PAGE_SIZE_DEFAULT, NULL_PAGE, FILE_ID_DATA};
U storage/btree/tree.{BTree};
U storage/btree/insert.{btree_insert};
U storage/btree/search.{btree_search};
U storage/btree/node.{BTreeLeafNode};
U storage/btree/key.{encode_i64_key, encode_u64_key};
U storage/buffer/pool.{BufferPool};
U storage/page/manager.{PageManager};
U storage/page/allocator.{PageAllocator};
U storage/page/freelist.{FreelistBitmap};
U storage/wal/group_commit.{GroupCommitManager};
U storage/wal/writer.{WalWriter};
U storage/txn/manager.{TxnManager};

# ============================================================================
# B+Tree Benchmarks
# ============================================================================

# Benchmark: B+Tree sequential insert (1K keys)
F bench_btree_insert_1k(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("btree_insert_1k"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        ~leaf = BTreeLeafNode.new(1, PAGE_SIZE_DEFAULT, true);
        ~inserted: u64 = 0;
        ~i: u64 = 0;
        W i < 1000 {
            ~key = encode_i64_key(i as i64);
            M leaf.insert(&key, i as u32) {
                Ok(()) => { inserted += 1; },
                Err(_) => {},
            }
            i += 1;
        }
        inserted
    })
}

# Benchmark: B+Tree sequential insert (10K keys)
F bench_btree_insert_10k(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("btree_insert_10k"));
    config = config.with_iterations(20);

    harness.run_simple(&config, || -> u64 {
        ~leaf = BTreeLeafNode.new(1, PAGE_SIZE_DEFAULT, true);
        ~inserted: u64 = 0;
        ~i: u64 = 0;
        W i < 10000 {
            ~key = encode_i64_key(i as i64);
            M leaf.insert(&key, i as u32) {
                Ok(()) => { inserted += 1; },
                Err(_) => {},
            }
            i += 1;
        }
        inserted
    })
}

# Benchmark: B+Tree sequential insert (100K keys, leaf-level only)
F bench_btree_insert_100k(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("btree_insert_100k"));
    config = config.with_iterations(10);

    harness.run_simple(&config, || -> u64 {
        # Use multiple leaf nodes to hold 100K keys
        ~total_inserted: u64 = 0;
        ~leaf_count: u64 = 0;
        ~i: u64 = 0;

        ~leaf = BTreeLeafNode.new(leaf_count as u32 + 1, PAGE_SIZE_DEFAULT, true);
        W i < 100000 {
            ~key = encode_i64_key(i as i64);
            M leaf.insert(&key, i as u32) {
                Ok(()) => { total_inserted += 1; },
                Err(_) => {
                    # Leaf full, create new leaf
                    leaf_count += 1;
                    leaf = BTreeLeafNode.new(leaf_count as u32 + 1, PAGE_SIZE_DEFAULT, true);
                    M leaf.insert(&key, i as u32) {
                        Ok(()) => { total_inserted += 1; },
                        Err(_) => {},
                    }
                },
            }
            i += 1;
        }
        total_inserted
    })
}

# Benchmark: B+Tree point search (after filling leaf)
F bench_btree_search(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("btree_search_point"));
    config = config.with_iterations(100);

    # Pre-build a filled leaf
    ~leaf = BTreeLeafNode.new(1, PAGE_SIZE_DEFAULT, true);
    ~max_keys: u64 = 0;
    ~k: u64 = 0;
    ~filled = false;
    W !filled {
        ~key = encode_i64_key(k as i64);
        M leaf.insert(&key, k as u32) {
            Ok(()) => { max_keys += 1; k += 1; },
            Err(_) => { filled = true; },
        }
    }

    harness.run_simple(&config, || -> u64 {
        ~found: u64 = 0;
        ~i: u64 = 0;
        W i < max_keys {
            ~key = encode_i64_key(i as i64);
            M leaf.get_tid(&key) {
                Some(_) => { found += 1; },
                None => {},
            }
            i += 1;
        }
        found
    })
}

# Benchmark: B+Tree range scan (iterate all entries in leaf order)
F bench_btree_range_scan(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("btree_range_scan"));
    config = config.with_iterations(100);

    # Pre-build a filled leaf
    ~leaf = BTreeLeafNode.new(1, PAGE_SIZE_DEFAULT, true);
    ~max_keys: u64 = 0;
    ~k: u64 = 0;
    ~filled = false;
    W !filled {
        ~key = encode_i64_key(k as i64);
        M leaf.insert(&key, k as u32) {
            Ok(()) => { max_keys += 1; k += 1; },
            Err(_) => { filled = true; },
        }
    }

    harness.run_simple(&config, || -> u64 {
        # Scan all entries sequentially via lower_bound
        ~scanned: u64 = 0;
        ~i: u64 = 0;
        W i < max_keys {
            ~key = leaf.get_key(i);
            scanned += 1;
            i += 1;
        }
        scanned
    })
}

# ============================================================================
# Buffer Pool Benchmarks
# ============================================================================

# Benchmark: Buffer pool sequential page access (simulated hit rate)
F bench_buffer_pool_sequential(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("bufpool_sequential_access"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        ~pm = PageManager.new_in_memory(PAGE_SIZE_DEFAULT);
        ~pool = BufferPool.new(pm, 256);

        # Allocate and write 100 pages
        ~pages: u64 = 100;
        ~i: u64 = 0;
        W i < pages {
            ~frame = pool.allocate_page(FILE_ID_DATA)!;
            pool.unpin_page(frame, true);
            i += 1;
        }

        # Sequential access pattern (should achieve high hit rate)
        ~accesses: u64 = 0;
        ~pass: u64 = 0;
        W pass < 5 {
            i = 0;
            W i < pages {
                ~frame = pool.fetch_page(FILE_ID_DATA, i as u32)!;
                pool.unpin_page(frame, false);
                accesses += 1;
                i += 1;
            }
            pass += 1;
        }
        accesses
    })
}

# Benchmark: Buffer pool random page access (stress eviction)
F bench_buffer_pool_random(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("bufpool_random_access"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        ~pm = PageManager.new_in_memory(PAGE_SIZE_DEFAULT);
        ~pool = BufferPool.new(pm, 64);  # Small pool to force evictions

        # Allocate 256 pages (4x pool capacity)
        ~total_pages: u64 = 256;
        ~i: u64 = 0;
        W i < total_pages {
            ~frame = pool.allocate_page(FILE_ID_DATA)!;
            pool.unpin_page(frame, true);
            i += 1;
        }

        # Random-ish access pattern using LCG
        ~state: u64 = 42;
        ~accesses: u64 = 0;
        i = 0;
        W i < 500 {
            state = state * 6364136223846793005 + 1442695040888963407;
            ~page_id = (state >> 33) % total_pages;
            ~frame = pool.fetch_page(FILE_ID_DATA, page_id as u32)!;
            pool.unpin_page(frame, false);
            accesses += 1;
            i += 1;
        }
        accesses
    })
}

# ============================================================================
# WAL Benchmarks
# ============================================================================

# Benchmark: WAL write throughput (simulated sync writes)
F bench_wal_write_sync(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("wal_write_sync"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        ~writer = WalWriter.new_in_memory();
        ~writes: u64 = 0;
        ~i: u64 = 0;
        W i < 1000 {
            # Simulate a typical WAL record (tuple insert ~64 bytes)
            ~payload: Vec<u8> = Vec.new();
            ~j: u64 = 0;
            W j < 64 {
                payload.push((i % 256) as u8);
                j += 1;
            }
            writer.write_record(0x01, i, &payload)!;
            writer.sync()!;
            writes += 1;
            i += 1;
        }
        writes
    })
}

# Benchmark: WAL write throughput (async / group commit)
F bench_wal_write_async(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("wal_write_async_group"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        ~writer = WalWriter.new_in_memory();
        ~gcm = GroupCommitManager.new(&~writer, 10);  # Group size 10
        ~writes: u64 = 0;
        ~i: u64 = 0;
        W i < 1000 {
            ~payload: Vec<u8> = Vec.new();
            ~j: u64 = 0;
            W j < 64 {
                payload.push((i % 256) as u8);
                j += 1;
            }
            gcm.submit_record(0x01, i, &payload)!;
            writes += 1;
            i += 1;
        }
        # Flush remaining
        gcm.flush()!;
        writes
    })
}

# ============================================================================
# Page Allocation Benchmarks
# ============================================================================

# Benchmark: Page allocation throughput
F bench_page_alloc(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("page_alloc_throughput"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        ~pm = PageManager.new_in_memory(PAGE_SIZE_DEFAULT);
        ~pool = BufferPool.new(pm, 1024);
        ~allocator = PageAllocator.new(&pool);

        ~allocated: u64 = 0;
        ~i: u64 = 0;
        W i < 500 {
            allocator.allocate_page(FILE_ID_DATA)!;
            allocated += 1;
            i += 1;
        }
        allocated
    })
}

# Benchmark: Page allocation + deallocation (mixed pattern)
F bench_page_alloc_dealloc(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("page_alloc_dealloc_mixed"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        ~pm = PageManager.new_in_memory(PAGE_SIZE_DEFAULT);
        ~pool = BufferPool.new(pm, 1024);
        ~allocator = PageAllocator.new(&pool);
        ~bitmap = FreelistBitmap.new(FILE_ID_DATA, &pool);

        ~ops: u64 = 0;
        ~page_ids: Vec<u32> = Vec.new();

        # Allocate 200 pages
        ~i: u64 = 0;
        W i < 200 {
            ~pid = allocator.allocate_page(FILE_ID_DATA)!;
            page_ids.push(pid);
            ops += 1;
            i += 1;
        }

        # Free even-numbered pages
        i = 0;
        W i < page_ids.len() {
            I i % 2 == 0 {
                allocator.free_page(FILE_ID_DATA, page_ids[i])!;
                ops += 1;
            }
            i += 1;
        }

        # Reallocate (should reuse freed pages)
        i = 0;
        W i < 100 {
            allocator.allocate_page(FILE_ID_DATA)!;
            ops += 1;
            i += 1;
        }

        ops
    })
}

# ============================================================================
# SQL DML Benchmarks (Simulated — operates on data structures directly)
# ============================================================================

# Benchmark: Simulated SQL INSERT single-row throughput
# Tests the insert path: key encode → B+Tree insert → index maintenance
F bench_sql_insert_throughput(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("sql_insert_single_row"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        # Simulate INSERT by encoding keys and inserting into B+Tree leaf
        ~leaf = BTreeLeafNode.new(1, PAGE_SIZE_DEFAULT, true);
        ~inserted: u64 = 0;
        ~i: u64 = 0;
        ~full = false;
        W i < 5000 && !full {
            ~key = encode_i64_key(i as i64);
            M leaf.insert(&key, i as u32) {
                Ok(()) => { inserted += 1; },
                Err(_) => { full = true; },
            }
            i += 1;
        }
        inserted
    })
}

# Benchmark: Simulated SQL SELECT single-row lookup
F bench_sql_select_throughput(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("sql_select_single_row"));
    config = config.with_iterations(100);

    # Pre-fill leaf
    ~leaf = BTreeLeafNode.new(1, PAGE_SIZE_DEFAULT, true);
    ~max_keys: u64 = 0;
    ~k: u64 = 0;
    ~filled = false;
    W !filled {
        ~key = encode_i64_key(k as i64);
        M leaf.insert(&key, k as u32) {
            Ok(()) => { max_keys += 1; k += 1; },
            Err(_) => { filled = true; },
        }
    }

    harness.run_simple(&config, || -> u64 {
        ~found: u64 = 0;
        ~i: u64 = 0;
        W i < max_keys {
            ~key = encode_i64_key(i as i64);
            M leaf.get_tid(&key) {
                Some(_) => { found += 1; },
                None => {},
            }
            i += 1;
        }
        found
    })
}

# Benchmark: Simulated SQL UPDATE (delete + re-insert)
F bench_sql_update_throughput(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("sql_update_single_row"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        ~leaf = BTreeLeafNode.new(1, PAGE_SIZE_DEFAULT, true);
        # Fill with 100 entries
        ~i: u64 = 0;
        W i < 100 {
            ~key = encode_i64_key(i as i64);
            leaf.insert(&key, i as u32)!;
            i += 1;
        }

        # Update each entry (delete + re-insert with new TID)
        ~updated: u64 = 0;
        i = 0;
        W i < 100 {
            ~key = encode_i64_key(i as i64);
            leaf.delete(&key);
            leaf.insert(&key, (i + 1000) as u32)!;
            updated += 1;
            i += 1;
        }
        updated
    })
}

# Benchmark: Simulated SQL DELETE throughput
F bench_sql_delete_throughput(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("sql_delete_single_row"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        ~leaf = BTreeLeafNode.new(1, PAGE_SIZE_DEFAULT, true);
        # Fill with entries
        ~max_keys: u64 = 0;
        ~i: u64 = 0;
        ~full = false;
        W !full {
            ~key = encode_i64_key(i as i64);
            M leaf.insert(&key, i as u32) {
                Ok(()) => { max_keys += 1; i += 1; },
                Err(_) => { full = true; },
            }
        }

        # Delete all entries
        ~deleted: u64 = 0;
        i = 0;
        W i < max_keys {
            ~key = encode_i64_key(i as i64);
            M leaf.delete(&key) {
                Some(_) => { deleted += 1; },
                None => {},
            }
            i += 1;
        }
        deleted
    })
}

# ============================================================================
# SQL JOIN Benchmarks
# ============================================================================

# Benchmark: Nested Loop Join simulation
# Simulates NLJ over two key sets
F bench_sql_join_nlj(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("sql_join_nlj"));
    config = config.with_iterations(30);

    harness.run_simple(&config, || -> u64 {
        # Build two "tables" as sorted key arrays
        ~left_size: u64 = 100;
        ~right_size: u64 = 100;

        ~left_keys: Vec<i64> = Vec.new();
        ~right_keys: Vec<i64> = Vec.new();

        ~i: u64 = 0;
        W i < left_size {
            left_keys.push(i as i64);
            i += 1;
        }
        i = 0;
        W i < right_size {
            # Overlap: right keys start at 50
            right_keys.push((i + 50) as i64);
            i += 1;
        }

        # NLJ: for each left row, scan right table for matches
        ~matched: u64 = 0;
        ~li: u64 = 0;
        W li < left_keys.len() {
            ~ri: u64 = 0;
            W ri < right_keys.len() {
                I left_keys[li] == right_keys[ri] {
                    matched += 1;
                }
                ri += 1;
            }
            li += 1;
        }
        matched
    })
}

# Benchmark: Hash Join simulation
# Simulates hash join over two key sets
F bench_sql_join_hash(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("sql_join_hash"));
    config = config.with_iterations(30);

    harness.run_simple(&config, || -> u64 {
        U std/hashmap.HashMap;

        ~left_size: u64 = 500;
        ~right_size: u64 = 500;

        # Build phase: hash the smaller (left) table
        ~hash_table: HashMap<i64, u64> = HashMap.new();
        ~i: u64 = 0;
        W i < left_size {
            hash_table.insert(i as i64, i);
            i += 1;
        }

        # Probe phase: scan right table and probe hash
        ~matched: u64 = 0;
        i = 0;
        W i < right_size {
            ~key = (i + 250) as i64;  # Overlap at 250
            M hash_table.get(&key) {
                Some(_) => { matched += 1; },
                None => {},
            }
            i += 1;
        }
        matched
    })
}

# Benchmark: Join scaling (NLJ vs Hash at different sizes)
F bench_sql_join_scaling(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("sql_join_scaling_1k"));
    config = config.with_iterations(10);

    harness.run_simple(&config, || -> u64 {
        U std/hashmap.HashMap;

        ~size: u64 = 1000;

        # Hash join at 1K x 1K
        ~hash_table: HashMap<i64, u64> = HashMap.new();
        ~i: u64 = 0;
        W i < size {
            hash_table.insert(i as i64, i);
            i += 1;
        }

        ~matched: u64 = 0;
        i = 0;
        W i < size {
            ~key = (i + 500) as i64;
            M hash_table.get(&key) {
                Some(_) => { matched += 1; },
                None => {},
            }
            i += 1;
        }
        matched
    })
}

# ============================================================================
# SQL Aggregation Benchmark
# ============================================================================

# Benchmark: GROUP BY aggregation (hash-based)
F bench_sql_groupby(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("sql_groupby_hash"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        U std/hashmap.HashMap;

        # Simulate GROUP BY on 10K rows with 100 distinct groups
        ~groups: HashMap<u64, (u64, i64)> = HashMap.new();  # group_key -> (count, sum)
        ~rows: u64 = 10000;
        ~i: u64 = 0;
        W i < rows {
            ~group_key = i % 100;
            ~value = i as i64;
            M groups.get(&group_key) {
                Some(~entry) => {
                    ~new_count = entry.0 + 1;
                    ~new_sum = entry.1 + value;
                    groups.insert(group_key, (new_count, new_sum));
                },
                None => {
                    groups.insert(group_key, (1, value));
                },
            }
            i += 1;
        }
        rows
    })
}

# ============================================================================
# Transaction Benchmarks
# ============================================================================

# Benchmark: Transaction commit throughput (simulated)
F bench_txn_commit(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("txn_commit_throughput"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        ~writer = WalWriter.new_in_memory();
        ~gcm = GroupCommitManager.new(&~writer, 10);

        ~committed: u64 = 0;
        ~i: u64 = 0;
        W i < 500 {
            # Simulate: begin txn + write + commit
            ~txn_id = i + 1;

            # Write a tuple insert WAL record
            ~payload: Vec<u8> = Vec.new();
            ~j: u64 = 0;
            W j < 32 {
                payload.push((i % 256) as u8);
                j += 1;
            }
            gcm.submit_record(0x01, txn_id, &payload)!;

            # Write commit record
            ~commit_payload: Vec<u8> = Vec.new();
            commit_payload.push(0x10);  # COMMIT type marker
            gcm.submit_record(0x10, txn_id, &commit_payload)!;

            committed += 1;
            i += 1;
        }
        gcm.flush()!;
        committed
    })
}

# Benchmark: Transaction rollback throughput (simulated)
F bench_txn_rollback(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("txn_rollback_throughput"));
    config = config.with_iterations(50);

    harness.run_simple(&config, || -> u64 {
        ~writer = WalWriter.new_in_memory();
        ~gcm = GroupCommitManager.new(&~writer, 10);

        ~rolled_back: u64 = 0;
        ~i: u64 = 0;
        W i < 500 {
            ~txn_id = i + 1;

            # Write a tuple insert WAL record
            ~payload: Vec<u8> = Vec.new();
            ~j: u64 = 0;
            W j < 32 {
                payload.push((i % 256) as u8);
                j += 1;
            }
            gcm.submit_record(0x01, txn_id, &payload)!;

            # Write abort record
            ~abort_payload: Vec<u8> = Vec.new();
            abort_payload.push(0x11);  # ABORT type marker
            gcm.submit_record(0x11, txn_id, &abort_payload)!;

            rolled_back += 1;
            i += 1;
        }
        gcm.flush()!;
        rolled_back
    })
}

# ============================================================================
# Suite Runner
# ============================================================================

# Run all storage and SQL benchmarks
F run_storage_sql_benchmarks() -> BenchmarkHarness {
    ~harness = BenchmarkHarness.new(Str.from("Storage & SQL"));
    harness.start();

    # B+Tree benchmarks
    bench_btree_insert_1k(&~harness);
    bench_btree_insert_10k(&~harness);
    bench_btree_insert_100k(&~harness);
    bench_btree_search(&~harness);
    bench_btree_range_scan(&~harness);

    # Buffer pool benchmarks
    bench_buffer_pool_sequential(&~harness);
    bench_buffer_pool_random(&~harness);

    # WAL benchmarks
    bench_wal_write_sync(&~harness);
    bench_wal_write_async(&~harness);

    # Page allocation benchmarks
    bench_page_alloc(&~harness);
    bench_page_alloc_dealloc(&~harness);

    # SQL DML benchmarks
    bench_sql_insert_throughput(&~harness);
    bench_sql_select_throughput(&~harness);
    bench_sql_update_throughput(&~harness);
    bench_sql_delete_throughput(&~harness);

    # SQL JOIN benchmarks
    bench_sql_join_nlj(&~harness);
    bench_sql_join_hash(&~harness);
    bench_sql_join_scaling(&~harness);

    # SQL aggregation
    bench_sql_groupby(&~harness);

    # Transaction benchmarks
    bench_txn_commit(&~harness);
    bench_txn_rollback(&~harness);

    harness.print_report();
    harness
}
