# Hybrid (Cross-Engine) Query Benchmarks
# Benchmarks for score fusion, multi-engine queries, plan cache, RAG pipeline
#
# ROADMAP target:
# - Hybrid: Vector+Graph+SQL < 2x slowest single-engine query

U benches/harness.{
    BenchmarkHarness, BenchmarkConfig, BenchmarkResult, Timer,
    generate_random_vector, generate_random_vectors, generate_random_text,
    format_with_commas,
};
U std/vec.Vec;
U std/string.Str;
U std/hashmap.HashMap;
U std/math.{sqrt};

# Score fusion
U fulltext/integration/fusion.{
    ScoreFusionMethod, ScoredDoc, fuse_results, normalize_scores,
};

# Plan cache
U planner/cache.{PlanCache, PlanCacheKey, normalize_query, extract_tables};
U planner/types.{HybridPlanNode, HybridCost, EngineType, FusionMethod};
U planner/explain.{explain_plan, ExplainOptions, ExplainFormat};

# Distance for vector simulation
U vector/distance.{cosine_distance};

# Tokenizer for text processing
U fulltext/types.{FullTextConfig};
U fulltext/tokenizer.{Tokenizer};
U fulltext/search/bm25.{BM25Scorer};

# RAG chunking
U rag/chunking/chunker.{SemanticChunker, ChunkingConfig};
U rag/chunking/strategies.{split_fixed_size, split_sentence_boundary, ChunkBoundary};

# ============================================================================
# Score Fusion Benchmarks
# ============================================================================

# Benchmark: WeightedSum fusion throughput
F bench_fusion_weighted_sum(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("fusion_weighted_sum"));
    config = config.with_iterations(100);

    # Pre-generate two result sets
    ~results_a: Vec<ScoredDoc> = Vec.new();
    ~results_b: Vec<ScoredDoc> = Vec.new();
    ~i: u64 = 0;
    W i < 1000 {
        results_a.push(ScoredDoc.new(i, 1.0 - (i as f64 / 1000.0)));
        results_b.push(ScoredDoc.new(i * 2, 0.5 + (i as f64 / 2000.0)));
        i += 1;
    }

    ~method = ScoreFusionMethod.WeightedSum { weight_a: 0.7, weight_b: 0.3 };

    harness.run_simple(&config, || -> u64 {
        ~fused = fuse_results(&method, &results_a, &results_b);
        fused.len()
    })
}

# Benchmark: Reciprocal Rank Fusion (RRF) throughput
F bench_fusion_rrf(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("fusion_rrf_k60"));
    config = config.with_iterations(100);

    ~results_a: Vec<ScoredDoc> = Vec.new();
    ~results_b: Vec<ScoredDoc> = Vec.new();
    ~i: u64 = 0;
    W i < 1000 {
        results_a.push(ScoredDoc.new(i, 1.0 - (i as f64 / 1000.0)));
        results_b.push(ScoredDoc.new(i * 2, 0.5 + (i as f64 / 2000.0)));
        i += 1;
    }

    ~method = ScoreFusionMethod.ReciprocalRankFusion { k: 60 };

    harness.run_simple(&config, || -> u64 {
        ~fused = fuse_results(&method, &results_a, &results_b);
        fused.len()
    })
}

# Benchmark: Score normalization throughput
F bench_score_normalization(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("score_normalization"));
    config = config.with_iterations(200);

    ~results: Vec<ScoredDoc> = Vec.new();
    ~i: u64 = 0;
    W i < 5000 {
        ~state = i * 6364136223846793005 + 1442695040888963407;
        ~score = (state >> 33) as f64 / 2147483648.0;
        results.push(ScoredDoc.new(i, score));
        i += 1;
    }

    harness.run_simple(&config, || -> u64 {
        ~normalized = normalize_scores(&results);
        normalized.len()
    })
}

# ============================================================================
# Vector+SQL Hybrid Query Simulation
# ============================================================================

# Benchmark: Vector search + SQL filter hybrid pipeline
# Simulates: SELECT * FROM t WHERE VECTOR_SEARCH(embedding, query, k=10) AND price < 100
F bench_hybrid_vector_sql(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("hybrid_vector_sql"));
    config = config.with_iterations(30);

    # Pre-build vector dataset
    ~vectors = generate_random_vectors(5000, 128, 42);
    # Pre-build "SQL rows" with price attribute
    ~prices: Vec<f64> = Vec.new();
    ~i: u64 = 0;
    W i < 5000 {
        ~state = i * 6364136223846793005 + 1442695040888963407;
        prices.push((state >> 40) as f64 % 200.0);
        i += 1;
    }

    harness.run_simple(&config, || -> u64 {
        ~query = generate_random_vector(128, 9999);

        # Phase 1: Vector search (brute-force kNN, k=50 to over-fetch)
        ~dists: Vec<(u64, f64)> = Vec.new();
        ~i: u64 = 0;
        W i < vectors.len() {
            ~d = cosine_distance(&query, &vectors[i])!;
            dists.push((i, d));
            i += 1;
        }

        # Partial sort top-50
        ~k: u64 = 50;
        ~ki: u64 = 0;
        W ki < k && ki < dists.len() {
            ~min_idx = ki;
            ~j: u64 = ki + 1;
            W j < dists.len() {
                I dists[j].1 < dists[min_idx].1 { min_idx = j; }
                j += 1;
            }
            I min_idx != ki {
                ~tmp = dists[ki];
                dists[ki] = dists[min_idx];
                dists[min_idx] = tmp;
            }
            ki += 1;
        }

        # Phase 2: SQL filter (price < 100)
        ~final_results: Vec<(u64, f64)> = Vec.new();
        i = 0;
        W i < k {
            ~doc_id = dists[i].0;
            I prices[doc_id] < 100.0 {
                final_results.push(dists[i]);
            }
            i += 1;
        }

        # Return top-10 from filtered
        I final_results.len() > 10 { 10 } E { final_results.len() }
    })
}

# ============================================================================
# Vector+Graph Combined Scoring Simulation
# ============================================================================

# Benchmark: Vector similarity + Graph proximity combined scoring
# Simulates: find similar items that are also close in the knowledge graph
F bench_hybrid_vector_graph(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("hybrid_vector_graph"));
    config = config.with_iterations(20);

    # Pre-build vector dataset
    ~vectors = generate_random_vectors(2000, 128, 42);

    # Pre-build graph adjacency (each node has ~4 neighbors)
    ~adj_list: Vec<Vec<u64>> = Vec.new();
    ~i: u64 = 0;
    W i < 2000 {
        ~neighbors: Vec<u64> = Vec.new();
        ~state = i * 6364136223846793005 + 1442695040888963407;
        ~j: u64 = 0;
        W j < 4 {
            state = state * 6364136223846793005 + 1442695040888963407;
            ~target = (state >> 33) % 2000;
            neighbors.push(target);
            j += 1;
        }
        adj_list.push(neighbors);
        i += 1;
    }

    harness.run_simple(&config, || -> u64 {
        ~query = generate_random_vector(128, 7777);

        # Phase 1: Vector kNN (top-50)
        ~dists: Vec<(u64, f64)> = Vec.new();
        ~i: u64 = 0;
        W i < vectors.len() {
            ~d = cosine_distance(&query, &vectors[i])!;
            dists.push((i, d));
            i += 1;
        }

        ~k: u64 = 50;
        ~ki: u64 = 0;
        W ki < k && ki < dists.len() {
            ~min_idx = ki;
            ~j: u64 = ki + 1;
            W j < dists.len() {
                I dists[j].1 < dists[min_idx].1 { min_idx = j; }
                j += 1;
            }
            I min_idx != ki {
                ~tmp = dists[ki];
                dists[ki] = dists[min_idx];
                dists[min_idx] = tmp;
            }
            ki += 1;
        }

        # Phase 2: Graph proximity boost (BFS depth 2 from each candidate)
        ~scored: Vec<ScoredDoc> = Vec.new();
        i = 0;
        W i < k {
            ~node = dists[i].0;
            ~vec_score = 1.0 - dists[i].1;  # Convert distance to similarity

            # 1-hop neighborhood size as graph score proxy
            ~neighborhood_size = adj_list[node].len();

            # 2-hop: count unique nodes reachable in 2 hops
            ~two_hop_count: u64 = 0;
            ~ni: u64 = 0;
            W ni < adj_list[node].len() {
                two_hop_count += adj_list[adj_list[node][ni]].len();
                ni += 1;
            }

            ~graph_score = (neighborhood_size as f64 + two_hop_count as f64 * 0.5) / 50.0;
            I graph_score > 1.0 { graph_score = 1.0; }

            # Combined: 0.7 * vector + 0.3 * graph
            ~combined = 0.7 * vec_score + 0.3 * graph_score;
            scored.push(ScoredDoc.new(node, combined));

            i += 1;
        }

        scored.len()
    })
}

# ============================================================================
# FullText+Vector Hybrid Search (BM25 + Cosine)
# ============================================================================

# Benchmark: BM25 + Vector cosine hybrid search simulation
F bench_hybrid_fulltext_vector(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("hybrid_ft_vector"));
    config = config.with_iterations(20);

    # Pre-build vector dataset
    ~vectors = generate_random_vectors(2000, 128, 42);

    # Pre-build BM25 scores (simulated)
    ~bm25_scores: Vec<ScoredDoc> = Vec.new();
    ~i: u64 = 0;
    W i < 500 {
        ~state = i * 6364136223846793005 + 1442695040888963407;
        ~score = (state >> 33) as f64 / 2147483648.0 * 10.0;  # BM25 range [0, 10]
        bm25_scores.push(ScoredDoc.new(i, score));
        i += 1;
    }

    harness.run_simple(&config, || -> u64 {
        ~query = generate_random_vector(128, 5555);

        # Phase 1: Vector kNN (top-100)
        ~vec_scores: Vec<ScoredDoc> = Vec.new();
        ~dists: Vec<(u64, f64)> = Vec.new();
        ~i: u64 = 0;
        W i < vectors.len() {
            ~d = cosine_distance(&query, &vectors[i])!;
            dists.push((i, d));
            i += 1;
        }

        # Top-100
        ~k: u64 = 100;
        ~ki: u64 = 0;
        W ki < k && ki < dists.len() {
            ~min_idx = ki;
            ~j: u64 = ki + 1;
            W j < dists.len() {
                I dists[j].1 < dists[min_idx].1 { min_idx = j; }
                j += 1;
            }
            I min_idx != ki {
                ~tmp = dists[ki];
                dists[ki] = dists[min_idx];
                dists[min_idx] = tmp;
            }
            ki += 1;
        }

        i = 0;
        W i < k {
            vec_scores.push(ScoredDoc.new(dists[i].0, 1.0 - dists[i].1));
            i += 1;
        }

        # Phase 2: Fuse BM25 + Vector with RRF
        ~method = ScoreFusionMethod.ReciprocalRankFusion { k: 60 };
        ~fused = fuse_results(&method, &bm25_scores, &vec_scores);

        # Return top-10
        I fused.len() > 10 { 10 } E { fused.len() }
    })
}

# ============================================================================
# Three-Engine Query (SQL+Vector+FullText)
# ============================================================================

# Benchmark: SQL filter + Vector search + FullText BM25 fusion
F bench_three_engine_query(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("three_engine_sql_vec_ft"));
    config = config.with_iterations(15);

    ~vectors = generate_random_vectors(3000, 128, 42);

    # Simulated SQL attribute
    ~categories: Vec<u64> = Vec.new();
    ~i: u64 = 0;
    W i < 3000 {
        categories.push(i % 10);  # 10 categories
        i += 1;
    }

    # Simulated BM25 scores for 1000 docs
    ~bm25_results: Vec<ScoredDoc> = Vec.new();
    i = 0;
    W i < 1000 {
        ~state = i * 6364136223846793005 + 1442695040888963407;
        ~score = (state >> 33) as f64 / 2147483648.0 * 8.0;
        bm25_results.push(ScoredDoc.new(i, score));
        i += 1;
    }

    harness.run_simple(&config, || -> u64 {
        ~query = generate_random_vector(128, 3333);

        # Phase 1: SQL filter (category == 3)
        ~sql_filtered: Vec<u64> = Vec.new();
        ~i: u64 = 0;
        W i < categories.len() {
            I categories[i] == 3 {
                sql_filtered.push(i);
            }
            i += 1;
        }

        # Phase 2: Vector kNN on filtered set
        ~vec_scores: Vec<ScoredDoc> = Vec.new();
        i = 0;
        W i < sql_filtered.len() {
            ~doc_id = sql_filtered[i];
            ~d = cosine_distance(&query, &vectors[doc_id])!;
            vec_scores.push(ScoredDoc.new(doc_id, 1.0 - d));
            i += 1;
        }

        # Phase 3: Fuse with BM25 (RRF)
        ~method = ScoreFusionMethod.ReciprocalRankFusion { k: 60 };
        ~fused = fuse_results(&method, &vec_scores, &bm25_results);

        I fused.len() > 10 { 10 } E { fused.len() }
    })
}

# ============================================================================
# Four-Engine Query (SQL+Vector+Graph+FullText)
# ============================================================================

# Benchmark: SQL + Vector + Graph + FullText all combined
F bench_four_engine_query(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("four_engine_all"));
    config = config.with_iterations(10);

    ~vectors = generate_random_vectors(2000, 128, 42);

    # SQL attributes
    ~categories: Vec<u64> = Vec.new();
    ~i: u64 = 0;
    W i < 2000 {
        categories.push(i % 10);
        i += 1;
    }

    # Graph adjacency
    ~adj_list: Vec<Vec<u64>> = Vec.new();
    i = 0;
    W i < 2000 {
        ~neighbors: Vec<u64> = Vec.new();
        ~state = i * 6364136223846793005 + 1442695040888963407;
        ~j: u64 = 0;
        W j < 4 {
            state = state * 6364136223846793005 + 1442695040888963407;
            neighbors.push((state >> 33) % 2000);
            j += 1;
        }
        adj_list.push(neighbors);
        i += 1;
    }

    # BM25 scores
    ~bm25_results: Vec<ScoredDoc> = Vec.new();
    i = 0;
    W i < 800 {
        ~state = i * 6364136223846793005 + 1442695040888963407;
        bm25_results.push(ScoredDoc.new(i, (state >> 33) as f64 / 2147483648.0 * 8.0));
        i += 1;
    }

    harness.run_simple(&config, || -> u64 {
        ~query = generate_random_vector(128, 1111);

        # Phase 1: SQL filter (category == 5)
        ~sql_filtered: Vec<u64> = Vec.new();
        ~i: u64 = 0;
        W i < categories.len() {
            I categories[i] == 5 {
                sql_filtered.push(i);
            }
            i += 1;
        }

        # Phase 2: Vector kNN on filtered set
        ~vec_scores: Vec<ScoredDoc> = Vec.new();
        i = 0;
        W i < sql_filtered.len() {
            ~doc_id = sql_filtered[i];
            ~d = cosine_distance(&query, &vectors[doc_id])!;
            vec_scores.push(ScoredDoc.new(doc_id, 1.0 - d));
            i += 1;
        }

        # Phase 3: Graph proximity boost for top vector results
        # Sort vec_scores by score descending (selection sort on top 30)
        ~top_k: u64 = 30;
        ~ki: u64 = 0;
        W ki < top_k && ki < vec_scores.len() {
            ~max_idx = ki;
            ~j: u64 = ki + 1;
            W j < vec_scores.len() {
                I vec_scores[j].score > vec_scores[max_idx].score { max_idx = j; }
                j += 1;
            }
            I max_idx != ki {
                ~tmp_id = vec_scores[ki].doc_id;
                ~tmp_score = vec_scores[ki].score;
                vec_scores[ki] = ScoredDoc.new(vec_scores[max_idx].doc_id, vec_scores[max_idx].score);
                vec_scores[max_idx] = ScoredDoc.new(tmp_id, tmp_score);
            }
            ki += 1;
        }

        ~graph_boosted: Vec<ScoredDoc> = Vec.new();
        i = 0;
        ~limit = I vec_scores.len() < top_k { vec_scores.len() } E { top_k };
        W i < limit {
            ~node = vec_scores[i].doc_id;
            ~graph_boost = adj_list[node].len() as f64 / 20.0;
            I graph_boost > 0.5 { graph_boost = 0.5; }
            ~boosted_score = vec_scores[i].score + graph_boost * 0.2;
            graph_boosted.push(ScoredDoc.new(node, boosted_score));
            i += 1;
        }

        # Phase 4: Fuse with BM25 (WeightedSum)
        ~method = ScoreFusionMethod.WeightedSum { weight_a: 0.6, weight_b: 0.4 };
        ~fused = fuse_results(&method, &graph_boosted, &bm25_results);

        I fused.len() > 10 { 10 } E { fused.len() }
    })
}

# ============================================================================
# Plan Cache Benchmarks
# ============================================================================

# Benchmark: Plan cache hit/miss impact
F bench_plan_cache(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("plan_cache_hit_miss"));
    config = config.with_iterations(100);

    harness.run_simple(&config, || -> u64 {
        ~cache = PlanCache.new();

        # Populate cache with 50 different query fingerprints
        ~i: u64 = 0;
        W i < 50 {
            ~sql = Str.from("SELECT * FROM t");
            sql.push_str(&i.to_string());
            sql.push_str(" WHERE id = $1");
            ~normalized = normalize_query(&sql);
            ~key = PlanCacheKey.new(normalized);
            ~cost = HybridCost.new();
            ~tables: Vec<Str> = Vec.new();
            tables.push(Str.from("t"));
            # Use a minimal plan node placeholder
            cache.put(key, HybridPlanNode.empty(), cost, tables);
            i += 1;
        }

        # Mix of hits and misses
        ~ops: u64 = 0;
        i = 0;
        W i < 1000 {
            ~sql = Str.from("SELECT * FROM t");
            ~idx = i % 75;  # 50 exist, 25 will miss
            sql.push_str(&idx.to_string());
            sql.push_str(" WHERE id = $1");
            ~normalized = normalize_query(&sql);
            ~key = PlanCacheKey.new(normalized);
            cache.get(&key);
            ops += 1;
            i += 1;
        }
        ops
    })
}

# ============================================================================
# Query Normalization Benchmark
# ============================================================================

# Benchmark: Query normalization throughput
F bench_query_normalization(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("query_normalization"));
    config = config.with_iterations(100);

    ~queries: Vec<Str> = Vec.new();
    queries.push(Str.from("SELECT * FROM users WHERE id = 42"));
    queries.push(Str.from("SELECT name, age FROM products WHERE price > 99.99 AND category = 'electronics'"));
    queries.push(Str.from("SELECT * FROM documents WHERE VECTOR_SEARCH(embedding, [0.1, 0.2], k=10)"));
    queries.push(Str.from("SELECT * FROM nodes WHERE GRAPH_TRAVERSE(start=1, depth=3)"));
    queries.push(Str.from("SELECT * FROM articles WHERE FULLTEXT_MATCH(body, 'database vector search')"));
    queries.push(Str.from("SELECT d.*, v.distance FROM docs d JOIN VECTOR_SEARCH(d.embedding, q, 10) v ON d.id = v.id WHERE d.category = 'tech'"));
    queries.push(Str.from("SELECT * FROM t1 JOIN t2 ON t1.id = t2.fk WHERE t1.x > 100 ORDER BY t1.y LIMIT 50"));
    queries.push(Str.from("SELECT department, COUNT(*), AVG(salary) FROM employees GROUP BY department HAVING COUNT(*) > 5"));

    harness.run_simple(&config, || -> u64 {
        ~normalized: u64 = 0;
        ~i: u64 = 0;
        W i < queries.len() {
            normalize_query(&queries[i]);
            normalized += 1;
            i += 1;
        }
        normalized
    })
}

# ============================================================================
# EXPLAIN Overhead Benchmark
# ============================================================================

# Benchmark: EXPLAIN format overhead (text format)
F bench_explain_overhead(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("explain_text_overhead"));
    config = config.with_iterations(100);

    harness.run_simple(&config, || -> u64 {
        # Build a synthetic plan tree and measure EXPLAIN formatting cost
        ~plan = HybridPlanNode.empty();
        ~options = ExplainOptions {
            analyze: false,
            verbose: true,
            format: ExplainFormat.Text,
        };

        ~formatted: u64 = 0;
        ~i: u64 = 0;
        W i < 100 {
            M explain_plan(&plan, &options) {
                Ok(_) => { formatted += 1; },
                Err(_) => {},
            }
            i += 1;
        }
        formatted
    })
}

# ============================================================================
# RAG Pipeline Benchmarks
# ============================================================================

# Benchmark: Document chunking throughput
F bench_rag_chunking(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("rag_chunking_pipeline"));
    config = config.with_iterations(30);

    # Generate large documents
    ~documents: Vec<Str> = Vec.new();
    ~i: u64 = 0;
    W i < 50 {
        documents.push(generate_random_text(2000, i + 1));  # ~2000 word documents
        i += 1;
    }

    ~chunk_config = ChunkingConfig.default();

    harness.run_simple(&config, || -> u64 {
        ~total_chunks: u64 = 0;
        ~i: u64 = 0;
        W i < documents.len() {
            # Use fixed-size splitting
            ~boundaries = split_fixed_size(&documents[i], chunk_config.target_chunk_size, chunk_config.overlap_tokens);
            total_chunks += boundaries.len();
            i += 1;
        }
        total_chunks
    })
}

# Benchmark: RAG end-to-end pipeline simulation (chunk + embed + search)
F bench_rag_end_to_end(harness: &~BenchmarkHarness) -> BenchmarkResult {
    ~config = BenchmarkConfig.default(Str.from("rag_e2e_pipeline"));
    config = config.with_iterations(10);

    # Pre-generate documents
    ~documents: Vec<Str> = Vec.new();
    ~i: u64 = 0;
    W i < 20 {
        documents.push(generate_random_text(1000, i + 1));
        i += 1;
    }

    ~chunk_config = ChunkingConfig.default();
    ~ft_config = FullTextConfig.default();
    ~tokenizer = Tokenizer.from_config(&ft_config);
    ~scorer = BM25Scorer.with_params(1.2, 0.75);

    harness.run_simple(&config, || -> u64 {
        # Stage 1: Chunk all documents
        ~all_chunks: Vec<Str> = Vec.new();
        ~chunk_to_doc: Vec<u64> = Vec.new();
        ~i: u64 = 0;
        W i < documents.len() {
            ~boundaries = split_fixed_size(
                &documents[i],
                chunk_config.target_chunk_size,
                chunk_config.overlap_tokens,
            );
            ~bi: u64 = 0;
            W bi < boundaries.len() {
                ~boundary = &boundaries[bi];
                ~chunk_text = documents[i].substring(boundary.start_byte as u64, boundary.end_byte as u64);
                all_chunks.push(chunk_text);
                chunk_to_doc.push(i);
                bi += 1;
            }
            i += 1;
        }

        # Stage 2: Generate embeddings (simulated — random vectors as proxy)
        ~embeddings = generate_random_vectors(all_chunks.len(), 128, 42);

        # Stage 3: Search — find chunks similar to query
        ~query_vec = generate_random_vector(128, 9999);
        ~query_text = Str.from("database vector search optimization");

        # Vector similarity
        ~vec_scores: Vec<ScoredDoc> = Vec.new();
        i = 0;
        W i < embeddings.len() {
            ~d = cosine_distance(&query_vec, &embeddings[i])!;
            vec_scores.push(ScoredDoc.new(i, 1.0 - d));
            i += 1;
        }

        # BM25 scoring (simplified — tokenize query + each chunk)
        ~query_tokens = tokenizer.tokenize(&query_text);
        ~bm25_scores: Vec<ScoredDoc> = Vec.new();
        ~avg_len: f64 = 300.0;
        ~total_docs = all_chunks.len();

        i = 0;
        W i < all_chunks.len() {
            ~chunk_tokens = tokenizer.tokenize_with_freqs(&all_chunks[i]);
            ~total_score: f64 = 0.0;
            # Score each query term
            ~qi: u64 = 0;
            W qi < query_tokens.len() {
                ~qt = &query_tokens[qi].term;
                # Find term in chunk
                ~ci: u64 = 0;
                W ci < chunk_tokens.len() {
                    I chunk_tokens[ci].term == *qt {
                        ~score = scorer.score(
                            chunk_tokens[ci].freq,
                            total_docs / 10,  # Rough df estimate
                            chunk_tokens.len() as u32,
                            avg_len,
                            total_docs,
                        );
                        total_score += score;
                    }
                    ci += 1;
                }
                qi += 1;
            }
            I total_score > 0.0 {
                bm25_scores.push(ScoredDoc.new(i, total_score));
            }
            i += 1;
        }

        # Stage 4: Fuse vector + BM25 results
        ~method = ScoreFusionMethod.ReciprocalRankFusion { k: 60 };
        ~fused = fuse_results(&method, &vec_scores, &bm25_scores);

        # Return top-5
        I fused.len() > 5 { 5 } E { fused.len() }
    })
}

# ============================================================================
# Suite Runner
# ============================================================================

# Run all hybrid (cross-engine) benchmarks
F run_hybrid_benchmarks() -> BenchmarkHarness {
    ~harness = BenchmarkHarness.new(Str.from("Hybrid Cross-Engine Queries"));
    harness.start();

    # Score fusion
    bench_fusion_weighted_sum(&~harness);
    bench_fusion_rrf(&~harness);
    bench_score_normalization(&~harness);

    # Hybrid queries
    bench_hybrid_vector_sql(&~harness);
    bench_hybrid_vector_graph(&~harness);
    bench_hybrid_fulltext_vector(&~harness);

    # Multi-engine
    bench_three_engine_query(&~harness);
    bench_four_engine_query(&~harness);

    # Plan cache
    bench_plan_cache(&~harness);
    bench_query_normalization(&~harness);

    # EXPLAIN overhead
    bench_explain_overhead(&~harness);

    # RAG pipeline
    bench_rag_chunking(&~harness);
    bench_rag_end_to_end(&~harness);

    harness.print_report();
    harness
}
